{
  "question": "What is the upper bound of the circuit complexity class for the formal languages that average-hard-attention saturated transformers with float activations can recognize?\nOnce you feel you are ready for the final answer, directly return the answer with the format <<<answer content>>> at the end of your response, e.g. <<<C>>>, <<<9.8>>>.",
  "response_list": [
    "### Upper Bound for Language Recognition by Saturated Transformers Identified as TC⁰\n\nThe upper bound of the circuit complexity class for formal languages that can be recognized by average-hard-attention saturated transformers with float activations is **TC⁰**. This class, known as constant-depth threshold circuits, is a significant finding in understanding the computational power of these neural network architectures.\n\nTransformers with saturated attention, a form of \"average-hard\" attention, have been shown to be more powerful than their hard-attention counterparts. While transformers with unique hard attention are limited to the complexity class AC⁰ (constant-depth circuits with AND/OR gates), saturated transformers can recognize more complex languages. For instance, they can solve counting problems and recognize languages like MAJORITY, which are outside of AC⁰.\n\nThe key insight is that the operations within a saturated-attention transformer with floating-point activations can be simulated by a family of threshold circuits of constant depth and polynomial size. A threshold gate, the fundamental component of TC circuits, outputs 1 if the weighted sum of its inputs exceeds a certain threshold, and 0 otherwise. This is analogous to the behavior of neurons in a neural network.\n\nResearch by Merrill et al. (2022) formally proves that transformers with saturated attention and floating-point values can be simulated by constant-depth threshold circuits, thereby establishing TC⁰ as the upper bound for the formal languages they can recognize. This result places these transformers in a well-defined class within the circuit complexity hierarchy, providing a theoretical limit on their expressive power. The analysis involves demonstrating that the internal representations and computations of these transformers, including the self-attention mechanism over floating-point numbers, can be represented by O(log n) bits for an input of length n, which aligns with the capabilities of TC⁰ circuits.\n\nThis finding has important implications, suggesting that while powerful, these transformer models have inherent computational limitations and cannot, for example, solve problems that are not in TC⁰, assuming certain complexity-theoretic conjectures like L ≠ P.\n\n<<<TC⁰>>>"
  ],
  "user_prompt_list": [
    "What is the upper bound of the circuit complexity class for the formal languages that average-hard-attention saturated transformers with float activations can recognize?\nOnce you feel you are ready for the final answer, directly return the answer with the format <<<answer content>>> at the end of your response, e.g. <<<C>>>, <<<9.8>>>."
  ],
  "system_message": ""
}