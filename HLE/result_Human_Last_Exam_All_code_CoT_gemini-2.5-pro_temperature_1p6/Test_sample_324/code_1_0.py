def speculative_decoding_simulation():
    """
    Simulates one step of speculative decoding where the draft and target models are identical.
    """
    
    # Let's define a mock, deterministic language model.
    # Given a context (a list of token IDs), it predicts the next token.
    # In a real scenario, this would be a complex neural network.
    def mock_llm(context_ids):
        # A simple, deterministic function for demonstration.
        # It calculates the next token based on the sum of previous tokens.
        if not context_ids:
            return 1  # Always start with token 1 if context is empty
        return (sum(context_ids) * 3 + 7) % 100 # A pseudo-random but deterministic rule

    # In this special case, the draft model and the target model are the same.
    draft_model = mock_llm
    target_model = mock_llm

    # --- Simulation Parameters ---
    initial_context = [42, 15, 8]
    draft_length = 5  # The number of tokens the draft model will generate

    print("--- Sanity Check for Speculative Decoding ---")
    print(f"Scenario: Draft Model == Target Model")
    print(f"Initial Context: {initial_context}")
    print(f"Draft Length (k): {draft_length}\n")

    # 1. DRAFTING PHASE
    # The draft model generates k new tokens autoregressively.
    print("--- 1. Drafting Phase ---")
    draft_tokens = []
    current_context = initial_context.copy()
    for i in range(draft_length):
        next_token = draft_model(current_context)
        draft_tokens.append(next_token)
        current_context.append(next_token)
        print(f"Draft step {i+1}: Generated token '{next_token}'")
    
    print(f"\nCompleted Draft: {draft_tokens}\n")

    # 2. VERIFICATION PHASE
    # The target model verifies each token from the draft.
    print("--- 2. Verification Phase ---")
    accepted_count = 0
    for i in range(draft_length):
        # The context used to verify the i-th token from the draft
        verification_context = initial_context + draft_tokens[:i]
        
        # What the target model would have predicted at this position
        target_prediction = target_model(verification_context)
        
        # The actual token generated by the draft model
        drafted_token = draft_tokens[i]

        print(f"Verifying token {i+1} ('{drafted_token}'):")
        print(f"  - Target model prediction with context {verification_context}: {target_prediction}")
        
        # Since draft_model and target_model are the same function,
        # target_prediction will ALWAYS equal drafted_token.
        if target_prediction == drafted_token:
            print("  - Result: MATCH. Token is accepted.")
            accepted_count += 1
        else:
            # This branch is unreachable in this scenario
            print("  - Result: MISMATCH. Token is rejected.")
            break 

    # 3. CALCULATE ACCEPTANCE RATE
    # The acceptance rate is the ratio of accepted tokens to total drafted tokens.
    print("\n--- 3. Final Result ---")
    print("The acceptance rate is the number of accepted tokens divided by the number of drafted tokens.")
    
    acceptance_rate = accepted_count / draft_length
    
    # Final equation as requested
    print(f"{accepted_count} / {draft_length} = {acceptance_rate}")

speculative_decoding_simulation()