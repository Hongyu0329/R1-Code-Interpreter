{
  "question": "What is the upper bound of the circuit complexity class for the formal languages that average-hard-attention saturated transformers with float activations can recognize?\nOnce you feel you are ready for the final answer, directly return the answer with the format <<<answer content>>> at the end of your response, e.g. <<<C>>>, <<<9.8>>>.",
  "response_list": [
    "Research into the theoretical capabilities of transformer neural networks has led to their classification within the framework of circuit complexity, which categorizes formal languages based on the size and depth of Boolean circuits required to recognize them.\n\nInitially, studies of transformers with a simplified \"hard attention\" mechanism, where each attention head focuses on a single input position, found their computational power to be quite limited. These models, including Unique Hard Attention (UHAT) and Generalized Unique Hard Attention (GUHAT), can only recognize formal languages within the complexity class **AC⁰**. AC⁰ consists of languages recognizable by families of Boolean circuits with constant depth and polynomial size, using unbounded fan-in AND and OR gates.\n\nHowever, the \"average-hard-attention\" or \"saturated attention\" mechanism is a more generalized and practically relevant model. In this model, attention is uniformly distributed over all input positions that share the maximum attention score. This allows for capabilities like counting, which are observed in practical transformer models.\n\nA key finding by Merrill, Sabharwal, and Smith (2022) establishes the upper bound for these more powerful transformers. Their work demonstrates that saturated transformers using floating-point values for activations can be simulated by constant-depth threshold circuits. This places the formal languages they can recognize in the circuit complexity class **TC⁰**.\n\nThe class TC⁰ includes problems decidable by constant-depth, polynomial-size circuit families with not only AND and OR gates but also majority gates. It is a strictly larger class than AC⁰, and includes languages like MAJORITY, which AC⁰ cannot recognize. This indicates that transformers with average-hard (saturated) attention are computationally more powerful than those restricted to simple hard attention. Subsequent work has also shown that assuming logarithmic precision for the transformer's internal computations leads to an upper bound of uniform TC⁰.\n\nTherefore, the upper bound of the circuit complexity class for formal languages that can be recognized by average-hard-attention saturated transformers with float activations is TC⁰.\n\n<<<TC⁰>>>"
  ],
  "user_prompt_list": [
    "What is the upper bound of the circuit complexity class for the formal languages that average-hard-attention saturated transformers with float activations can recognize?\nOnce you feel you are ready for the final answer, directly return the answer with the format <<<answer content>>> at the end of your response, e.g. <<<C>>>, <<<9.8>>>."
  ],
  "system_message": ""
}