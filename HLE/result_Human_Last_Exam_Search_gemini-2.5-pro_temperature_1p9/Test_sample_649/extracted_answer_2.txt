The tightest lower bound on the minimax risk \(R_n^*\) is given by Fano's method. The bound is obtained by selecting a finite family of distributions and applying Fano's inequality. To get the tightest possible bound from this method, one must optimize over all such families. The resulting lower bound is:

\[ R_n^* \ge \sup_{N \ge 2, \{P_1, \dots, P_N\} \subset \mathcal{P}} \left[ \Phi\left(\frac{\delta}{2}\right) \left( 1 - \frac{I_n(\{P_j\}) + \log 2}{\log N} \right) \right] \]

where the supremum is taken over all integers \( N \ge 2 \) and all subsets of distributions \( \{P_1, \dots, P_N\} \) from \( \mathcal{P} \). The terms in the bound are defined as:
*   \( \delta = \min_{j \neq k} \rho(\theta(P_j), \theta(P_k)) \) is the minimum pairwise distance between the parameters of the chosen distributions.
*   \( \Phi \) is the non-decreasing function from the loss definition.
*   \( I_n(\{P_j\}) \) is the mutual information between the index of the true distribution \(J \in \{1, \dots, N\}\) (chosen uniformly at random) and the \(n\) i.i.d. samples \( S \sim P_J^n \). It is given by:
    \[ I_n(\{P_j\}) = \frac{1}{N} \sum_{j=1}^N D_{KL}(P_j^n \middle\| \frac{1}{N}\sum_{k=1}^N P_k^n) \]
    This mutual information can be further upper-bounded, since the data are i.i.d., by the maximum pairwise KL divergence between the single-sample distributions:
    \[ I_n(\{P_j\}) \le n \cdot \max_{j,k \in \{1, \dots, N\}} D_{KL}(P_j \| P_k) \]

This general formula allows one to compute a concrete numerical lower bound for specific families of distributions \( \mathcal{P} \) by making a judicious choice of \( \{P_j\} \) to maximize the right-hand side.