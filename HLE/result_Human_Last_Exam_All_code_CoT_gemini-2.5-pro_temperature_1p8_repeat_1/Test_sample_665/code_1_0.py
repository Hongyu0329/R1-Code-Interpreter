import numpy as np
import lime
import lime.lime_tabular
import warnings

# Suppress a pending deprecation warning from the LIME library
warnings.filterwarnings("ignore", category=DeprecationWarning) 

def solve():
    """
    This script sets up a LIME explanation scenario to determine feature importances
    for a given model and two different input instances (explicands).
    """
    
    # 1. Define the Model
    lookup_table = {1.0: 1.0, 0.0: 0.0}

    def f_raw(input1, input2):
        """The original model function on scalar inputs."""
        # The logic is: if input1 is 0 or 1, the output depends only on input1.
        # Otherwise, the output depends only on input2.
        return lookup_table.get(input1, input1 * 0 + input2 * 0.5 + 0.5)

    def predict_fn(X):
        """
        LIME-compatible prediction function.
        X is a numpy array of shape (n_samples, n_features).
        Returns a numpy array of shape (n_samples, 1).
        """
        outputs = np.array([f_raw(row[0], row[1]) for row in X])
        return outputs.reshape(-1, 1)

    # 2. Set up LIME Explainer
    # We interpret "baseline dataset is the same as the lookup table" by creating
    # training data where input1 values are keys from the lookup table.
    training_data = np.array([
        [0.0, 0.0],
        [0.0, 1.0],
        [1.0, 0.0],
        [1.0, 1.0]
    ])
    
    feature_names = ['input1', 'input2']
    
    # Using default hyperparameters, which includes `discretize_continuous=True`.
    # This means perturbations are generated by sampling from the training data.
    explainer = lime.lime_tabular.LimeTabularExplainer(
        training_data=training_data,
        feature_names=feature_names,
        class_names=['output'],
        mode='regression',
        verbose=False,
        random_state=42 # for reproducibility
    )

    print("Analyzing feature importances using LIME...\n")

    # 3. Case i: E = (0.0, 0.0)
    print("--- Case i) Explicand E = (0.0, 0.0) ---")
    E1 = np.array([0.0, 0.0])
    
    explanation1 = explainer.explain_instance(
        data_row=E1,
        predict_fn=predict_fn,
        num_features=2,
        num_samples=5000
    )
    
    print(f"LIME explanation for E1=(0.0, 0.0):")
    explanation_list1 = explanation1.as_list()
    # Extract feature names and their absolute weights
    importances1 = {item[0]: abs(item[1]) for item in explanation_list1}
    # The feature name in the output might be a condition, we parse it
    feature1_imp = importances1.get('input1 <= 0.00', importances1.get('0.00 < input1', 0))
    feature2_imp = importances1.get('input2 <= 0.00', importances1.get('0.00 < input2', 0))

    # The local model equation LIME builds is: y = intercept + sum(weights)
    print(f"Prediction(local) = {explanation1.local_pred[0]:.4f}")
    print(f"Prediction(model) = {predict_fn(E1.reshape(1,-1))[0][0]:.4f}")
    print(f"Intercept = {explanation1.intercept[0]:.4f}")
    for feature, weight in explanation_list1:
        print(f"Feature: {feature}, Weight: {weight:.4f}")

    if feature1_imp > feature2_imp:
        print("\nConclusion for Case i): 'input1' is more important.\n")
    else:
        print("\nConclusion for Case i): 'input2' is more important.\n")


    # 4. Case ii: E = (-1.0, -1.0)
    print("--- Case ii) Explicand E = (-1.0, -1.0) ---")
    E2 = np.array([-1.0, -1.0])
    
    explanation2 = explainer.explain_instance(
        data_row=E2,
        predict_fn=predict_fn,
        num_features=2,
        num_samples=5000
    )
    
    print(f"LIME explanation for E2=(-1.0, -1.0):")
    explanation_list2 = explanation2.as_list()
    # Extract feature names and their absolute weights
    importances2 = {item[0]: abs(item[1]) for item in explanation_list2}
    # The feature name might be a range condition. Find the corresponding feature.
    imp2_feat1 = 0
    imp2_feat2 = 0
    for feature, weight in importances2.items():
        if 'input1' in feature:
            imp2_feat1 = max(imp2_feat1, weight)
        if 'input2' in feature:
            imp2_feat2 = max(imp2_feat2, weight)
            
    # The local model equation LIME builds is: y = intercept + sum(weights)
    print(f"Prediction(local) = {explanation2.local_pred[0]:.4f}")
    print(f"Prediction(model) = {predict_fn(E2.reshape(1,-1))[0][0]:.4f}")
    print(f"Intercept = {explanation2.intercept[0]:.4f}")
    for feature, weight in explanation_list2:
        print(f"Feature: {feature}, Weight: {weight:.4f}")

    if imp2_feat1 > imp2_feat2:
        print("\nConclusion for Case ii): 'input1' is more important.")
    else:
        print("\nConclusion for Case ii): 'input2' is more important.")


solve()