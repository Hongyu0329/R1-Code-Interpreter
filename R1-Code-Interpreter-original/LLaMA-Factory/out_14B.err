W0503 13:01:00.642000 237574 site-packages/torch/distributed/run.py:792] 
W0503 13:01:00.642000 237574 site-packages/torch/distributed/run.py:792] *****************************************
W0503 13:01:00.642000 237574 site-packages/torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0503 13:01:00.642000 237574 site-packages/torch/distributed/run.py:792] *****************************************
[INFO|tokenization_utils_base.py:2060] 2025-05-03 13:01:08,995 >> loading file vocab.json from cache at /proj/long-multi/kqian/hf_cache/hub/models--Qwen--Qwen2.5-14B-Instruct-1M/snapshots/620fad32de7bdd2293b3d99b39eba2fe63e97438/vocab.json
[INFO|tokenization_utils_base.py:2060] 2025-05-03 13:01:08,995 >> loading file merges.txt from cache at /proj/long-multi/kqian/hf_cache/hub/models--Qwen--Qwen2.5-14B-Instruct-1M/snapshots/620fad32de7bdd2293b3d99b39eba2fe63e97438/merges.txt
[INFO|tokenization_utils_base.py:2060] 2025-05-03 13:01:08,995 >> loading file tokenizer.json from cache at /proj/long-multi/kqian/hf_cache/hub/models--Qwen--Qwen2.5-14B-Instruct-1M/snapshots/620fad32de7bdd2293b3d99b39eba2fe63e97438/tokenizer.json
[INFO|tokenization_utils_base.py:2060] 2025-05-03 13:01:08,995 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2060] 2025-05-03 13:01:08,995 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2060] 2025-05-03 13:01:08,995 >> loading file tokenizer_config.json from cache at /proj/long-multi/kqian/hf_cache/hub/models--Qwen--Qwen2.5-14B-Instruct-1M/snapshots/620fad32de7bdd2293b3d99b39eba2fe63e97438/tokenizer_config.json
[INFO|tokenization_utils_base.py:2060] 2025-05-03 13:01:08,995 >> loading file chat_template.jinja from cache at None
[INFO|tokenization_utils_base.py:2323] 2025-05-03 13:01:09,228 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:699] 2025-05-03 13:01:09,854 >> loading configuration file config.json from cache at /proj/long-multi/kqian/hf_cache/hub/models--Qwen--Qwen2.5-14B-Instruct-1M/snapshots/620fad32de7bdd2293b3d99b39eba2fe63e97438/config.json
[INFO|configuration_utils.py:771] 2025-05-03 13:01:09,856 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dual_chunk_attention_config": {
    "chunk_size": 262144,
    "local_size": 8192,
    "original_max_position_embeddings": 262144
  },
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "initializer_range": 0.02,
  "intermediate_size": 13824,
  "max_position_embeddings": 1010000,
  "max_window_layers": 48,
  "model_type": "qwen2",
  "num_attention_heads": 40,
  "num_hidden_layers": 48,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000000.0,
  "sliding_window": 1010000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.50.0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2060] 2025-05-03 13:01:09,903 >> loading file vocab.json from cache at /proj/long-multi/kqian/hf_cache/hub/models--Qwen--Qwen2.5-14B-Instruct-1M/snapshots/620fad32de7bdd2293b3d99b39eba2fe63e97438/vocab.json
[INFO|tokenization_utils_base.py:2060] 2025-05-03 13:01:09,903 >> loading file merges.txt from cache at /proj/long-multi/kqian/hf_cache/hub/models--Qwen--Qwen2.5-14B-Instruct-1M/snapshots/620fad32de7bdd2293b3d99b39eba2fe63e97438/merges.txt
[INFO|tokenization_utils_base.py:2060] 2025-05-03 13:01:09,903 >> loading file tokenizer.json from cache at /proj/long-multi/kqian/hf_cache/hub/models--Qwen--Qwen2.5-14B-Instruct-1M/snapshots/620fad32de7bdd2293b3d99b39eba2fe63e97438/tokenizer.json
[INFO|tokenization_utils_base.py:2060] 2025-05-03 13:01:09,905 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2060] 2025-05-03 13:01:09,905 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2060] 2025-05-03 13:01:09,905 >> loading file tokenizer_config.json from cache at /proj/long-multi/kqian/hf_cache/hub/models--Qwen--Qwen2.5-14B-Instruct-1M/snapshots/620fad32de7bdd2293b3d99b39eba2fe63e97438/tokenizer_config.json
[INFO|tokenization_utils_base.py:2060] 2025-05-03 13:01:09,905 >> loading file chat_template.jinja from cache at None
[INFO|tokenization_utils_base.py:2323] 2025-05-03 13:01:10,105 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Converting format of dataset (num_proc=16):   0%|          | 0/7564 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16):  29%|██▉       | 2194/7564 [00:00<00:00, 21934.22 examples/s]Converting format of dataset (num_proc=16): 100%|██████████| 7564/7564 [00:00<00:00, 31986.24 examples/s]
[rank0]:[W503 13:01:11.905966428 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank3]:[W503 13:01:11.630714658 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank4]:[W503 13:01:11.631237299 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 4]  using GPU 4 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank7]:[W503 13:01:11.645937231 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 7]  using GPU 7 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank5]:[W503 13:01:11.672386654 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 5]  using GPU 5 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank6]:[W503 13:01:11.677992542 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 6]  using GPU 6 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank1]:[W503 13:01:11.777307178 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank2]:[W503 13:01:11.824388173 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Running tokenizer on dataset (num_proc=16):   0%|          | 0/7564 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   6%|▋         | 473/7564 [00:01<00:21, 336.60 examples/s]Running tokenizer on dataset (num_proc=16):  13%|█▎        | 946/7564 [00:01<00:08, 741.24 examples/s]Running tokenizer on dataset (num_proc=16):  38%|███▊      | 2838/7564 [00:01<00:01, 2546.95 examples/s]Running tokenizer on dataset (num_proc=16):  50%|█████     | 3784/7564 [00:01<00:01, 3219.28 examples/s]Running tokenizer on dataset (num_proc=16):  63%|██████▎   | 4730/7564 [00:01<00:00, 4088.51 examples/s]Running tokenizer on dataset (num_proc=16):  75%|███████▌  | 5676/7564 [00:02<00:00, 4847.79 examples/s]Running tokenizer on dataset (num_proc=16):  94%|█████████▍| 7092/7564 [00:02<00:00, 5750.72 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 7564/7564 [00:02<00:00, 3144.54 examples/s]
[INFO|configuration_utils.py:699] 2025-05-03 13:01:19,616 >> loading configuration file config.json from cache at /proj/long-multi/kqian/hf_cache/hub/models--Qwen--Qwen2.5-14B-Instruct-1M/snapshots/620fad32de7bdd2293b3d99b39eba2fe63e97438/config.json
[INFO|configuration_utils.py:771] 2025-05-03 13:01:19,617 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dual_chunk_attention_config": {
    "chunk_size": 262144,
    "local_size": 8192,
    "original_max_position_embeddings": 262144
  },
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "initializer_range": 0.02,
  "intermediate_size": 13824,
  "max_position_embeddings": 1010000,
  "max_window_layers": 48,
  "model_type": "qwen2",
  "num_attention_heads": 40,
  "num_hidden_layers": 48,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000000.0,
  "sliding_window": 1010000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.50.0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|modeling_utils.py:1154] 2025-05-03 13:01:19,757 >> loading weights file model.safetensors from cache at /proj/long-multi/kqian/hf_cache/hub/models--Qwen--Qwen2.5-14B-Instruct-1M/snapshots/620fad32de7bdd2293b3d99b39eba2fe63e97438/model.safetensors.index.json
[INFO|modeling_utils.py:3747] 2025-05-03 13:01:19,762 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|configuration_utils.py:1139] 2025-05-03 13:01:19,770 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "use_cache": false
}

[WARNING|logging.py:329] 2025-05-03 13:01:19,792 >> Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:03<00:27,  3.90s/it]Loading checkpoint shards:  12%|█▎        | 1/8 [00:03<00:27,  3.90s/it]Loading checkpoint shards:  12%|█▎        | 1/8 [00:03<00:27,  3.90s/it]Loading checkpoint shards:  12%|█▎        | 1/8 [00:03<00:27,  3.90s/it]Loading checkpoint shards:  12%|█▎        | 1/8 [00:03<00:27,  3.90s/it]Loading checkpoint shards:  12%|█▎        | 1/8 [00:03<00:27,  3.90s/it]Loading checkpoint shards:  12%|█▎        | 1/8 [00:03<00:27,  3.90s/it]Loading checkpoint shards:  12%|█▎        | 1/8 [00:04<00:28,  4.10s/it]Loading checkpoint shards:  25%|██▌       | 2/8 [00:07<00:23,  3.85s/it]Loading checkpoint shards:  25%|██▌       | 2/8 [00:07<00:23,  3.85s/it]Loading checkpoint shards:  25%|██▌       | 2/8 [00:07<00:23,  3.85s/it]Loading checkpoint shards:  25%|██▌       | 2/8 [00:07<00:23,  3.85s/it]Loading checkpoint shards:  25%|██▌       | 2/8 [00:07<00:23,  3.85s/it]Loading checkpoint shards:  25%|██▌       | 2/8 [00:07<00:23,  3.85s/it]Loading checkpoint shards:  25%|██▌       | 2/8 [00:07<00:23,  3.85s/it]Loading checkpoint shards:  25%|██▌       | 2/8 [00:08<00:24,  4.02s/it]Loading checkpoint shards:  38%|███▊      | 3/8 [00:11<00:18,  3.79s/it]Loading checkpoint shards:  38%|███▊      | 3/8 [00:11<00:18,  3.79s/it]Loading checkpoint shards:  38%|███▊      | 3/8 [00:11<00:18,  3.79s/it]Loading checkpoint shards:  38%|███▊      | 3/8 [00:11<00:18,  3.79s/it]Loading checkpoint shards:  38%|███▊      | 3/8 [00:11<00:18,  3.79s/it]Loading checkpoint shards:  38%|███▊      | 3/8 [00:11<00:18,  3.79s/it]Loading checkpoint shards:  38%|███▊      | 3/8 [00:11<00:18,  3.79s/it]Loading checkpoint shards:  38%|███▊      | 3/8 [00:11<00:19,  3.92s/it]Loading checkpoint shards:  50%|█████     | 4/8 [00:15<00:15,  3.78s/it]Loading checkpoint shards:  50%|█████     | 4/8 [00:15<00:15,  3.78s/it]Loading checkpoint shards:  50%|█████     | 4/8 [00:15<00:15,  3.78s/it]Loading checkpoint shards:  50%|█████     | 4/8 [00:15<00:15,  3.78s/it]Loading checkpoint shards:  50%|█████     | 4/8 [00:15<00:15,  3.78s/it]Loading checkpoint shards:  50%|█████     | 4/8 [00:15<00:15,  3.78s/it]Loading checkpoint shards:  50%|█████     | 4/8 [00:15<00:15,  3.78s/it]Loading checkpoint shards:  50%|█████     | 4/8 [00:15<00:15,  3.91s/it]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:19<00:11,  3.93s/it]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:19<00:11,  3.93s/it]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:19<00:11,  3.93s/it]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:19<00:11,  3.93s/it]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:19<00:11,  3.93s/it]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:19<00:11,  3.93s/it]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:19<00:11,  3.93s/it]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:19<00:11,  3.86s/it]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:23<00:07,  3.82s/it]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:23<00:07,  3.82s/it]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:23<00:07,  3.82s/it]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:23<00:07,  3.82s/it]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:23<00:07,  3.82s/it]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:23<00:07,  3.82s/it]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:23<00:07,  3.82s/it]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:23<00:07,  3.82s/it]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:26<00:03,  3.76s/it]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:26<00:03,  3.76s/it]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:26<00:03,  3.76s/it]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:26<00:03,  3.76s/it]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:26<00:03,  3.76s/it]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:26<00:03,  3.76s/it]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:26<00:03,  3.76s/it]Loading checkpoint shards: 100%|██████████| 8/8 [00:26<00:00,  3.33s/it]
Loading checkpoint shards: 100%|██████████| 8/8 [00:26<00:00,  3.33s/it]Loading checkpoint shards: 100%|██████████| 8/8 [00:26<00:00,  3.33s/it]
Loading checkpoint shards: 100%|██████████| 8/8 [00:26<00:00,  3.33s/it]
Loading checkpoint shards: 100%|██████████| 8/8 [00:26<00:00,  3.33s/it]

Loading checkpoint shards: 100%|██████████| 8/8 [00:26<00:00,  3.33s/it]
Loading checkpoint shards: 100%|██████████| 8/8 [00:26<00:00,  3.33s/it]
Loading checkpoint shards:  88%|████████▊ | 7/8 [00:27<00:03,  3.80s/it]Loading checkpoint shards: 100%|██████████| 8/8 [00:28<00:00,  3.20s/it]Loading checkpoint shards: 100%|██████████| 8/8 [00:28<00:00,  3.62s/it]
[INFO|modeling_utils.py:4987] 2025-05-03 13:01:48,989 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.

[INFO|modeling_utils.py:4995] 2025-05-03 13:01:48,989 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at Qwen/Qwen2.5-14B-Instruct-1M.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.
[INFO|configuration_utils.py:1094] 2025-05-03 13:01:49,132 >> loading configuration file generation_config.json from cache at /proj/long-multi/kqian/hf_cache/hub/models--Qwen--Qwen2.5-14B-Instruct-1M/snapshots/620fad32de7bdd2293b3d99b39eba2fe63e97438/generation_config.json
[INFO|configuration_utils.py:1139] 2025-05-03 13:01:49,133 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "repetition_penalty": 1.05,
  "temperature": 0.7,
  "top_k": 20,
  "top_p": 0.8
}

[INFO|trainer.py:748] 2025-05-03 13:01:49,156 >> Using auto half precision backend
[INFO|trainer.py:2409] 2025-05-03 13:01:52,564 >> ***** Running training *****
[INFO|trainer.py:2410] 2025-05-03 13:01:52,564 >>   Num examples = 6,807
[INFO|trainer.py:2411] 2025-05-03 13:01:52,564 >>   Num Epochs = 3
[INFO|trainer.py:2412] 2025-05-03 13:01:52,564 >>   Instantaneous batch size per device = 5
[INFO|trainer.py:2415] 2025-05-03 13:01:52,564 >>   Total train batch size (w. parallel, distributed & accumulation) = 80
[INFO|trainer.py:2416] 2025-05-03 13:01:52,564 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:2417] 2025-05-03 13:01:52,564 >>   Total optimization steps = 255
[INFO|trainer.py:2418] 2025-05-03 13:01:52,565 >>   Number of trainable parameters = 14,770,033,664
  0%|          | 0/255 [00:00<?, ?it/s]  0%|          | 1/255 [00:10<44:17, 10.46s/it]  1%|          | 2/255 [00:17<34:52,  8.27s/it]  1%|          | 3/255 [00:24<32:19,  7.69s/it]  2%|▏         | 4/255 [00:31<30:54,  7.39s/it]  2%|▏         | 5/255 [00:37<29:56,  7.19s/it]  2%|▏         | 6/255 [00:44<29:21,  7.08s/it]  3%|▎         | 7/255 [00:51<28:52,  6.99s/it]  3%|▎         | 8/255 [00:58<28:28,  6.92s/it]  4%|▎         | 9/255 [01:05<28:05,  6.85s/it]  4%|▍         | 10/255 [01:12<28:03,  6.87s/it][INFO|trainer.py:4289] 2025-05-03 13:03:04,587 >> 
***** Running Evaluation *****
[INFO|trainer.py:4291] 2025-05-03 13:03:04,587 >>   Num examples = 757
[INFO|trainer.py:4294] 2025-05-03 13:03:04,587 >>   Batch size = 1

  0%|          | 0/95 [00:00<?, ?it/s][A
  2%|▏         | 2/95 [00:00<00:25,  3.60it/s][A
  3%|▎         | 3/95 [00:00<00:24,  3.74it/s][A
  4%|▍         | 4/95 [00:01<00:23,  3.81it/s][A
  5%|▌         | 5/95 [00:01<00:23,  3.88it/s][A
  6%|▋         | 6/95 [00:01<00:22,  3.91it/s][A
  7%|▋         | 7/95 [00:01<00:22,  3.94it/s][A
  8%|▊         | 8/95 [00:02<00:22,  3.93it/s][A
  9%|▉         | 9/95 [00:02<00:21,  3.95it/s][A
 11%|█         | 10/95 [00:02<00:21,  3.97it/s][A
 12%|█▏        | 11/95 [00:02<00:21,  3.98it/s][A
 13%|█▎        | 12/95 [00:03<00:20,  3.98it/s][A
 14%|█▎        | 13/95 [00:03<00:20,  3.98it/s][A
 15%|█▍        | 14/95 [00:03<00:20,  3.98it/s][A
 16%|█▌        | 15/95 [00:03<00:20,  3.97it/s][A
 17%|█▋        | 16/95 [00:04<00:19,  3.97it/s][A
 18%|█▊        | 17/95 [00:04<00:19,  3.97it/s][A
 19%|█▉        | 18/95 [00:04<00:19,  3.97it/s][A
 20%|██        | 19/95 [00:04<00:19,  3.99it/s][A
 21%|██        | 20/95 [00:05<00:18,  3.99it/s][A
 22%|██▏       | 21/95 [00:05<00:18,  3.98it/s][A
 23%|██▎       | 22/95 [00:05<00:18,  3.98it/s][A
 24%|██▍       | 23/95 [00:05<00:18,  3.98it/s][A
 25%|██▌       | 24/95 [00:06<00:17,  3.99it/s][A
 26%|██▋       | 25/95 [00:06<00:17,  4.00it/s][A
 27%|██▋       | 26/95 [00:06<00:17,  3.98it/s][A
 28%|██▊       | 27/95 [00:06<00:17,  3.98it/s][A
 29%|██▉       | 28/95 [00:07<00:16,  3.99it/s][A
 31%|███       | 29/95 [00:07<00:16,  3.98it/s][A
 32%|███▏      | 30/95 [00:07<00:16,  3.99it/s][A
 33%|███▎      | 31/95 [00:07<00:16,  3.99it/s][A
 34%|███▎      | 32/95 [00:08<00:15,  4.00it/s][A
 35%|███▍      | 33/95 [00:08<00:15,  4.00it/s][A
 36%|███▌      | 34/95 [00:08<00:15,  4.01it/s][A
 37%|███▋      | 35/95 [00:08<00:14,  4.01it/s][A
 38%|███▊      | 36/95 [00:09<00:14,  4.00it/s][A
 39%|███▉      | 37/95 [00:09<00:14,  4.00it/s][A
 40%|████      | 38/95 [00:09<00:14,  3.98it/s][A
 41%|████      | 39/95 [00:09<00:14,  3.99it/s][A
 42%|████▏     | 40/95 [00:10<00:13,  4.00it/s][A
 43%|████▎     | 41/95 [00:10<00:13,  4.01it/s][A
 44%|████▍     | 42/95 [00:10<00:13,  4.01it/s][A
 45%|████▌     | 43/95 [00:10<00:12,  4.02it/s][A
 46%|████▋     | 44/95 [00:11<00:12,  4.02it/s][A
 47%|████▋     | 45/95 [00:11<00:12,  4.02it/s][A
 48%|████▊     | 46/95 [00:11<00:12,  4.02it/s][A
 49%|████▉     | 47/95 [00:11<00:11,  4.03it/s][A
 51%|█████     | 48/95 [00:12<00:11,  4.02it/s][A
 52%|█████▏    | 49/95 [00:12<00:11,  4.03it/s][A
 53%|█████▎    | 50/95 [00:12<00:11,  4.02it/s][A
 54%|█████▎    | 51/95 [00:12<00:10,  4.01it/s][A
 55%|█████▍    | 52/95 [00:13<00:10,  4.01it/s][A
 56%|█████▌    | 53/95 [00:13<00:10,  4.01it/s][A
 57%|█████▋    | 54/95 [00:13<00:10,  4.01it/s][A
 58%|█████▊    | 55/95 [00:13<00:09,  4.01it/s][A
 59%|█████▉    | 56/95 [00:14<00:09,  4.01it/s][A
 60%|██████    | 57/95 [00:14<00:09,  4.02it/s][A
 61%|██████    | 58/95 [00:14<00:09,  4.00it/s][A
 62%|██████▏   | 59/95 [00:14<00:09,  3.99it/s][A
 63%|██████▎   | 60/95 [00:15<00:08,  3.99it/s][A
 64%|██████▍   | 61/95 [00:15<00:08,  3.98it/s][A
 65%|██████▌   | 62/95 [00:15<00:08,  3.98it/s][A
 66%|██████▋   | 63/95 [00:15<00:08,  3.98it/s][A
 67%|██████▋   | 64/95 [00:16<00:07,  3.98it/s][A
 68%|██████▊   | 65/95 [00:16<00:07,  3.98it/s][A
 69%|██████▉   | 66/95 [00:16<00:07,  3.99it/s][A
 71%|███████   | 67/95 [00:16<00:07,  3.99it/s][A
 72%|███████▏  | 68/95 [00:17<00:06,  4.00it/s][A
 73%|███████▎  | 69/95 [00:17<00:06,  4.00it/s][A
 74%|███████▎  | 70/95 [00:17<00:06,  4.01it/s][A
 75%|███████▍  | 71/95 [00:17<00:05,  4.01it/s][A
 76%|███████▌  | 72/95 [00:18<00:05,  4.00it/s][A
 77%|███████▋  | 73/95 [00:18<00:05,  3.99it/s][A
 78%|███████▊  | 74/95 [00:18<00:05,  3.99it/s][A
 79%|███████▉  | 75/95 [00:18<00:05,  3.98it/s][A
 80%|████████  | 76/95 [00:19<00:04,  3.99it/s][A
 81%|████████  | 77/95 [00:19<00:04,  3.98it/s][A
 82%|████████▏ | 78/95 [00:19<00:04,  3.98it/s][A
 83%|████████▎ | 79/95 [00:19<00:04,  3.98it/s][A
 84%|████████▍ | 80/95 [00:20<00:03,  4.00it/s][A
 85%|████████▌ | 81/95 [00:20<00:03,  4.02it/s][A
 86%|████████▋ | 82/95 [00:20<00:03,  4.03it/s][A
 87%|████████▋ | 83/95 [00:20<00:02,  4.04it/s][A
 88%|████████▊ | 84/95 [00:21<00:02,  4.02it/s][A
 89%|████████▉ | 85/95 [00:21<00:02,  4.01it/s][A
 91%|█████████ | 86/95 [00:21<00:02,  4.00it/s][A
 92%|█████████▏| 87/95 [00:21<00:02,  3.99it/s][A
 93%|█████████▎| 88/95 [00:22<00:01,  3.98it/s][A
 94%|█████████▎| 89/95 [00:22<00:01,  4.00it/s][A
 95%|█████████▍| 90/95 [00:22<00:01,  4.02it/s][A
 96%|█████████▌| 91/95 [00:22<00:00,  4.02it/s][A
 97%|█████████▋| 92/95 [00:23<00:00,  4.00it/s][A
 98%|█████████▊| 93/95 [00:23<00:00,  4.01it/s][A
 99%|█████████▉| 94/95 [00:23<00:00,  4.01it/s][A
100%|██████████| 95/95 [00:23<00:00,  4.02it/s][A                                                
                                               [A  4%|▍         | 10/255 [01:36<28:03,  6.87s/it]
100%|██████████| 95/95 [00:23<00:00,  4.02it/s][A
                                               [A  4%|▍         | 11/255 [01:42<57:42, 14.19s/it]  5%|▍         | 12/255 [01:49<48:11, 11.90s/it]  5%|▌         | 13/255 [01:56<41:32, 10.30s/it]  5%|▌         | 14/255 [02:02<36:53,  9.18s/it]  6%|▌         | 15/255 [02:09<34:04,  8.52s/it]  6%|▋         | 16/255 [02:16<31:50,  7.99s/it]  7%|▋         | 17/255 [02:23<30:20,  7.65s/it]  7%|▋         | 18/255 [02:30<29:11,  7.39s/it]  7%|▋         | 19/255 [02:37<28:32,  7.26s/it]  8%|▊         | 20/255 [02:43<27:56,  7.13s/it]                                                  8%|▊         | 20/255 [02:43<27:56,  7.13s/it][INFO|trainer.py:4289] 2025-05-03 13:04:36,430 >> 
***** Running Evaluation *****
[INFO|trainer.py:4291] 2025-05-03 13:04:36,431 >>   Num examples = 757
[INFO|trainer.py:4294] 2025-05-03 13:04:36,431 >>   Batch size = 1

  0%|          | 0/95 [00:00<?, ?it/s][A
  2%|▏         | 2/95 [00:00<00:11,  7.99it/s][A
  3%|▎         | 3/95 [00:00<00:16,  5.66it/s][A
  4%|▍         | 4/95 [00:00<00:18,  4.88it/s][A
  5%|▌         | 5/95 [00:00<00:19,  4.54it/s][A
  6%|▋         | 6/95 [00:01<00:20,  4.33it/s][A
  7%|▋         | 7/95 [00:01<00:20,  4.20it/s][A
  8%|▊         | 8/95 [00:01<00:21,  4.12it/s][A
  9%|▉         | 9/95 [00:02<00:21,  4.09it/s][A
 11%|█         | 10/95 [00:02<00:20,  4.05it/s][A
 12%|█▏        | 11/95 [00:02<00:20,  4.04it/s][A
 13%|█▎        | 12/95 [00:02<00:20,  4.04it/s][A
 14%|█▎        | 13/95 [00:03<00:20,  4.04it/s][A
 15%|█▍        | 14/95 [00:03<00:20,  4.03it/s][A
 16%|█▌        | 15/95 [00:03<00:19,  4.00it/s][A
 17%|█▋        | 16/95 [00:03<00:19,  4.00it/s][A
 18%|█▊        | 17/95 [00:04<00:19,  3.99it/s][A
 19%|█▉        | 18/95 [00:04<00:19,  3.99it/s][A
 20%|██        | 19/95 [00:04<00:18,  4.01it/s][A
 21%|██        | 20/95 [00:04<00:18,  4.00it/s][A
 22%|██▏       | 21/95 [00:05<00:18,  3.99it/s][A
 23%|██▎       | 22/95 [00:05<00:18,  3.99it/s][A
 24%|██▍       | 23/95 [00:05<00:18,  3.99it/s][A
 25%|██▌       | 24/95 [00:05<00:17,  3.99it/s][A
 26%|██▋       | 25/95 [00:06<00:17,  4.00it/s][A
 27%|██▋       | 26/95 [00:06<00:17,  3.98it/s][A
 28%|██▊       | 27/95 [00:06<00:17,  3.98it/s][A
 29%|██▉       | 28/95 [00:06<00:16,  3.99it/s][A
 31%|███       | 29/95 [00:07<00:16,  3.98it/s][A
 32%|███▏      | 30/95 [00:07<00:16,  3.99it/s][A
 33%|███▎      | 31/95 [00:07<00:16,  4.00it/s][A
 34%|███▎      | 32/95 [00:07<00:15,  3.99it/s][A
 35%|███▍      | 33/95 [00:08<00:15,  3.99it/s][A
 36%|███▌      | 34/95 [00:08<00:15,  4.01it/s][A
 37%|███▋      | 35/95 [00:08<00:14,  4.02it/s][A
 38%|███▊      | 36/95 [00:08<00:14,  4.00it/s][A
 39%|███▉      | 37/95 [00:09<00:14,  3.99it/s][A
 40%|████      | 38/95 [00:09<00:14,  3.99it/s][A
 41%|████      | 39/95 [00:09<00:14,  4.00it/s][A
 42%|████▏     | 40/95 [00:09<00:13,  4.01it/s][A
 43%|████▎     | 41/95 [00:10<00:13,  4.02it/s][A
 44%|████▍     | 42/95 [00:10<00:13,  4.02it/s][A
 45%|████▌     | 43/95 [00:10<00:12,  4.02it/s][A
 46%|████▋     | 44/95 [00:10<00:12,  4.03it/s][A
 47%|████▋     | 45/95 [00:11<00:12,  4.03it/s][A
 48%|████▊     | 46/95 [00:11<00:12,  4.03it/s][A
 49%|████▉     | 47/95 [00:11<00:11,  4.03it/s][A
 51%|█████     | 48/95 [00:11<00:11,  4.03it/s][A
 52%|█████▏    | 49/95 [00:11<00:11,  4.04it/s][A
 53%|█████▎    | 50/95 [00:12<00:11,  4.03it/s][A
 54%|█████▎    | 51/95 [00:12<00:10,  4.02it/s][A
 55%|█████▍    | 52/95 [00:12<00:10,  4.02it/s][A
 56%|█████▌    | 53/95 [00:12<00:10,  4.03it/s][A
 57%|█████▋    | 54/95 [00:13<00:10,  4.02it/s][A
 58%|█████▊    | 55/95 [00:13<00:09,  4.02it/s][A
 59%|█████▉    | 56/95 [00:13<00:09,  4.02it/s][A
 60%|██████    | 57/95 [00:13<00:09,  4.02it/s][A
 61%|██████    | 58/95 [00:14<00:09,  4.00it/s][A
 62%|██████▏   | 59/95 [00:14<00:09,  3.99it/s][A
 63%|██████▎   | 60/95 [00:14<00:08,  4.00it/s][A
 64%|██████▍   | 61/95 [00:14<00:08,  3.99it/s][A
 65%|██████▌   | 62/95 [00:15<00:08,  3.98it/s][A
 66%|██████▋   | 63/95 [00:15<00:08,  3.99it/s][A
 67%|██████▋   | 64/95 [00:15<00:07,  3.99it/s][A
 68%|██████▊   | 65/95 [00:15<00:07,  3.99it/s][A
 69%|██████▉   | 66/95 [00:16<00:07,  3.98it/s][A
 71%|███████   | 67/95 [00:16<00:07,  3.96it/s][A
 72%|███████▏  | 68/95 [00:16<00:06,  3.96it/s][A
 73%|███████▎  | 69/95 [00:17<00:06,  3.97it/s][A
 74%|███████▎  | 70/95 [00:17<00:06,  3.98it/s][A
 75%|███████▍  | 71/95 [00:17<00:06,  3.99it/s][A
 76%|███████▌  | 72/95 [00:17<00:05,  4.00it/s][A
 77%|███████▋  | 73/95 [00:18<00:05,  4.01it/s][A
 78%|███████▊  | 74/95 [00:18<00:05,  4.02it/s][A
 79%|███████▉  | 75/95 [00:18<00:04,  4.01it/s][A
 80%|████████  | 76/95 [00:18<00:04,  4.01it/s][A
 81%|████████  | 77/95 [00:19<00:04,  3.99it/s][A
 82%|████████▏ | 78/95 [00:19<00:04,  3.99it/s][A
 83%|████████▎ | 79/95 [00:19<00:04,  3.99it/s][A
 84%|████████▍ | 80/95 [00:19<00:03,  4.01it/s][A
 85%|████████▌ | 81/95 [00:19<00:03,  4.02it/s][A
 86%|████████▋ | 82/95 [00:20<00:03,  4.04it/s][A
 87%|████████▋ | 83/95 [00:20<00:02,  4.04it/s][A
 88%|████████▊ | 84/95 [00:20<00:02,  4.02it/s][A
 89%|████████▉ | 85/95 [00:20<00:02,  4.01it/s][A
 91%|█████████ | 86/95 [00:21<00:02,  4.01it/s][A
 92%|█████████▏| 87/95 [00:21<00:02,  3.99it/s][A
 93%|█████████▎| 88/95 [00:21<00:01,  3.98it/s][A
 94%|█████████▎| 89/95 [00:21<00:01,  4.00it/s][A
 95%|█████████▍| 90/95 [00:22<00:01,  4.02it/s][A
 96%|█████████▌| 91/95 [00:22<00:00,  4.02it/s][A
 97%|█████████▋| 92/95 [00:22<00:00,  4.00it/s][A
 98%|█████████▊| 93/95 [00:22<00:00,  4.01it/s][A
 99%|█████████▉| 94/95 [00:23<00:00,  4.02it/s][A
100%|██████████| 95/95 [00:23<00:00,  4.03it/s][A                                                
                                               [A  8%|▊         | 20/255 [03:07<27:56,  7.13s/it]
100%|██████████| 95/95 [00:23<00:00,  4.03it/s][A
                                               [A[INFO|trainer.py:3966] 2025-05-03 13:05:07,635 >> Saving model checkpoint to /proj/long-multi/kqian/speech_2/LLaMA-Factory/saves/qwen-14b-1M-R1-CI-round4_v1/full/sft/checkpoint-20
[INFO|configuration_utils.py:423] 2025-05-03 13:05:07,639 >> Configuration saved in /proj/long-multi/kqian/speech_2/LLaMA-Factory/saves/qwen-14b-1M-R1-CI-round4_v1/full/sft/checkpoint-20/config.json
[INFO|configuration_utils.py:908] 2025-05-03 13:05:07,640 >> Configuration saved in /proj/long-multi/kqian/speech_2/LLaMA-Factory/saves/qwen-14b-1M-R1-CI-round4_v1/full/sft/checkpoint-20/generation_config.json
[INFO|modeling_utils.py:3594] 2025-05-03 13:05:19,260 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 6 checkpoint shards. You can find where each parameters has been saved in the index located at /proj/long-multi/kqian/speech_2/LLaMA-Factory/saves/qwen-14b-1M-R1-CI-round4_v1/full/sft/checkpoint-20/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2510] 2025-05-03 13:05:19,261 >> tokenizer config file saved in /proj/long-multi/kqian/speech_2/LLaMA-Factory/saves/qwen-14b-1M-R1-CI-round4_v1/full/sft/checkpoint-20/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-05-03 13:05:19,262 >> Special tokens file saved in /proj/long-multi/kqian/speech_2/LLaMA-Factory/saves/qwen-14b-1M-R1-CI-round4_v1/full/sft/checkpoint-20/special_tokens_map.json
  8%|▊         | 21/255 [03:51<1:38:44, 25.32s/it]  9%|▊         | 22/255 [03:58<1:16:32, 19.71s/it]  9%|▉         | 23/255 [04:04<1:00:43, 15.70s/it]  9%|▉         | 24/255 [04:11<50:09, 13.03s/it]   10%|▉         | 25/255 [04:18<42:37, 11.12s/it] 10%|█         | 26/255 [04:24<37:30,  9.83s/it] 11%|█         | 27/255 [04:31<33:43,  8.87s/it] 11%|█         | 28/255 [04:38<31:03,  8.21s/it] 11%|█▏        | 29/255 [04:44<29:12,  7.76s/it] 12%|█▏        | 30/255 [04:51<28:03,  7.48s/it][INFO|trainer.py:4289] 2025-05-03 13:06:44,246 >> 
***** Running Evaluation *****
[INFO|trainer.py:4291] 2025-05-03 13:06:44,246 >>   Num examples = 757
[INFO|trainer.py:4294] 2025-05-03 13:06:44,246 >>   Batch size = 1

  0%|          | 0/95 [00:00<?, ?it/s][A
  2%|▏         | 2/95 [00:00<00:11,  8.02it/s][A
  3%|▎         | 3/95 [00:00<00:16,  5.69it/s][A
  4%|▍         | 4/95 [00:00<00:18,  4.90it/s][A
  5%|▌         | 5/95 [00:00<00:19,  4.58it/s][A
  6%|▋         | 6/95 [00:01<00:20,  4.39it/s][A
  7%|▋         | 7/95 [00:01<00:20,  4.26it/s][A
  8%|▊         | 8/95 [00:01<00:20,  4.18it/s][A
  9%|▉         | 9/95 [00:01<00:20,  4.13it/s][A
 11%|█         | 10/95 [00:02<00:20,  4.10it/s][A
 12%|█▏        | 11/95 [00:02<00:20,  4.08it/s][A
 13%|█▎        | 12/95 [00:02<00:20,  4.07it/s][A
 14%|█▎        | 13/95 [00:02<00:20,  4.06it/s][A
 15%|█▍        | 14/95 [00:03<00:20,  4.05it/s][A
 16%|█▌        | 15/95 [00:03<00:19,  4.03it/s][A
 17%|█▋        | 16/95 [00:03<00:19,  4.02it/s][A
 18%|█▊        | 17/95 [00:03<00:19,  4.02it/s][A
 19%|█▉        | 18/95 [00:04<00:19,  4.02it/s][A
 20%|██        | 19/95 [00:04<00:18,  4.02it/s][A
 21%|██        | 20/95 [00:04<00:18,  4.02it/s][A
 22%|██▏       | 21/95 [00:04<00:18,  4.01it/s][A
 23%|██▎       | 22/95 [00:05<00:18,  4.00it/s][A
 24%|██▍       | 23/95 [00:05<00:17,  4.01it/s][A
 25%|██▌       | 24/95 [00:05<00:17,  4.02it/s][A
 26%|██▋       | 25/95 [00:05<00:17,  4.01it/s][A
 27%|██▋       | 26/95 [00:06<00:17,  4.00it/s][A
 28%|██▊       | 27/95 [00:06<00:16,  4.00it/s][A
 29%|██▉       | 28/95 [00:06<00:16,  4.01it/s][A
 31%|███       | 29/95 [00:06<00:16,  4.00it/s][A
 32%|███▏      | 30/95 [00:07<00:16,  4.01it/s][A
 33%|███▎      | 31/95 [00:07<00:15,  4.02it/s][A
 34%|███▎      | 32/95 [00:07<00:15,  4.02it/s][A
 35%|███▍      | 33/95 [00:07<00:15,  4.02it/s][A
 36%|███▌      | 34/95 [00:08<00:15,  4.03it/s][A
 37%|███▋      | 35/95 [00:08<00:14,  4.02it/s][A
 38%|███▊      | 36/95 [00:08<00:14,  3.99it/s][A
 39%|███▉      | 37/95 [00:08<00:14,  3.99it/s][A
 40%|████      | 38/95 [00:09<00:14,  3.99it/s][A
 41%|████      | 39/95 [00:09<00:14,  4.00it/s][A
 42%|████▏     | 40/95 [00:09<00:13,  4.01it/s][A
 43%|████▎     | 41/95 [00:09<00:13,  3.99it/s][A
 44%|████▍     | 42/95 [00:10<00:13,  3.99it/s][A
 45%|████▌     | 43/95 [00:10<00:12,  4.00it/s][A
 46%|████▋     | 44/95 [00:10<00:12,  4.02it/s][A
 47%|████▋     | 45/95 [00:10<00:12,  4.01it/s][A
 48%|████▊     | 46/95 [00:11<00:12,  4.03it/s][A
 49%|████▉     | 47/95 [00:11<00:11,  4.03it/s][A
 51%|█████     | 48/95 [00:11<00:11,  4.03it/s][A
 52%|█████▏    | 49/95 [00:11<00:11,  4.04it/s][A
 53%|█████▎    | 50/95 [00:12<00:11,  4.03it/s][A
 54%|█████▎    | 51/95 [00:12<00:10,  4.03it/s][A
 55%|█████▍    | 52/95 [00:12<00:10,  4.03it/s][A
 56%|█████▌    | 53/95 [00:12<00:10,  4.04it/s][A
 57%|█████▋    | 54/95 [00:13<00:10,  4.03it/s][A
 58%|█████▊    | 55/95 [00:13<00:09,  4.03it/s][A
 59%|█████▉    | 56/95 [00:13<00:09,  4.03it/s][A
 60%|██████    | 57/95 [00:13<00:09,  4.04it/s][A
 61%|██████    | 58/95 [00:14<00:09,  4.02it/s][A
 62%|██████▏   | 59/95 [00:14<00:08,  4.02it/s][A
 63%|██████▎   | 60/95 [00:14<00:08,  4.02it/s][A
 64%|██████▍   | 61/95 [00:14<00:08,  4.00it/s][A
 65%|██████▌   | 62/95 [00:15<00:08,  4.00it/s][A
 66%|██████▋   | 63/95 [00:15<00:07,  4.00it/s][A
 67%|██████▋   | 64/95 [00:15<00:07,  4.01it/s][A
 68%|██████▊   | 65/95 [00:15<00:07,  4.01it/s][A
 69%|██████▉   | 66/95 [00:16<00:07,  4.00it/s][A
 71%|███████   | 67/95 [00:16<00:06,  4.01it/s][A
 72%|███████▏  | 68/95 [00:16<00:06,  4.00it/s][A
 73%|███████▎  | 69/95 [00:16<00:06,  4.00it/s][A
 74%|███████▎  | 70/95 [00:17<00:06,  4.01it/s][A
 75%|███████▍  | 71/95 [00:17<00:05,  4.02it/s][A
 76%|███████▌  | 72/95 [00:17<00:05,  4.02it/s][A
 77%|███████▋  | 73/95 [00:17<00:05,  4.03it/s][A
 78%|███████▊  | 74/95 [00:18<00:05,  4.03it/s][A
 79%|███████▉  | 75/95 [00:18<00:04,  4.02it/s][A
 80%|████████  | 76/95 [00:18<00:04,  4.03it/s][A
 81%|████████  | 77/95 [00:18<00:04,  4.01it/s][A
 82%|████████▏ | 78/95 [00:19<00:04,  4.01it/s][A
 83%|████████▎ | 79/95 [00:19<00:03,  4.01it/s][A
 84%|████████▍ | 80/95 [00:19<00:03,  4.03it/s][A
 85%|████████▌ | 81/95 [00:19<00:03,  4.04it/s][A
 86%|████████▋ | 82/95 [00:20<00:03,  4.06it/s][A
 87%|████████▋ | 83/95 [00:20<00:02,  4.06it/s][A
 88%|████████▊ | 84/95 [00:20<00:02,  4.04it/s][A
 89%|████████▉ | 85/95 [00:20<00:02,  4.04it/s][A
 91%|█████████ | 86/95 [00:21<00:02,  4.03it/s][A
 92%|█████████▏| 87/95 [00:21<00:01,  4.01it/s][A
 93%|█████████▎| 88/95 [00:21<00:01,  4.01it/s][A
 94%|█████████▎| 89/95 [00:21<00:01,  4.02it/s][A
 95%|█████████▍| 90/95 [00:22<00:01,  4.04it/s][A
 96%|█████████▌| 91/95 [00:22<00:00,  4.03it/s][A
 97%|█████████▋| 92/95 [00:22<00:00,  4.02it/s][A
 98%|█████████▊| 93/95 [00:22<00:00,  4.03it/s][A
 99%|█████████▉| 94/95 [00:23<00:00,  4.04it/s][A
100%|██████████| 95/95 [00:23<00:00,  4.03it/s][A                                                
                                               [A 12%|█▏        | 30/255 [05:15<28:03,  7.48s/it]
100%|██████████| 95/95 [00:23<00:00,  4.03it/s][A
                                               [A 12%|█▏        | 31/255 [05:21<52:54, 14.17s/it] 13%|█▎        | 32/255 [05:28<44:22, 11.94s/it] 13%|█▎        | 33/255 [05:34<38:25, 10.38s/it] 13%|█▎        | 34/255 [05:41<34:16,  9.30s/it] 14%|█▎        | 35/255 [05:48<31:11,  8.51s/it] 14%|█▍        | 36/255 [05:55<29:07,  7.98s/it] 15%|█▍        | 37/255 [06:01<27:34,  7.59s/it] 15%|█▍        | 38/255 [06:08<26:37,  7.36s/it] 15%|█▌        | 39/255 [06:15<25:55,  7.20s/it] 16%|█▌        | 40/255 [06:22<25:21,  7.08s/it]                                                 16%|█▌        | 40/255 [06:22<25:21,  7.08s/it][INFO|trainer.py:4289] 2025-05-03 13:08:14,827 >> 
***** Running Evaluation *****
[INFO|trainer.py:4291] 2025-05-03 13:08:14,827 >>   Num examples = 757
[INFO|trainer.py:4294] 2025-05-03 13:08:14,827 >>   Batch size = 1

  0%|          | 0/95 [00:00<?, ?it/s][A
  2%|▏         | 2/95 [00:00<00:11,  8.02it/s][A
  3%|▎         | 3/95 [00:00<00:16,  5.68it/s][A
  4%|▍         | 4/95 [00:00<00:18,  4.90it/s][A
  5%|▌         | 5/95 [00:00<00:19,  4.57it/s][A
  6%|▋         | 6/95 [00:01<00:20,  4.38it/s][A
  7%|▋         | 7/95 [00:01<00:20,  4.26it/s][A
  8%|▊         | 8/95 [00:01<00:20,  4.18it/s][A
  9%|▉         | 9/95 [00:01<00:20,  4.13it/s][A
 11%|█         | 10/95 [00:02<00:20,  4.11it/s][A
 12%|█▏        | 11/95 [00:02<00:20,  4.10it/s][A
 13%|█▎        | 12/95 [00:02<00:20,  4.08it/s][A
 14%|█▎        | 13/95 [00:02<00:20,  4.08it/s][A
 15%|█▍        | 14/95 [00:03<00:19,  4.06it/s][A
 16%|█▌        | 15/95 [00:03<00:19,  4.05it/s][A
 17%|█▋        | 16/95 [00:03<00:19,  4.03it/s][A
 18%|█▊        | 17/95 [00:03<00:19,  4.02it/s][A
 19%|█▉        | 18/95 [00:04<00:19,  4.02it/s][A
 20%|██        | 19/95 [00:04<00:18,  4.03it/s][A
 21%|██        | 20/95 [00:04<00:18,  4.03it/s][A
 22%|██▏       | 21/95 [00:04<00:18,  4.03it/s][A
 23%|██▎       | 22/95 [00:05<00:18,  4.01it/s][A
 24%|██▍       | 23/95 [00:05<00:17,  4.03it/s][A
 25%|██▌       | 24/95 [00:05<00:17,  4.03it/s][A
 26%|██▋       | 25/95 [00:05<00:17,  4.02it/s][A
 27%|██▋       | 26/95 [00:06<00:17,  4.01it/s][A
 28%|██▊       | 27/95 [00:06<00:16,  4.01it/s][A
 29%|██▉       | 28/95 [00:06<00:16,  4.02it/s][A
 31%|███       | 29/95 [00:06<00:16,  4.01it/s][A
 32%|███▏      | 30/95 [00:07<00:16,  4.03it/s][A
 33%|███▎      | 31/95 [00:07<00:15,  4.04it/s][A
 34%|███▎      | 32/95 [00:07<00:15,  4.05it/s][A
 35%|███▍      | 33/95 [00:07<00:15,  4.02it/s][A
 36%|███▌      | 34/95 [00:08<00:15,  4.02it/s][A
 37%|███▋      | 35/95 [00:08<00:14,  4.02it/s][A
 38%|███▊      | 36/95 [00:08<00:14,  4.00it/s][A
 39%|███▉      | 37/95 [00:08<00:14,  4.01it/s][A
 40%|████      | 38/95 [00:09<00:14,  4.02it/s][A
 41%|████      | 39/95 [00:09<00:13,  4.03it/s][A
 42%|████▏     | 40/95 [00:09<00:13,  4.04it/s][A
 43%|████▎     | 41/95 [00:09<00:13,  4.05it/s][A
 44%|████▍     | 42/95 [00:10<00:13,  4.05it/s][A
 45%|████▌     | 43/95 [00:10<00:12,  4.05it/s][A
 46%|████▋     | 44/95 [00:10<00:12,  4.05it/s][A
 47%|████▋     | 45/95 [00:10<00:12,  4.05it/s][A
 48%|████▊     | 46/95 [00:11<00:12,  4.06it/s][A
 49%|████▉     | 47/95 [00:11<00:11,  4.06it/s][A
 51%|█████     | 48/95 [00:11<00:11,  4.06it/s][A
 52%|█████▏    | 49/95 [00:11<00:11,  4.07it/s][A
 53%|█████▎    | 50/95 [00:12<00:11,  4.06it/s][A
 54%|█████▎    | 51/95 [00:12<00:10,  4.05it/s][A
 55%|█████▍    | 52/95 [00:12<00:10,  4.05it/s][A
 56%|█████▌    | 53/95 [00:12<00:10,  4.06it/s][A
 57%|█████▋    | 54/95 [00:13<00:10,  4.06it/s][A
 58%|█████▊    | 55/95 [00:13<00:09,  4.06it/s][A
 59%|█████▉    | 56/95 [00:13<00:09,  4.06it/s][A
 60%|██████    | 57/95 [00:13<00:09,  4.07it/s][A
 61%|██████    | 58/95 [00:14<00:09,  4.05it/s][A
 62%|██████▏   | 59/95 [00:14<00:08,  4.03it/s][A
 63%|██████▎   | 60/95 [00:14<00:08,  4.03it/s][A
 64%|██████▍   | 61/95 [00:14<00:08,  4.02it/s][A
 65%|██████▌   | 62/95 [00:15<00:08,  4.01it/s][A
 66%|██████▋   | 63/95 [00:15<00:07,  4.01it/s][A
 67%|██████▋   | 64/95 [00:15<00:07,  4.01it/s][A
 68%|██████▊   | 65/95 [00:15<00:07,  4.02it/s][A
 69%|██████▉   | 66/95 [00:16<00:07,  4.02it/s][A
 71%|███████   | 67/95 [00:16<00:06,  4.02it/s][A
 72%|███████▏  | 68/95 [00:16<00:06,  4.02it/s][A
 73%|███████▎  | 69/95 [00:16<00:06,  4.04it/s][A
 74%|███████▎  | 70/95 [00:17<00:06,  4.04it/s][A
 75%|███████▍  | 71/95 [00:17<00:05,  4.05it/s][A
 76%|███████▌  | 72/95 [00:17<00:05,  4.06it/s][A
 77%|███████▋  | 73/95 [00:17<00:05,  4.06it/s][A
 78%|███████▊  | 74/95 [00:18<00:05,  4.07it/s][A
 79%|███████▉  | 75/95 [00:18<00:04,  4.06it/s][A
 80%|████████  | 76/95 [00:18<00:04,  4.06it/s][A
 81%|████████  | 77/95 [00:18<00:04,  4.04it/s][A
 82%|████████▏ | 78/95 [00:19<00:04,  4.04it/s][A
 83%|████████▎ | 79/95 [00:19<00:03,  4.04it/s][A
 84%|████████▍ | 80/95 [00:19<00:03,  4.06it/s][A
 85%|████████▌ | 81/95 [00:19<00:03,  4.07it/s][A
 86%|████████▋ | 82/95 [00:20<00:03,  4.09it/s][A
 87%|████████▋ | 83/95 [00:20<00:02,  4.09it/s][A
 88%|████████▊ | 84/95 [00:20<00:02,  4.07it/s][A
 89%|████████▉ | 85/95 [00:20<00:02,  4.06it/s][A
 91%|█████████ | 86/95 [00:21<00:02,  4.04it/s][A
 92%|█████████▏| 87/95 [00:21<00:01,  4.03it/s][A
 93%|█████████▎| 88/95 [00:21<00:01,  4.02it/s][A
 94%|█████████▎| 89/95 [00:21<00:01,  4.03it/s][A
 95%|█████████▍| 90/95 [00:22<00:01,  4.05it/s][A
 96%|█████████▌| 91/95 [00:22<00:00,  4.05it/s][A
 97%|█████████▋| 92/95 [00:22<00:00,  4.04it/s][A
 98%|█████████▊| 93/95 [00:22<00:00,  4.03it/s][A
 99%|█████████▉| 94/95 [00:23<00:00,  4.01it/s][A
100%|██████████| 95/95 [00:23<00:00,  4.02it/s][A                                                
                                               [A 16%|█▌        | 40/255 [06:45<25:21,  7.08s/it]
100%|██████████| 95/95 [00:23<00:00,  4.02it/s][A
                                               [A[INFO|trainer.py:3966] 2025-05-03 13:08:45,715 >> Saving model checkpoint to /proj/long-multi/kqian/speech_2/LLaMA-Factory/saves/qwen-14b-1M-R1-CI-round4_v1/full/sft/checkpoint-40
[INFO|configuration_utils.py:423] 2025-05-03 13:08:45,720 >> Configuration saved in /proj/long-multi/kqian/speech_2/LLaMA-Factory/saves/qwen-14b-1M-R1-CI-round4_v1/full/sft/checkpoint-40/config.json
[INFO|configuration_utils.py:908] 2025-05-03 13:08:45,720 >> Configuration saved in /proj/long-multi/kqian/speech_2/LLaMA-Factory/saves/qwen-14b-1M-R1-CI-round4_v1/full/sft/checkpoint-40/generation_config.json
[INFO|modeling_utils.py:3594] 2025-05-03 13:08:57,537 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 6 checkpoint shards. You can find where each parameters has been saved in the index located at /proj/long-multi/kqian/speech_2/LLaMA-Factory/saves/qwen-14b-1M-R1-CI-round4_v1/full/sft/checkpoint-40/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2510] 2025-05-03 13:08:57,538 >> tokenizer config file saved in /proj/long-multi/kqian/speech_2/LLaMA-Factory/saves/qwen-14b-1M-R1-CI-round4_v1/full/sft/checkpoint-40/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-05-03 13:08:57,538 >> Special tokens file saved in /proj/long-multi/kqian/speech_2/LLaMA-Factory/saves/qwen-14b-1M-R1-CI-round4_v1/full/sft/checkpoint-40/special_tokens_map.json
 16%|█▌        | 41/255 [07:30<1:30:26, 25.36s/it] 16%|█▋        | 42/255 [07:36<1:10:10, 19.77s/it] 17%|█▋        | 43/255 [07:43<55:50, 15.80s/it]   17%|█▋        | 44/255 [07:50<45:53, 13.05s/it] 18%|█▊        | 45/255 [07:55<37:55, 10.83s/it] 18%|█▊        | 46/255 [08:02<33:21,  9.58s/it] 18%|█▊        | 47/255 [08:09<30:11,  8.71s/it] 19%|█▉        | 48/255 [08:15<28:05,  8.14s/it] 19%|█▉        | 49/255 [08:22<26:31,  7.73s/it] 20%|█▉        | 50/255 [08:29<25:32,  7.47s/it][INFO|trainer.py:4289] 2025-05-03 13:10:22,185 >> 
***** Running Evaluation *****
[INFO|trainer.py:4291] 2025-05-03 13:10:22,185 >>   Num examples = 757
[INFO|trainer.py:4294] 2025-05-03 13:10:22,185 >>   Batch size = 1

  0%|          | 0/95 [00:00<?, ?it/s][A
  2%|▏         | 2/95 [00:00<00:11,  8.02it/s][A
  3%|▎         | 3/95 [00:00<00:16,  5.65it/s][A
  4%|▍         | 4/95 [00:00<00:18,  4.86it/s][A
  5%|▌         | 5/95 [00:01<00:20,  4.48it/s][A
  6%|▋         | 6/95 [00:01<00:20,  4.31it/s][A
  7%|▋         | 7/95 [00:01<00:20,  4.19it/s][A
  8%|▊         | 8/95 [00:01<00:21,  4.13it/s][A
  9%|▉         | 9/95 [00:02<00:21,  4.09it/s][A
 11%|█         | 10/95 [00:02<00:20,  4.06it/s][A
 12%|█▏        | 11/95 [00:02<00:20,  4.04it/s][A
 13%|█▎        | 12/95 [00:02<00:20,  4.04it/s][A
 14%|█▎        | 13/95 [00:03<00:20,  4.03it/s][A
 15%|█▍        | 14/95 [00:03<00:20,  4.02it/s][A
 16%|█▌        | 15/95 [00:03<00:20,  3.99it/s][A
 17%|█▋        | 16/95 [00:03<00:19,  3.99it/s][A
 18%|█▊        | 17/95 [00:04<00:19,  3.99it/s][A
 19%|█▉        | 18/95 [00:04<00:19,  4.00it/s][A
 20%|██        | 19/95 [00:04<00:18,  4.01it/s][A
 21%|██        | 20/95 [00:04<00:18,  4.01it/s][A
 22%|██▏       | 21/95 [00:05<00:18,  4.00it/s][A
 23%|██▎       | 22/95 [00:05<00:18,  4.00it/s][A
 24%|██▍       | 23/95 [00:05<00:17,  4.01it/s][A
 25%|██▌       | 24/95 [00:05<00:17,  4.01it/s][A
 26%|██▋       | 25/95 [00:06<00:17,  4.01it/s][A
 27%|██▋       | 26/95 [00:06<00:17,  4.00it/s][A
 28%|██▊       | 27/95 [00:06<00:17,  4.00it/s][A
 29%|██▉       | 28/95 [00:06<00:16,  4.01it/s][A
 31%|███       | 29/95 [00:07<00:16,  4.00it/s][A
 32%|███▏      | 30/95 [00:07<00:16,  4.01it/s][A
 33%|███▎      | 31/95 [00:07<00:15,  4.01it/s][A
 34%|███▎      | 32/95 [00:07<00:15,  4.01it/s][A
 35%|███▍      | 33/95 [00:08<00:15,  4.01it/s][A
 36%|███▌      | 34/95 [00:08<00:15,  4.03it/s][A
 37%|███▋      | 35/95 [00:08<00:14,  4.03it/s][A
 38%|███▊      | 36/95 [00:08<00:14,  4.02it/s][A
 39%|███▉      | 37/95 [00:08<00:14,  4.02it/s][A
 40%|████      | 38/95 [00:09<00:14,  4.02it/s][A
 41%|████      | 39/95 [00:09<00:13,  4.02it/s][A
 42%|████▏     | 40/95 [00:09<00:13,  4.03it/s][A
 43%|████▎     | 41/95 [00:09<00:13,  4.04it/s][A
 44%|████▍     | 42/95 [00:10<00:13,  4.04it/s][A
 45%|████▌     | 43/95 [00:10<00:12,  4.04it/s][A
 46%|████▋     | 44/95 [00:10<00:12,  4.04it/s][A
 47%|████▋     | 45/95 [00:10<00:12,  4.04it/s][A
 48%|████▊     | 46/95 [00:11<00:12,  4.04it/s][A
 49%|████▉     | 47/95 [00:11<00:11,  4.03it/s][A
 51%|█████     | 48/95 [00:11<00:11,  4.02it/s][A
 52%|█████▏    | 49/95 [00:11<00:11,  4.04it/s][A
 53%|█████▎    | 50/95 [00:12<00:11,  4.02it/s][A
 54%|█████▎    | 51/95 [00:12<00:10,  4.02it/s][A
 55%|█████▍    | 52/95 [00:12<00:10,  4.02it/s][A
 56%|█████▌    | 53/95 [00:12<00:10,  4.02it/s][A
 57%|█████▋    | 54/95 [00:13<00:10,  4.02it/s][A
 58%|█████▊    | 55/95 [00:13<00:09,  4.02it/s][A
 59%|█████▉    | 56/95 [00:13<00:09,  4.02it/s][A
 60%|██████    | 57/95 [00:13<00:09,  4.02it/s][A
 61%|██████    | 58/95 [00:14<00:09,  4.00it/s][A
 62%|██████▏   | 59/95 [00:14<00:09,  4.00it/s][A
 63%|██████▎   | 60/95 [00:14<00:08,  4.00it/s][A
 64%|██████▍   | 61/95 [00:14<00:08,  3.99it/s][A
 65%|██████▌   | 62/95 [00:15<00:08,  3.99it/s][A
 66%|██████▋   | 63/95 [00:15<00:08,  3.99it/s][A
 67%|██████▋   | 64/95 [00:15<00:07,  3.98it/s][A
 68%|██████▊   | 65/95 [00:15<00:07,  3.98it/s][A
 69%|██████▉   | 66/95 [00:16<00:07,  3.98it/s][A
 71%|███████   | 67/95 [00:16<00:07,  3.99it/s][A
 72%|███████▏  | 68/95 [00:16<00:06,  4.00it/s][A
 73%|███████▎  | 69/95 [00:16<00:06,  4.01it/s][A
 74%|███████▎  | 70/95 [00:17<00:06,  4.01it/s][A
 75%|███████▍  | 71/95 [00:17<00:05,  4.01it/s][A
 76%|███████▌  | 72/95 [00:17<00:05,  4.02it/s][A
 77%|███████▋  | 73/95 [00:17<00:05,  4.03it/s][A
 78%|███████▊  | 74/95 [00:18<00:05,  4.04it/s][A
 79%|███████▉  | 75/95 [00:18<00:04,  4.02it/s][A
 80%|████████  | 76/95 [00:18<00:04,  4.03it/s][A
 81%|████████  | 77/95 [00:18<00:04,  4.01it/s][A
 82%|████████▏ | 78/95 [00:19<00:04,  4.01it/s][A
 83%|████████▎ | 79/95 [00:19<00:03,  4.01it/s][A
 84%|████████▍ | 80/95 [00:19<00:03,  4.03it/s][A
 85%|████████▌ | 81/95 [00:19<00:03,  4.04it/s][A
 86%|████████▋ | 82/95 [00:20<00:03,  4.04it/s][A
 87%|████████▋ | 83/95 [00:20<00:02,  4.05it/s][A
 88%|████████▊ | 84/95 [00:20<00:02,  4.03it/s][A
 89%|████████▉ | 85/95 [00:20<00:02,  4.02it/s][A
 91%|█████████ | 86/95 [00:21<00:02,  4.01it/s][A
 92%|█████████▏| 87/95 [00:21<00:01,  4.00it/s][A
 93%|█████████▎| 88/95 [00:21<00:01,  4.00it/s][A
 94%|█████████▎| 89/95 [00:21<00:01,  4.01it/s][A
 95%|█████████▍| 90/95 [00:22<00:01,  4.03it/s][A
 96%|█████████▌| 91/95 [00:22<00:00,  4.03it/s][A
 97%|█████████▋| 92/95 [00:22<00:00,  4.01it/s][A
 98%|█████████▊| 93/95 [00:22<00:00,  4.00it/s][A
 99%|█████████▉| 94/95 [00:23<00:00,  4.01it/s][A
100%|██████████| 95/95 [00:23<00:00,  4.02it/s][A                                                
                                               [A 20%|█▉        | 50/255 [08:53<25:32,  7.47s/it]
100%|██████████| 95/95 [00:23<00:00,  4.02it/s][A
                                               [A 20%|██        | 51/255 [09:00<48:50, 14.36s/it] 20%|██        | 52/255 [09:06<40:45, 12.05s/it] 21%|██        | 53/255 [09:13<35:08, 10.44s/it] 21%|██        | 54/255 [09:19<30:53,  9.22s/it] 22%|██▏       | 55/255 [09:26<28:05,  8.43s/it] 22%|██▏       | 56/255 [09:32<26:11,  7.90s/it] 22%|██▏       | 57/255 [09:39<24:53,  7.54s/it] 23%|██▎       | 58/255 [09:46<23:45,  7.24s/it] 23%|██▎       | 59/255 [09:52<23:09,  7.09s/it] 24%|██▎       | 60/255 [09:59<22:35,  6.95s/it]                                                 24%|██▎       | 60/255 [09:59<22:35,  6.95s/it][INFO|trainer.py:4289] 2025-05-03 13:11:52,170 >> 
***** Running Evaluation *****
[INFO|trainer.py:4291] 2025-05-03 13:11:52,171 >>   Num examples = 757
[INFO|trainer.py:4294] 2025-05-03 13:11:52,171 >>   Batch size = 1

  0%|          | 0/95 [00:00<?, ?it/s][A
  2%|▏         | 2/95 [00:00<00:11,  8.03it/s][A
  3%|▎         | 3/95 [00:00<00:16,  5.65it/s][A
  4%|▍         | 4/95 [00:00<00:18,  4.86it/s][A
  5%|▌         | 5/95 [00:01<00:19,  4.51it/s][A
  6%|▋         | 6/95 [00:01<00:20,  4.32it/s][A
  7%|▋         | 7/95 [00:01<00:20,  4.21it/s][A
  8%|▊         | 8/95 [00:01<00:20,  4.15it/s][A
  9%|▉         | 9/95 [00:02<00:20,  4.11it/s][A
 11%|█         | 10/95 [00:02<00:20,  4.08it/s][A
 12%|█▏        | 11/95 [00:02<00:20,  4.07it/s][A
 13%|█▎        | 12/95 [00:02<00:20,  4.06it/s][A
 14%|█▎        | 13/95 [00:02<00:20,  4.05it/s][A
 15%|█▍        | 14/95 [00:03<00:20,  4.05it/s][A
 16%|█▌        | 15/95 [00:03<00:19,  4.02it/s][A
 17%|█▋        | 16/95 [00:03<00:19,  4.01it/s][A
 18%|█▊        | 17/95 [00:03<00:19,  4.01it/s][A
 19%|█▉        | 18/95 [00:04<00:19,  4.01it/s][A
 20%|██        | 19/95 [00:04<00:18,  4.02it/s][A
 21%|██        | 20/95 [00:04<00:18,  4.02it/s][A
 22%|██▏       | 21/95 [00:04<00:18,  4.01it/s][A
 23%|██▎       | 22/95 [00:05<00:18,  4.00it/s][A
 24%|██▍       | 23/95 [00:05<00:17,  4.01it/s][A
 25%|██▌       | 24/95 [00:05<00:17,  4.00it/s][A
 26%|██▋       | 25/95 [00:05<00:17,  4.00it/s][A
 27%|██▋       | 26/95 [00:06<00:17,  4.00it/s][A
 28%|██▊       | 27/95 [00:06<00:17,  4.00it/s][A
 29%|██▉       | 28/95 [00:06<00:16,  4.01it/s][A
 31%|███       | 29/95 [00:06<00:16,  4.00it/s][A
 32%|███▏      | 30/95 [00:07<00:16,  4.01it/s][A
 33%|███▎      | 31/95 [00:07<00:15,  4.02it/s][A
 34%|███▎      | 32/95 [00:07<00:15,  4.02it/s][A
 35%|███▍      | 33/95 [00:07<00:15,  4.01it/s][A
 36%|███▌      | 34/95 [00:08<00:15,  4.01it/s][A
 37%|███▋      | 35/95 [00:08<00:14,  4.02it/s][A
 38%|███▊      | 36/95 [00:08<00:14,  4.00it/s][A
 39%|███▉      | 37/95 [00:08<00:14,  4.00it/s][A
 40%|████      | 38/95 [00:09<00:14,  4.00it/s][A
 41%|████      | 39/95 [00:09<00:13,  4.01it/s][A
 42%|████▏     | 40/95 [00:09<00:13,  4.03it/s][A
 43%|████▎     | 41/95 [00:09<00:13,  4.04it/s][A
 44%|████▍     | 42/95 [00:10<00:13,  4.04it/s][A
 45%|████▌     | 43/95 [00:10<00:12,  4.05it/s][A
 46%|████▋     | 44/95 [00:10<00:12,  4.04it/s][A
 47%|████▋     | 45/95 [00:10<00:12,  4.04it/s][A
 48%|████▊     | 46/95 [00:11<00:12,  4.04it/s][A
 49%|████▉     | 47/95 [00:11<00:11,  4.03it/s][A
 51%|█████     | 48/95 [00:11<00:11,  4.03it/s][A
 52%|█████▏    | 49/95 [00:11<00:11,  4.04it/s][A
 53%|█████▎    | 50/95 [00:12<00:11,  4.03it/s][A
 54%|█████▎    | 51/95 [00:12<00:10,  4.03it/s][A
 55%|█████▍    | 52/95 [00:12<00:10,  4.03it/s][A
 56%|█████▌    | 53/95 [00:12<00:10,  4.04it/s][A
 57%|█████▋    | 54/95 [00:13<00:10,  4.04it/s][A
 58%|█████▊    | 55/95 [00:13<00:09,  4.04it/s][A
 59%|█████▉    | 56/95 [00:13<00:09,  4.03it/s][A
 60%|██████    | 57/95 [00:13<00:09,  4.04it/s][A
 61%|██████    | 58/95 [00:14<00:09,  4.02it/s][A
 62%|██████▏   | 59/95 [00:14<00:08,  4.02it/s][A
 63%|██████▎   | 60/95 [00:14<00:08,  4.02it/s][A
 64%|██████▍   | 61/95 [00:14<00:08,  4.00it/s][A
 65%|██████▌   | 62/95 [00:15<00:08,  4.00it/s][A
 66%|██████▋   | 63/95 [00:15<00:08,  4.00it/s][A
 67%|██████▋   | 64/95 [00:15<00:07,  3.98it/s][A
 68%|██████▊   | 65/95 [00:15<00:07,  3.98it/s][A
 69%|██████▉   | 66/95 [00:16<00:07,  3.98it/s][A
 71%|███████   | 67/95 [00:16<00:07,  3.99it/s][A
 72%|███████▏  | 68/95 [00:16<00:06,  4.00it/s][A
 73%|███████▎  | 69/95 [00:16<00:06,  4.00it/s][A
 74%|███████▎  | 70/95 [00:17<00:06,  4.02it/s][A
 75%|███████▍  | 71/95 [00:17<00:05,  4.02it/s][A
 76%|███████▌  | 72/95 [00:17<00:05,  4.02it/s][A
 77%|███████▋  | 73/95 [00:17<00:05,  4.03it/s][A
 78%|███████▊  | 74/95 [00:18<00:05,  4.04it/s][A
 79%|███████▉  | 75/95 [00:18<00:04,  4.03it/s][A
 80%|████████  | 76/95 [00:18<00:04,  4.02it/s][A
 81%|████████  | 77/95 [00:18<00:04,  4.00it/s][A
 82%|████████▏ | 78/95 [00:19<00:04,  4.00it/s][A
 83%|████████▎ | 79/95 [00:19<00:04,  3.99it/s][A
 84%|████████▍ | 80/95 [00:19<00:03,  4.01it/s][A
 85%|████████▌ | 81/95 [00:19<00:03,  4.03it/s][A
 86%|████████▋ | 82/95 [00:20<00:03,  4.04it/s][A
 87%|████████▋ | 83/95 [00:20<00:02,  4.05it/s][A
 88%|████████▊ | 84/95 [00:20<00:02,  4.03it/s][A
 89%|████████▉ | 85/95 [00:20<00:02,  4.02it/s][A
 91%|█████████ | 86/95 [00:21<00:02,  4.01it/s][A
 92%|█████████▏| 87/95 [00:21<00:02,  3.99it/s][A
 93%|█████████▎| 88/95 [00:21<00:01,  3.99it/s][A
 94%|█████████▎| 89/95 [00:21<00:01,  4.01it/s][A
 95%|█████████▍| 90/95 [00:22<00:01,  4.03it/s][A
 96%|█████████▌| 91/95 [00:22<00:00,  4.03it/s][A
 97%|█████████▋| 92/95 [00:22<00:00,  4.03it/s][A
 98%|█████████▊| 93/95 [00:22<00:00,  4.03it/s][A
 99%|█████████▉| 94/95 [00:23<00:00,  4.04it/s][A
100%|██████████| 95/95 [00:23<00:00,  4.05it/s][A                                                
                                               [A 24%|██▎       | 60/255 [10:23<22:35,  6.95s/it]
100%|██████████| 95/95 [00:23<00:00,  4.05it/s][A
                                               [A[INFO|trainer.py:3966] 2025-05-03 13:12:23,339 >> Saving model checkpoint to /proj/long-multi/kqian/speech_2/LLaMA-Factory/saves/qwen-14b-1M-R1-CI-round4_v1/full/sft/checkpoint-60
[INFO|configuration_utils.py:423] 2025-05-03 13:12:23,343 >> Configuration saved in /proj/long-multi/kqian/speech_2/LLaMA-Factory/saves/qwen-14b-1M-R1-CI-round4_v1/full/sft/checkpoint-60/config.json
[INFO|configuration_utils.py:908] 2025-05-03 13:12:23,343 >> Configuration saved in /proj/long-multi/kqian/speech_2/LLaMA-Factory/saves/qwen-14b-1M-R1-CI-round4_v1/full/sft/checkpoint-60/generation_config.json
[INFO|modeling_utils.py:3594] 2025-05-03 13:12:35,157 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 6 checkpoint shards. You can find where each parameters has been saved in the index located at /proj/long-multi/kqian/speech_2/LLaMA-Factory/saves/qwen-14b-1M-R1-CI-round4_v1/full/sft/checkpoint-60/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2510] 2025-05-03 13:12:35,158 >> tokenizer config file saved in /proj/long-multi/kqian/speech_2/LLaMA-Factory/saves/qwen-14b-1M-R1-CI-round4_v1/full/sft/checkpoint-60/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-05-03 13:12:35,159 >> Special tokens file saved in /proj/long-multi/kqian/speech_2/LLaMA-Factory/saves/qwen-14b-1M-R1-CI-round4_v1/full/sft/checkpoint-60/special_tokens_map.json
 24%|██▍       | 61/255 [11:07<1:21:56, 25.34s/it] 24%|██▍       | 62/255 [11:14<1:03:31, 19.75s/it] 25%|██▍       | 63/255 [11:21<50:41, 15.84s/it]   25%|██▌       | 64/255 [11:27<41:41, 13.10s/it] 25%|██▌       | 65/255 [11:34<35:26, 11.19s/it] 26%|██▌       | 66/255 [11:41<30:58,  9.84s/it] 26%|██▋       | 67/255 [11:46<26:46,  8.55s/it] 27%|██▋       | 68/255 [11:53<24:52,  7.98s/it] 27%|██▋       | 69/255 [11:59<23:00,  7.42s/it] 27%|██▋       | 70/255 [12:06<22:14,  7.22s/it][INFO|trainer.py:4289] 2025-05-03 13:13:59,012 >> 
***** Running Evaluation *****
[INFO|trainer.py:4291] 2025-05-03 13:13:59,012 >>   Num examples = 757
[INFO|trainer.py:4294] 2025-05-03 13:13:59,012 >>   Batch size = 1

  0%|          | 0/95 [00:00<?, ?it/s][A
  2%|▏         | 2/95 [00:00<00:11,  8.05it/s][A
  3%|▎         | 3/95 [00:00<00:16,  5.66it/s][A
  4%|▍         | 4/95 [00:00<00:18,  4.89it/s][A
  5%|▌         | 5/95 [00:00<00:19,  4.55it/s][A
  6%|▋         | 6/95 [00:01<00:20,  4.35it/s][A
  7%|▋         | 7/95 [00:01<00:20,  4.23it/s][A
  8%|▊         | 8/95 [00:01<00:20,  4.16it/s][A
  9%|▉         | 9/95 [00:01<00:20,  4.12it/s][A
 11%|█         | 10/95 [00:02<00:20,  4.09it/s][A
 12%|█▏        | 11/95 [00:02<00:20,  4.06it/s][A
 13%|█▎        | 12/95 [00:02<00:20,  4.06it/s][A
 14%|█▎        | 13/95 [00:02<00:20,  4.04it/s][A
 15%|█▍        | 14/95 [00:03<00:20,  4.04it/s][A
 16%|█▌        | 15/95 [00:03<00:19,  4.01it/s][A
 17%|█▋        | 16/95 [00:03<00:19,  4.01it/s][A
 18%|█▊        | 17/95 [00:03<00:19,  4.01it/s][A
 19%|█▉        | 18/95 [00:04<00:19,  4.00it/s][A
 20%|██        | 19/95 [00:04<00:18,  4.01it/s][A
 21%|██        | 20/95 [00:04<00:18,  4.00it/s][A
 22%|██▏       | 21/95 [00:04<00:18,  3.99it/s][A
 23%|██▎       | 22/95 [00:05<00:18,  3.99it/s][A
 24%|██▍       | 23/95 [00:05<00:18,  3.98it/s][A
 25%|██▌       | 24/95 [00:05<00:17,  3.98it/s][A
 26%|██▋       | 25/95 [00:06<00:17,  3.98it/s][A
 27%|██▋       | 26/95 [00:06<00:17,  3.96it/s][A
 28%|██▊       | 27/95 [00:06<00:17,  3.96it/s][A
 29%|██▉       | 28/95 [00:06<00:16,  3.97it/s][A
 31%|███       | 29/95 [00:07<00:16,  3.98it/s][A
 32%|███▏      | 30/95 [00:07<00:16,  3.99it/s][A
 33%|███▎      | 31/95 [00:07<00:16,  4.00it/s][A
 34%|███▎      | 32/95 [00:07<00:15,  3.99it/s][A
 35%|███▍      | 33/95 [00:08<00:15,  3.98it/s][A
 36%|███▌      | 34/95 [00:08<00:15,  4.00it/s][A
 37%|███▋      | 35/95 [00:08<00:14,  4.00it/s][A
 38%|███▊      | 36/95 [00:08<00:14,  3.97it/s][A
 39%|███▉      | 37/95 [00:09<00:14,  3.96it/s][A
 40%|████      | 38/95 [00:09<00:14,  3.96it/s][A
 41%|████      | 39/95 [00:09<00:14,  3.97it/s][A
 42%|████▏     | 40/95 [00:09<00:13,  3.99it/s][A
 43%|████▎     | 41/95 [00:10<00:13,  4.01it/s][A
 44%|████▍     | 42/95 [00:10<00:13,  4.02it/s][A
 45%|████▌     | 43/95 [00:10<00:12,  4.03it/s][A
 46%|████▋     | 44/95 [00:10<00:12,  4.03it/s][A
 47%|████▋     | 45/95 [00:11<00:12,  4.03it/s][A
 48%|████▊     | 46/95 [00:11<00:12,  4.03it/s][A
 49%|████▉     | 47/95 [00:11<00:11,  4.03it/s][A
 51%|█████     | 48/95 [00:11<00:11,  4.03it/s][A
 52%|█████▏    | 49/95 [00:11<00:11,  4.03it/s][A
 53%|█████▎    | 50/95 [00:12<00:11,  4.02it/s][A
 54%|█████▎    | 51/95 [00:12<00:10,  4.02it/s][A
 55%|█████▍    | 52/95 [00:12<00:10,  4.01it/s][A
 56%|█████▌    | 53/95 [00:12<00:10,  4.01it/s][A
 57%|█████▋    | 54/95 [00:13<00:10,  4.00it/s][A
 58%|█████▊    | 55/95 [00:13<00:09,  4.00it/s][A
 59%|█████▉    | 56/95 [00:13<00:09,  4.00it/s][A
 60%|██████    | 57/95 [00:13<00:09,  4.01it/s][A
 61%|██████    | 58/95 [00:14<00:09,  3.99it/s][A
 62%|██████▏   | 59/95 [00:14<00:09,  3.99it/s][A
 63%|██████▎   | 60/95 [00:14<00:08,  4.00it/s][A
 64%|██████▍   | 61/95 [00:15<00:08,  3.99it/s][A
 65%|██████▌   | 62/95 [00:15<00:08,  3.99it/s][A
 66%|██████▋   | 63/95 [00:15<00:08,  3.99it/s][A
 67%|██████▋   | 64/95 [00:15<00:07,  3.99it/s][A
 68%|██████▊   | 65/95 [00:16<00:07,  3.99it/s][A
 69%|██████▉   | 66/95 [00:16<00:07,  3.99it/s][A
 71%|███████   | 67/95 [00:16<00:07,  3.99it/s][A
 72%|███████▏  | 68/95 [00:16<00:06,  3.99it/s][A
 73%|███████▎  | 69/95 [00:17<00:06,  3.99it/s][A
 74%|███████▎  | 70/95 [00:17<00:06,  3.99it/s][A
 75%|███████▍  | 71/95 [00:17<00:06,  4.00it/s][A
 76%|███████▌  | 72/95 [00:17<00:05,  3.99it/s][A
 77%|███████▋  | 73/95 [00:18<00:05,  4.00it/s][A
 78%|███████▊  | 74/95 [00:18<00:05,  4.01it/s][A
 79%|███████▉  | 75/95 [00:18<00:04,  4.00it/s][A
 80%|████████  | 76/95 [00:18<00:04,  4.00it/s][A
 81%|████████  | 77/95 [00:19<00:04,  3.99it/s][A
 82%|████████▏ | 78/95 [00:19<00:04,  3.98it/s][A
 83%|████████▎ | 79/95 [00:19<00:04,  3.98it/s][A
 84%|████████▍ | 80/95 [00:19<00:03,  3.99it/s][A
 85%|████████▌ | 81/95 [00:20<00:03,  4.01it/s][A
 86%|████████▋ | 82/95 [00:20<00:03,  4.02it/s][A
 87%|████████▋ | 83/95 [00:20<00:02,  4.03it/s][A
 88%|████████▊ | 84/95 [00:20<00:02,  4.01it/s][A
 89%|████████▉ | 85/95 [00:21<00:02,  4.01it/s][A
 91%|█████████ | 86/95 [00:21<00:02,  4.01it/s][A
 92%|█████████▏| 87/95 [00:21<00:02,  4.00it/s][A
 93%|█████████▎| 88/95 [00:21<00:01,  3.99it/s][A
 94%|█████████▎| 89/95 [00:22<00:01,  4.00it/s][A
 95%|█████████▍| 90/95 [00:22<00:01,  4.01it/s][A
 96%|█████████▌| 91/95 [00:22<00:00,  4.02it/s][A
 97%|█████████▋| 92/95 [00:22<00:00,  4.00it/s][A
 98%|█████████▊| 93/95 [00:23<00:00,  4.01it/s][A
 99%|█████████▉| 94/95 [00:23<00:00,  4.01it/s][A
100%|██████████| 95/95 [00:23<00:00,  4.02it/s][A                                                
                                               [A 27%|██▋       | 70/255 [12:30<22:14,  7.22s/it]
100%|██████████| 95/95 [00:23<00:00,  4.02it/s][A
                                               [A 28%|██▊       | 71/255 [12:36<43:01, 14.03s/it] 28%|██▊       | 72/255 [12:43<36:05, 11.83s/it] 29%|██▊       | 73/255 [12:49<31:13, 10.29s/it] 29%|██▉       | 74/255 [12:56<27:46,  9.21s/it] 29%|██▉       | 75/255 [13:03<25:21,  8.45s/it] 30%|██▉       | 76/255 [13:09<23:41,  7.94s/it] 30%|███       | 77/255 [13:16<22:23,  7.55s/it] 31%|███       | 78/255 [13:23<21:30,  7.29s/it] 31%|███       | 79/255 [13:29<20:48,  7.09s/it] 31%|███▏      | 80/255 [13:36<20:14,  6.94s/it]                                                 31%|███▏      | 80/255 [13:36<20:14,  6.94s/it][INFO|trainer.py:4289] 2025-05-03 13:15:28,993 >> 
***** Running Evaluation *****
[INFO|trainer.py:4291] 2025-05-03 13:15:28,993 >>   Num examples = 757
[INFO|trainer.py:4294] 2025-05-03 13:15:28,993 >>   Batch size = 1

  0%|          | 0/95 [00:00<?, ?it/s][A
  2%|▏         | 2/95 [00:00<00:11,  8.03it/s][A
  3%|▎         | 3/95 [00:00<00:16,  5.65it/s][A
  4%|▍         | 4/95 [00:00<00:18,  4.88it/s][A
  5%|▌         | 5/95 [00:00<00:19,  4.54it/s][A
  6%|▋         | 6/95 [00:01<00:20,  4.35it/s][A
  7%|▋         | 7/95 [00:01<00:20,  4.23it/s][A
  8%|▊         | 8/95 [00:01<00:20,  4.16it/s][A
  9%|▉         | 9/95 [00:01<00:20,  4.12it/s][A
 11%|█         | 10/95 [00:02<00:20,  4.08it/s][A
 12%|█▏        | 11/95 [00:02<00:20,  4.07it/s][A
 13%|█▎        | 12/95 [00:02<00:20,  4.06it/s][A
 14%|█▎        | 13/95 [00:02<00:20,  4.05it/s][A
 15%|█▍        | 14/95 [00:03<00:20,  4.03it/s][A
 16%|█▌        | 15/95 [00:03<00:19,  4.01it/s][A
 17%|█▋        | 16/95 [00:03<00:19,  4.01it/s][A
 18%|█▊        | 17/95 [00:03<00:19,  4.02it/s][A
 19%|█▉        | 18/95 [00:04<00:19,  4.00it/s][A
 20%|██        | 19/95 [00:04<00:18,  4.01it/s][A
 21%|██        | 20/95 [00:04<00:18,  4.00it/s][A
 22%|██▏       | 21/95 [00:04<00:18,  3.99it/s][A
 23%|██▎       | 22/95 [00:05<00:18,  3.99it/s][A
 24%|██▍       | 23/95 [00:05<00:18,  4.00it/s][A
 25%|██▌       | 24/95 [00:05<00:17,  4.00it/s][A
 26%|██▋       | 25/95 [00:05<00:17,  4.00it/s][A
 27%|██▋       | 26/95 [00:06<00:17,  3.99it/s][A
 28%|██▊       | 27/95 [00:06<00:16,  4.00it/s][A
 29%|██▉       | 28/95 [00:06<00:16,  4.00it/s][A
 31%|███       | 29/95 [00:06<00:16,  4.00it/s][A
 32%|███▏      | 30/95 [00:07<00:16,  4.01it/s][A
 33%|███▎      | 31/95 [00:07<00:15,  4.01it/s][A
 34%|███▎      | 32/95 [00:07<00:15,  3.99it/s][A
 35%|███▍      | 33/95 [00:07<00:15,  3.98it/s][A
 36%|███▌      | 34/95 [00:08<00:15,  4.00it/s][A
 37%|███▋      | 35/95 [00:08<00:14,  4.00it/s][A
 38%|███▊      | 36/95 [00:08<00:14,  3.97it/s][A
 39%|███▉      | 37/95 [00:09<00:14,  3.96it/s][A
 40%|████      | 38/95 [00:09<00:14,  3.95it/s][A
 41%|████      | 39/95 [00:09<00:14,  3.97it/s][A
 42%|████▏     | 40/95 [00:09<00:13,  4.00it/s][A
 43%|████▎     | 41/95 [00:10<00:13,  4.01it/s][A
 44%|████▍     | 42/95 [00:10<00:13,  4.02it/s][A
 45%|████▌     | 43/95 [00:10<00:12,  4.03it/s][A
 46%|████▋     | 44/95 [00:10<00:12,  4.03it/s][A
 47%|████▋     | 45/95 [00:10<00:12,  4.03it/s][A
 48%|████▊     | 46/95 [00:11<00:12,  4.03it/s][A
 49%|████▉     | 47/95 [00:11<00:11,  4.02it/s][A
 51%|█████     | 48/95 [00:11<00:11,  4.02it/s][A
 52%|█████▏    | 49/95 [00:11<00:11,  4.03it/s][A
 53%|█████▎    | 50/95 [00:12<00:11,  4.02it/s][A
 54%|█████▎    | 51/95 [00:12<00:10,  4.01it/s][A
 55%|█████▍    | 52/95 [00:12<00:10,  4.00it/s][A
 56%|█████▌    | 53/95 [00:12<00:10,  4.01it/s][A
 57%|█████▋    | 54/95 [00:13<00:10,  4.01it/s][A
 58%|█████▊    | 55/95 [00:13<00:09,  4.01it/s][A
 59%|█████▉    | 56/95 [00:13<00:09,  4.01it/s][A
 60%|██████    | 57/95 [00:13<00:09,  4.02it/s][A
 61%|██████    | 58/95 [00:14<00:09,  3.99it/s][A
 62%|██████▏   | 59/95 [00:14<00:09,  3.99it/s][A
 63%|██████▎   | 60/95 [00:14<00:08,  4.00it/s][A
 64%|██████▍   | 61/95 [00:14<00:08,  4.00it/s][A
 65%|██████▌   | 62/95 [00:15<00:08,  3.99it/s][A
 66%|██████▋   | 63/95 [00:15<00:08,  4.00it/s][A
 67%|██████▋   | 64/95 [00:15<00:07,  4.00it/s][A
 68%|██████▊   | 65/95 [00:15<00:07,  4.00it/s][A
 69%|██████▉   | 66/95 [00:16<00:07,  4.00it/s][A
 71%|███████   | 67/95 [00:16<00:07,  3.99it/s][A
 72%|███████▏  | 68/95 [00:16<00:06,  4.00it/s][A
 73%|███████▎  | 69/95 [00:16<00:06,  4.00it/s][A
 74%|███████▎  | 70/95 [00:17<00:06,  4.01it/s][A
 75%|███████▍  | 71/95 [00:17<00:05,  4.01it/s][A
 76%|███████▌  | 72/95 [00:17<00:05,  4.01it/s][A
 77%|███████▋  | 73/95 [00:17<00:05,  4.02it/s][A
 78%|███████▊  | 74/95 [00:18<00:05,  4.03it/s][A
 79%|███████▉  | 75/95 [00:18<00:04,  4.02it/s][A
 80%|████████  | 76/95 [00:18<00:04,  4.01it/s][A
 81%|████████  | 77/95 [00:18<00:04,  4.00it/s][A
 82%|████████▏ | 78/95 [00:19<00:04,  4.00it/s][A
 83%|████████▎ | 79/95 [00:19<00:04,  3.97it/s][A
 84%|████████▍ | 80/95 [00:19<00:03,  3.98it/s][A
 85%|████████▌ | 81/95 [00:19<00:03,  3.99it/s][A
 86%|████████▋ | 82/95 [00:20<00:03,  4.00it/s][A
 87%|████████▋ | 83/95 [00:20<00:03,  4.00it/s][A
 88%|████████▊ | 84/95 [00:20<00:02,  3.98it/s][A
 89%|████████▉ | 85/95 [00:20<00:02,  3.98it/s][A
 91%|█████████ | 86/95 [00:21<00:02,  3.99it/s][A
 92%|█████████▏| 87/95 [00:21<00:02,  3.98it/s][A
 93%|█████████▎| 88/95 [00:21<00:01,  3.98it/s][A
 94%|█████████▎| 89/95 [00:21<00:01,  3.98it/s][A
 95%|█████████▍| 90/95 [00:22<00:01,  4.00it/s][A
 96%|█████████▌| 91/95 [00:22<00:00,  4.00it/s][A
 97%|█████████▋| 92/95 [00:22<00:00,  4.00it/s][A
 98%|█████████▊| 93/95 [00:22<00:00,  4.00it/s][A
 99%|█████████▉| 94/95 [00:23<00:00,  4.01it/s][A
100%|██████████| 95/95 [00:23<00:00,  4.01it/s][A                                                
                                               [A 31%|███▏      | 80/255 [14:00<20:14,  6.94s/it]
100%|██████████| 95/95 [00:23<00:00,  4.01it/s][A
                                               [A[INFO|trainer.py:3966] 2025-05-03 13:16:00,124 >> Saving model checkpoint to /proj/long-multi/kqian/speech_2/LLaMA-Factory/saves/qwen-14b-1M-R1-CI-round4_v1/full/sft/checkpoint-80
[INFO|configuration_utils.py:423] 2025-05-03 13:16:00,127 >> Configuration saved in /proj/long-multi/kqian/speech_2/LLaMA-Factory/saves/qwen-14b-1M-R1-CI-round4_v1/full/sft/checkpoint-80/config.json
[INFO|configuration_utils.py:908] 2025-05-03 13:16:00,128 >> Configuration saved in /proj/long-multi/kqian/speech_2/LLaMA-Factory/saves/qwen-14b-1M-R1-CI-round4_v1/full/sft/checkpoint-80/generation_config.json
[INFO|modeling_utils.py:3594] 2025-05-03 13:16:11,919 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 6 checkpoint shards. You can find where each parameters has been saved in the index located at /proj/long-multi/kqian/speech_2/LLaMA-Factory/saves/qwen-14b-1M-R1-CI-round4_v1/full/sft/checkpoint-80/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2510] 2025-05-03 13:16:11,921 >> tokenizer config file saved in /proj/long-multi/kqian/speech_2/LLaMA-Factory/saves/qwen-14b-1M-R1-CI-round4_v1/full/sft/checkpoint-80/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-05-03 13:16:11,921 >> Special tokens file saved in /proj/long-multi/kqian/speech_2/LLaMA-Factory/saves/qwen-14b-1M-R1-CI-round4_v1/full/sft/checkpoint-80/special_tokens_map.json
 32%|███▏      | 81/255 [14:44<1:13:20, 25.29s/it] 32%|███▏      | 82/255 [14:51<56:48, 19.70s/it]   33%|███▎      | 83/255 [14:57<45:17, 15.80s/it] 33%|███▎      | 84/255 [15:04<37:11, 13.05s/it] 33%|███▎      | 85/255 [15:11<31:35, 11.15s/it] 34%|███▎      | 86/255 [15:14<24:46,  8.79s/it] 34%|███▍      | 87/255 [15:21<22:49,  8.15s/it] 35%|███▍      | 88/255 [15:27<21:31,  7.73s/it] 35%|███▍      | 89/255 [15:34<20:30,  7.42s/it] 35%|███▌      | 90/255 [15:41<19:46,  7.19s/it][INFO|trainer.py:4289] 2025-05-03 13:17:33,862 >> 
***** Running Evaluation *****
[INFO|trainer.py:4291] 2025-05-03 13:17:33,862 >>   Num examples = 757
[INFO|trainer.py:4294] 2025-05-03 13:17:33,862 >>   Batch size = 1

  0%|          | 0/95 [00:00<?, ?it/s][A
  2%|▏         | 2/95 [00:00<00:11,  7.99it/s][A
  3%|▎         | 3/95 [00:00<00:16,  5.63it/s][A
  4%|▍         | 4/95 [00:00<00:18,  4.86it/s][A
  5%|▌         | 5/95 [00:01<00:19,  4.53it/s][A
  6%|▋         | 6/95 [00:01<00:20,  4.31it/s][A
  7%|▋         | 7/95 [00:01<00:20,  4.20it/s][A
  8%|▊         | 8/95 [00:01<00:21,  4.12it/s][A
  9%|▉         | 9/95 [00:02<00:21,  4.08it/s][A
 11%|█         | 10/95 [00:02<00:21,  4.04it/s][A
 12%|█▏        | 11/95 [00:02<00:20,  4.03it/s][A
 13%|█▎        | 12/95 [00:02<00:20,  4.02it/s][A
 14%|█▎        | 13/95 [00:03<00:20,  4.02it/s][A
 15%|█▍        | 14/95 [00:03<00:20,  4.01it/s][A
 16%|█▌        | 15/95 [00:03<00:20,  3.99it/s][A
 17%|█▋        | 16/95 [00:03<00:19,  3.98it/s][A
 18%|█▊        | 17/95 [00:04<00:19,  3.96it/s][A
 19%|█▉        | 18/95 [00:04<00:19,  3.94it/s][A
 20%|██        | 19/95 [00:04<00:19,  3.96it/s][A
 21%|██        | 20/95 [00:04<00:18,  3.95it/s][A
 22%|██▏       | 21/95 [00:05<00:18,  3.94it/s][A
 23%|██▎       | 22/95 [00:05<00:18,  3.95it/s][A
 24%|██▍       | 23/95 [00:05<00:18,  3.96it/s][A
 25%|██▌       | 24/95 [00:05<00:17,  3.97it/s][A
 26%|██▋       | 25/95 [00:06<00:17,  3.96it/s][A
 27%|██▋       | 26/95 [00:06<00:17,  3.95it/s][A
 28%|██▊       | 27/95 [00:06<00:17,  3.95it/s][A
 29%|██▉       | 28/95 [00:06<00:16,  3.96it/s][A
 31%|███       | 29/95 [00:07<00:16,  3.96it/s][A
 32%|███▏      | 30/95 [00:07<00:16,  3.96it/s][A
 33%|███▎      | 31/95 [00:07<00:16,  3.98it/s][A
 34%|███▎      | 32/95 [00:07<00:15,  3.98it/s][A
 35%|███▍      | 33/95 [00:08<00:15,  3.98it/s][A
 36%|███▌      | 34/95 [00:08<00:15,  3.99it/s][A
 37%|███▋      | 35/95 [00:08<00:15,  3.99it/s][A
 38%|███▊      | 36/95 [00:08<00:14,  3.97it/s][A
 39%|███▉      | 37/95 [00:09<00:14,  3.98it/s][A
 40%|████      | 38/95 [00:09<00:14,  3.98it/s][A
 41%|████      | 39/95 [00:09<00:14,  3.97it/s][A
 42%|████▏     | 40/95 [00:09<00:13,  3.99it/s][A
 43%|████▎     | 41/95 [00:10<00:13,  4.01it/s][A
 44%|████▍     | 42/95 [00:10<00:13,  4.01it/s][A
 45%|████▌     | 43/95 [00:10<00:12,  4.01it/s][A
 46%|████▋     | 44/95 [00:10<00:12,  4.01it/s][A
 47%|████▋     | 45/95 [00:11<00:12,  4.00it/s][A
 48%|████▊     | 46/95 [00:11<00:12,  4.00it/s][A
 49%|████▉     | 47/95 [00:11<00:11,  4.00it/s][A
 51%|█████     | 48/95 [00:11<00:11,  4.00it/s][A
 52%|█████▏    | 49/95 [00:12<00:11,  4.01it/s][A
 53%|█████▎    | 50/95 [00:12<00:11,  4.01it/s][A
 54%|█████▎    | 51/95 [00:12<00:11,  4.00it/s][A
 55%|█████▍    | 52/95 [00:12<00:10,  4.00it/s][A
 56%|█████▌    | 53/95 [00:13<00:10,  4.00it/s][A
 57%|█████▋    | 54/95 [00:13<00:10,  3.98it/s][A
 58%|█████▊    | 55/95 [00:13<00:10,  3.98it/s][A
 59%|█████▉    | 56/95 [00:13<00:09,  3.98it/s][A
 60%|██████    | 57/95 [00:14<00:09,  3.99it/s][A
 61%|██████    | 58/95 [00:14<00:09,  3.97it/s][A
 62%|██████▏   | 59/95 [00:14<00:09,  3.98it/s][A
 63%|██████▎   | 60/95 [00:14<00:08,  3.98it/s][A
 64%|██████▍   | 61/95 [00:15<00:08,  3.96it/s][A
 65%|██████▌   | 62/95 [00:15<00:08,  3.96it/s][A
 66%|██████▋   | 63/95 [00:15<00:08,  3.97it/s][A
 67%|██████▋   | 64/95 [00:15<00:07,  3.96it/s][A
 68%|██████▊   | 65/95 [00:16<00:07,  3.96it/s][A
 69%|██████▉   | 66/95 [00:16<00:07,  3.96it/s][A
 71%|███████   | 67/95 [00:16<00:07,  3.96it/s][A
 72%|███████▏  | 68/95 [00:16<00:06,  3.97it/s][A
 73%|███████▎  | 69/95 [00:17<00:06,  3.97it/s][A
 74%|███████▎  | 70/95 [00:17<00:06,  3.98it/s][A
 75%|███████▍  | 71/95 [00:17<00:06,  3.99it/s][A
 76%|███████▌  | 72/95 [00:17<00:05,  3.99it/s][A
 77%|███████▋  | 73/95 [00:18<00:05,  4.00it/s][A
 78%|███████▊  | 74/95 [00:18<00:05,  4.01it/s][A
 79%|███████▉  | 75/95 [00:18<00:05,  4.00it/s][A
 80%|████████  | 76/95 [00:18<00:04,  3.98it/s][A
 81%|████████  | 77/95 [00:19<00:04,  3.96it/s][A
 82%|████████▏ | 78/95 [00:19<00:04,  3.95it/s][A
 83%|████████▎ | 79/95 [00:19<00:04,  3.94it/s][A
 84%|████████▍ | 80/95 [00:19<00:03,  3.96it/s][A
 85%|████████▌ | 81/95 [00:20<00:03,  3.98it/s][A
 86%|████████▋ | 82/95 [00:20<00:03,  4.00it/s][A
 87%|████████▋ | 83/95 [00:20<00:02,  4.01it/s][A
 88%|████████▊ | 84/95 [00:20<00:02,  3.99it/s][A
 89%|████████▉ | 85/95 [00:21<00:02,  3.98it/s][A
 91%|█████████ | 86/95 [00:21<00:02,  3.98it/s][A
 92%|█████████▏| 87/95 [00:21<00:02,  3.97it/s][A
 93%|█████████▎| 88/95 [00:21<00:01,  3.97it/s][A
 94%|█████████▎| 89/95 [00:22<00:01,  3.98it/s][A
 95%|█████████▍| 90/95 [00:22<00:01,  4.00it/s][A
 96%|█████████▌| 91/95 [00:22<00:00,  4.00it/s][A
 97%|█████████▋| 92/95 [00:22<00:00,  3.99it/s][A
 98%|█████████▊| 93/95 [00:23<00:00,  3.99it/s][A
 99%|█████████▉| 94/95 [00:23<00:00,  4.00it/s][A
100%|██████████| 95/95 [00:23<00:00,  4.01it/s][A                                                
                                               [A 35%|███▌      | 90/255 [16:05<19:46,  7.19s/it]
100%|██████████| 95/95 [00:23<00:00,  4.01it/s][A
                                               [A 36%|███▌      | 91/255 [16:11<38:48, 14.20s/it] 36%|███▌      | 92/255 [16:18<32:22, 11.92s/it] 36%|███▋      | 93/255 [16:25<27:58, 10.36s/it] 37%|███▋      | 94/255 [16:31<24:50,  9.26s/it] 37%|███▋      | 95/255 [16:38<22:37,  8.48s/it] 38%|███▊      | 96/255 [16:45<21:01,  7.93s/it] 38%|███▊      | 97/255 [16:51<19:55,  7.57s/it] 38%|███▊      | 98/255 [16:58<18:55,  7.23s/it] 39%|███▉      | 99/255 [17:05<18:26,  7.09s/it] 39%|███▉      | 100/255 [17:11<18:01,  6.98s/it]                                                  39%|███▉      | 100/255 [17:11<18:01,  6.98s/it][INFO|trainer.py:4289] 2025-05-03 13:19:04,398 >> 
***** Running Evaluation *****
[INFO|trainer.py:4291] 2025-05-03 13:19:04,398 >>   Num examples = 757
[INFO|trainer.py:4294] 2025-05-03 13:19:04,398 >>   Batch size = 1

  0%|          | 0/95 [00:00<?, ?it/s][A
  2%|▏         | 2/95 [00:00<00:11,  7.98it/s][A
  3%|▎         | 3/95 [00:00<00:16,  5.62it/s][A
  4%|▍         | 4/95 [00:00<00:18,  4.86it/s][A
  5%|▌         | 5/95 [00:01<00:19,  4.52it/s][A
  6%|▋         | 6/95 [00:01<00:20,  4.33it/s][A
  7%|▋         | 7/95 [00:01<00:20,  4.22it/s][A
  8%|▊         | 8/95 [00:01<00:21,  4.14it/s][A
  9%|▉         | 9/95 [00:02<00:21,  4.09it/s][A
 11%|█         | 10/95 [00:02<00:20,  4.06it/s][A
 12%|█▏        | 11/95 [00:02<00:20,  4.05it/s][A
 13%|█▎        | 12/95 [00:02<00:20,  4.04it/s][A
 14%|█▎        | 13/95 [00:03<00:20,  4.03it/s][A
 15%|█▍        | 14/95 [00:03<00:20,  4.02it/s][A
 16%|█▌        | 15/95 [00:03<00:20,  3.98it/s][A
 17%|█▋        | 16/95 [00:03<00:19,  3.97it/s][A
 18%|█▊        | 17/95 [00:04<00:19,  3.97it/s][A
 19%|█▉        | 18/95 [00:04<00:19,  3.95it/s][A
 20%|██        | 19/95 [00:04<00:19,  3.96it/s][A
 21%|██        | 20/95 [00:04<00:18,  3.96it/s][A
 22%|██▏       | 21/95 [00:05<00:18,  3.95it/s][A
 23%|██▎       | 22/95 [00:05<00:18,  3.95it/s][A
 24%|██▍       | 23/95 [00:05<00:18,  3.96it/s][A
 25%|██▌       | 24/95 [00:05<00:17,  3.97it/s][A
 26%|██▋       | 25/95 [00:06<00:17,  3.97it/s][A
 27%|██▋       | 26/95 [00:06<00:17,  3.96it/s][A
 28%|██▊       | 27/95 [00:06<00:17,  3.96it/s][A
 29%|██▉       | 28/95 [00:06<00:16,  3.96it/s][A
 31%|███       | 29/95 [00:07<00:16,  3.95it/s][A
 32%|███▏      | 30/95 [00:07<00:16,  3.97it/s][A
 33%|███▎      | 31/95 [00:07<00:16,  3.99it/s][A
 34%|███▎      | 32/95 [00:07<00:15,  3.99it/s][A
 35%|███▍      | 33/95 [00:08<00:15,  3.98it/s][A
 36%|███▌      | 34/95 [00:08<00:15,  4.00it/s][A
 37%|███▋      | 35/95 [00:08<00:15,  4.00it/s][A
 38%|███▊      | 36/95 [00:08<00:14,  3.98it/s][A
 39%|███▉      | 37/95 [00:09<00:14,  3.98it/s][A
 40%|████      | 38/95 [00:09<00:14,  3.98it/s][A
 41%|████      | 39/95 [00:09<00:14,  3.98it/s][A
 42%|████▏     | 40/95 [00:09<00:13,  3.99it/s][A
 43%|████▎     | 41/95 [00:10<00:13,  4.00it/s][A
 44%|████▍     | 42/95 [00:10<00:13,  4.00it/s][A
 45%|████▌     | 43/95 [00:10<00:12,  4.00it/s][A
 46%|████▋     | 44/95 [00:10<00:12,  4.01it/s][A
 47%|████▋     | 45/95 [00:11<00:12,  4.01it/s][A
 48%|████▊     | 46/95 [00:11<00:12,  4.02it/s][A
 49%|████▉     | 47/95 [00:11<00:11,  4.01it/s][A
 51%|█████     | 48/95 [00:11<00:11,  4.01it/s][A
 52%|█████▏    | 49/95 [00:12<00:11,  4.02it/s][A
 53%|█████▎    | 50/95 [00:12<00:11,  4.01it/s][A
 54%|█████▎    | 51/95 [00:12<00:11,  4.00it/s][A
 55%|█████▍    | 52/95 [00:12<00:10,  4.00it/s][A
 56%|█████▌    | 53/95 [00:13<00:10,  4.00it/s][A
 57%|█████▋    | 54/95 [00:13<00:10,  4.00it/s][A
 58%|█████▊    | 55/95 [00:13<00:09,  4.00it/s][A
 59%|█████▉    | 56/95 [00:13<00:09,  4.00it/s][A
 60%|██████    | 57/95 [00:14<00:09,  4.01it/s][A
 61%|██████    | 58/95 [00:14<00:09,  3.98it/s][A
 62%|██████▏   | 59/95 [00:14<00:09,  3.97it/s][A
 63%|██████▎   | 60/95 [00:14<00:08,  3.98it/s][A
 64%|██████▍   | 61/95 [00:15<00:08,  3.97it/s][A
 65%|██████▌   | 62/95 [00:15<00:08,  3.97it/s][A
 66%|██████▋   | 63/95 [00:15<00:08,  3.97it/s][A
 67%|██████▋   | 64/95 [00:15<00:07,  3.98it/s][A
 68%|██████▊   | 65/95 [00:16<00:07,  3.98it/s][A
 69%|██████▉   | 66/95 [00:16<00:07,  3.96it/s][A
 71%|███████   | 67/95 [00:16<00:07,  3.96it/s][A
 72%|███████▏  | 68/95 [00:16<00:06,  3.96it/s][A
 73%|███████▎  | 69/95 [00:17<00:06,  3.97it/s][A
 74%|███████▎  | 70/95 [00:17<00:06,  3.98it/s][A
 75%|███████▍  | 71/95 [00:17<00:06,  3.98it/s][A
 76%|███████▌  | 72/95 [00:17<00:05,  3.98it/s][A
 77%|███████▋  | 73/95 [00:18<00:05,  3.99it/s][A
 78%|███████▊  | 74/95 [00:18<00:05,  3.99it/s][A
 79%|███████▉  | 75/95 [00:18<00:05,  3.97it/s][A
 80%|████████  | 76/95 [00:18<00:04,  3.95it/s][A
 81%|████████  | 77/95 [00:19<00:04,  3.95it/s][A
 82%|████████▏ | 78/95 [00:19<00:04,  3.95it/s][A
 83%|████████▎ | 79/95 [00:19<00:04,  3.95it/s][A
 84%|████████▍ | 80/95 [00:19<00:03,  3.97it/s][A
 85%|████████▌ | 81/95 [00:20<00:03,  3.99it/s][A
 86%|████████▋ | 82/95 [00:20<00:03,  4.01it/s][A
 87%|████████▋ | 83/95 [00:20<00:02,  4.01it/s][A
 88%|████████▊ | 84/95 [00:20<00:02,  3.99it/s][A
 89%|████████▉ | 85/95 [00:21<00:02,  3.99it/s][A
 91%|█████████ | 86/95 [00:21<00:02,  3.99it/s][A
 92%|█████████▏| 87/95 [00:21<00:02,  3.95it/s][A
 93%|█████████▎| 88/95 [00:21<00:01,  3.95it/s][A
 94%|█████████▎| 89/95 [00:22<00:01,  3.97it/s][A
 95%|█████████▍| 90/95 [00:22<00:01,  3.99it/s][A
 96%|█████████▌| 91/95 [00:22<00:01,  3.99it/s][A
 97%|█████████▋| 92/95 [00:22<00:00,  3.99it/s][A
 98%|█████████▊| 93/95 [00:23<00:00,  3.99it/s][A
 99%|█████████▉| 94/95 [00:23<00:00,  4.00it/s][A
100%|██████████| 95/95 [00:23<00:00,  4.01it/s][A                                                 
                                               [A 39%|███▉      | 100/255 [17:35<18:01,  6.98s/it]
100%|██████████| 95/95 [00:23<00:00,  4.01it/s][A
                                               [A[INFO|trainer.py:3966] 2025-05-03 13:19:35,634 >> Saving model checkpoint to /proj/long-multi/kqian/speech_2/LLaMA-Factory/saves/qwen-14b-1M-R1-CI-round4_v1/full/sft/checkpoint-100
[INFO|configuration_utils.py:423] 2025-05-03 13:19:35,638 >> Configuration saved in /proj/long-multi/kqian/speech_2/LLaMA-Factory/saves/qwen-14b-1M-R1-CI-round4_v1/full/sft/checkpoint-100/config.json
[INFO|configuration_utils.py:908] 2025-05-03 13:19:35,638 >> Configuration saved in /proj/long-multi/kqian/speech_2/LLaMA-Factory/saves/qwen-14b-1M-R1-CI-round4_v1/full/sft/checkpoint-100/generation_config.json
[INFO|modeling_utils.py:3594] 2025-05-03 13:19:47,490 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 6 checkpoint shards. You can find where each parameters has been saved in the index located at /proj/long-multi/kqian/speech_2/LLaMA-Factory/saves/qwen-14b-1M-R1-CI-round4_v1/full/sft/checkpoint-100/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2510] 2025-05-03 13:19:47,491 >> tokenizer config file saved in /proj/long-multi/kqian/speech_2/LLaMA-Factory/saves/qwen-14b-1M-R1-CI-round4_v1/full/sft/checkpoint-100/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-05-03 13:19:47,491 >> Special tokens file saved in /proj/long-multi/kqian/speech_2/LLaMA-Factory/saves/qwen-14b-1M-R1-CI-round4_v1/full/sft/checkpoint-100/special_tokens_map.json
 40%|███▉      | 101/255 [18:20<1:05:09, 25.38s/it] 40%|████      | 102/255 [18:26<50:23, 19.76s/it]   40%|████      | 103/255 [18:33<40:05, 15.82s/it] 41%|████      | 104/255 [18:40<32:53, 13.07s/it] 41%|████      | 105/255 [18:46<27:50, 11.14s/it] 42%|████▏     | 106/255 [18:53<24:21,  9.81s/it] 42%|████▏     | 107/255 [19:00<21:55,  8.89s/it] 42%|████▏     | 108/255 [19:06<20:08,  8.22s/it] 43%|████▎     | 109/255 [19:13<18:50,  7.74s/it] 43%|████▎     | 110/255 [19:20<17:52,  7.39s/it][INFO|trainer.py:4289] 2025-05-03 13:21:12,592 >> 
***** Running Evaluation *****
[INFO|trainer.py:4291] 2025-05-03 13:21:12,592 >>   Num examples = 757
[INFO|trainer.py:4294] 2025-05-03 13:21:12,592 >>   Batch size = 1

  0%|          | 0/95 [00:00<?, ?it/s][A
  2%|▏         | 2/95 [00:00<00:11,  8.01it/s][A
  3%|▎         | 3/95 [00:00<00:16,  5.64it/s][A
  4%|▍         | 4/95 [00:00<00:18,  4.87it/s][A
  5%|▌         | 5/95 [00:01<00:19,  4.53it/s][A
  6%|▋         | 6/95 [00:01<00:20,  4.34it/s][A
  7%|▋         | 7/95 [00:01<00:20,  4.22it/s][A
  8%|▊         | 8/95 [00:01<00:20,  4.15it/s][A
  9%|▉         | 9/95 [00:02<00:20,  4.10it/s][A
 11%|█         | 10/95 [00:02<00:20,  4.08it/s][A
 12%|█▏        | 11/95 [00:02<00:20,  4.06it/s][A
 13%|█▎        | 12/95 [00:02<00:20,  4.05it/s][A
 14%|█▎        | 13/95 [00:02<00:20,  4.04it/s][A
 15%|█▍        | 14/95 [00:03<00:20,  4.03it/s][A
 16%|█▌        | 15/95 [00:03<00:19,  4.01it/s][A
 17%|█▋        | 16/95 [00:03<00:19,  4.00it/s][A
 18%|█▊        | 17/95 [00:04<00:19,  4.00it/s][A
 19%|█▉        | 18/95 [00:04<00:19,  3.99it/s][A
 20%|██        | 19/95 [00:04<00:19,  4.00it/s][A
 21%|██        | 20/95 [00:04<00:18,  3.98it/s][A
 22%|██▏       | 21/95 [00:05<00:18,  3.95it/s][A
 23%|██▎       | 22/95 [00:05<00:18,  3.95it/s][A
 24%|██▍       | 23/95 [00:05<00:18,  3.96it/s][A
 25%|██▌       | 24/95 [00:05<00:17,  3.97it/s][A
 26%|██▋       | 25/95 [00:06<00:17,  3.97it/s][A
 27%|██▋       | 26/95 [00:06<00:17,  3.95it/s][A
 28%|██▊       | 27/95 [00:06<00:17,  3.96it/s][A
 29%|██▉       | 28/95 [00:06<00:16,  3.97it/s][A
 31%|███       | 29/95 [00:07<00:16,  3.97it/s][A
 32%|███▏      | 30/95 [00:07<00:16,  3.98it/s][A
 33%|███▎      | 31/95 [00:07<00:16,  3.99it/s][A
 34%|███▎      | 32/95 [00:07<00:15,  3.99it/s][A
 35%|███▍      | 33/95 [00:08<00:15,  3.99it/s][A
 36%|███▌      | 34/95 [00:08<00:15,  4.00it/s][A
 37%|███▋      | 35/95 [00:08<00:15,  4.00it/s][A
 38%|███▊      | 36/95 [00:08<00:14,  3.99it/s][A
 39%|███▉      | 37/95 [00:09<00:14,  3.99it/s][A
 40%|████      | 38/95 [00:09<00:14,  3.99it/s][A
 41%|████      | 39/95 [00:09<00:14,  3.99it/s][A
 42%|████▏     | 40/95 [00:09<00:13,  4.01it/s][A
 43%|████▎     | 41/95 [00:10<00:13,  4.01it/s][A
 44%|████▍     | 42/95 [00:10<00:13,  3.98it/s][A
 45%|████▌     | 43/95 [00:10<00:13,  3.98it/s][A
 46%|████▋     | 44/95 [00:10<00:12,  3.98it/s][A
 47%|████▋     | 45/95 [00:11<00:12,  3.98it/s][A
 48%|████▊     | 46/95 [00:11<00:12,  4.00it/s][A
 49%|████▉     | 47/95 [00:11<00:11,  4.00it/s][A
 51%|█████     | 48/95 [00:11<00:11,  4.01it/s][A
 52%|█████▏    | 49/95 [00:12<00:11,  4.02it/s][A
 53%|█████▎    | 50/95 [00:12<00:11,  4.01it/s][A
 54%|█████▎    | 51/95 [00:12<00:10,  4.01it/s][A
 55%|█████▍    | 52/95 [00:12<00:10,  4.00it/s][A
 56%|█████▌    | 53/95 [00:13<00:10,  4.01it/s][A
 57%|█████▋    | 54/95 [00:13<00:10,  4.00it/s][A
 58%|█████▊    | 55/95 [00:13<00:09,  4.01it/s][A
 59%|█████▉    | 56/95 [00:13<00:09,  4.01it/s][A
 60%|██████    | 57/95 [00:14<00:09,  4.02it/s][A
 61%|██████    | 58/95 [00:14<00:09,  4.00it/s][A
 62%|██████▏   | 59/95 [00:14<00:08,  4.00it/s][A
 63%|██████▎   | 60/95 [00:14<00:08,  3.99it/s][A
 64%|██████▍   | 61/95 [00:15<00:08,  3.99it/s][A
 65%|██████▌   | 62/95 [00:15<00:08,  3.98it/s][A
 66%|██████▋   | 63/95 [00:15<00:08,  3.97it/s][A
 67%|██████▋   | 64/95 [00:15<00:07,  3.96it/s][A
 68%|██████▊   | 65/95 [00:16<00:07,  3.96it/s][A
 69%|██████▉   | 66/95 [00:16<00:07,  3.97it/s][A
 71%|███████   | 67/95 [00:16<00:07,  3.97it/s][A
 72%|███████▏  | 68/95 [00:16<00:06,  3.98it/s][A
 73%|███████▎  | 69/95 [00:17<00:06,  3.97it/s][A
 74%|███████▎  | 70/95 [00:17<00:06,  3.99it/s][A
 75%|███████▍  | 71/95 [00:17<00:06,  3.99it/s][A
 76%|███████▌  | 72/95 [00:17<00:05,  4.00it/s][A
 77%|███████▋  | 73/95 [00:18<00:05,  4.01it/s][A
 78%|███████▊  | 74/95 [00:18<00:05,  4.02it/s][A
 79%|███████▉  | 75/95 [00:18<00:04,  4.00it/s][A
 80%|████████  | 76/95 [00:18<00:04,  4.00it/s][A
 81%|████████  | 77/95 [00:19<00:04,  3.99it/s][A
 82%|████████▏ | 78/95 [00:19<00:04,  3.99it/s][A
 83%|████████▎ | 79/95 [00:19<00:04,  3.99it/s][A
 84%|████████▍ | 80/95 [00:19<00:03,  4.00it/s][A
 85%|████████▌ | 81/95 [00:20<00:03,  4.01it/s][A
 86%|████████▋ | 82/95 [00:20<00:03,  4.03it/s][A
 87%|████████▋ | 83/95 [00:20<00:02,  4.04it/s][A
 88%|████████▊ | 84/95 [00:20<00:02,  4.01it/s][A
 89%|████████▉ | 85/95 [00:21<00:02,  4.01it/s][A
 91%|█████████ | 86/95 [00:21<00:02,  4.00it/s][A
 92%|█████████▏| 87/95 [00:21<00:02,  3.99it/s][A
 93%|█████████▎| 88/95 [00:21<00:01,  3.98it/s][A
 94%|█████████▎| 89/95 [00:22<00:01,  4.00it/s][A
 95%|█████████▍| 90/95 [00:22<00:01,  4.01it/s][A
 96%|█████████▌| 91/95 [00:22<00:00,  4.01it/s][A
 97%|█████████▋| 92/95 [00:22<00:00,  4.00it/s][A
 98%|█████████▊| 93/95 [00:23<00:00,  4.01it/s][A
 99%|█████████▉| 94/95 [00:23<00:00,  4.01it/s][A
100%|██████████| 95/95 [00:23<00:00,  4.02it/s][A                                                 
                                               [A 43%|████▎     | 110/255 [19:43<17:52,  7.39s/it]
100%|██████████| 95/95 [00:23<00:00,  4.02it/s][A
                                               [A 44%|████▎     | 111/255 [19:50<34:24, 14.34s/it] 44%|████▍     | 112/255 [19:57<28:41, 12.04s/it] 44%|████▍     | 113/255 [20:03<24:41, 10.43s/it] 45%|████▍     | 114/255 [20:10<21:55,  9.33s/it] 45%|████▌     | 115/255 [20:17<19:58,  8.56s/it] 45%|████▌     | 116/255 [20:24<18:30,  7.99s/it] 46%|████▌     | 117/255 [20:30<17:29,  7.60s/it] 46%|████▋     | 118/255 [20:37<16:44,  7.33s/it] 47%|████▋     | 119/255 [20:44<16:07,  7.12s/it] 47%|████▋     | 120/255 [20:50<15:32,  6.91s/it]                                                  47%|████▋     | 120/255 [20:50<15:32,  6.91s/it][INFO|trainer.py:4289] 2025-05-03 13:22:43,091 >> 
***** Running Evaluation *****
[INFO|trainer.py:4291] 2025-05-03 13:22:43,091 >>   Num examples = 757
[INFO|trainer.py:4294] 2025-05-03 13:22:43,091 >>   Batch size = 1

  0%|          | 0/95 [00:00<?, ?it/s][A
  2%|▏         | 2/95 [00:00<00:11,  8.00it/s][A
  3%|▎         | 3/95 [00:00<00:16,  5.63it/s][A
  4%|▍         | 4/95 [00:00<00:18,  4.88it/s][A
  5%|▌         | 5/95 [00:01<00:19,  4.54it/s][A
  6%|▋         | 6/95 [00:01<00:20,  4.35it/s][A
  7%|▋         | 7/95 [00:01<00:20,  4.23it/s][A
  8%|▊         | 8/95 [00:01<00:20,  4.15it/s][A
  9%|▉         | 9/95 [00:01<00:20,  4.11it/s][A
 11%|█         | 10/95 [00:02<00:20,  4.08it/s][A
 12%|█▏        | 11/95 [00:02<00:20,  4.06it/s][A
 13%|█▎        | 12/95 [00:02<00:20,  4.06it/s][A
 14%|█▎        | 13/95 [00:02<00:20,  4.05it/s][A
 15%|█▍        | 14/95 [00:03<00:20,  4.04it/s][A
 16%|█▌        | 15/95 [00:03<00:19,  4.02it/s][A
 17%|█▋        | 16/95 [00:03<00:19,  4.01it/s][A
 18%|█▊        | 17/95 [00:03<00:19,  4.01it/s][A
 19%|█▉        | 18/95 [00:04<00:19,  4.00it/s][A
 20%|██        | 19/95 [00:04<00:18,  4.01it/s][A
 21%|██        | 20/95 [00:04<00:18,  4.00it/s][A
 22%|██▏       | 21/95 [00:04<00:18,  3.99it/s][A
 23%|██▎       | 22/95 [00:05<00:18,  3.99it/s][A
 24%|██▍       | 23/95 [00:05<00:18,  3.97it/s][A
 25%|██▌       | 24/95 [00:05<00:17,  3.98it/s][A
 26%|██▋       | 25/95 [00:06<00:17,  3.98it/s][A
 27%|██▋       | 26/95 [00:06<00:17,  3.97it/s][A
 28%|██▊       | 27/95 [00:06<00:17,  3.98it/s][A
 29%|██▉       | 28/95 [00:06<00:16,  3.98it/s][A
 31%|███       | 29/95 [00:07<00:16,  3.97it/s][A
 32%|███▏      | 30/95 [00:07<00:16,  3.98it/s][A
 33%|███▎      | 31/95 [00:07<00:16,  4.00it/s][A
 34%|███▎      | 32/95 [00:07<00:15,  4.00it/s][A
 35%|███▍      | 33/95 [00:08<00:15,  3.99it/s][A
 36%|███▌      | 34/95 [00:08<00:15,  4.01it/s][A
 37%|███▋      | 35/95 [00:08<00:14,  4.00it/s][A
 38%|███▊      | 36/95 [00:08<00:14,  3.99it/s][A
 39%|███▉      | 37/95 [00:09<00:14,  3.99it/s][A
 40%|████      | 38/95 [00:09<00:14,  3.99it/s][A
 41%|████      | 39/95 [00:09<00:14,  4.00it/s][A
 42%|████▏     | 40/95 [00:09<00:13,  4.00it/s][A
 43%|████▎     | 41/95 [00:10<00:13,  3.99it/s][A
 44%|████▍     | 42/95 [00:10<00:13,  4.00it/s][A
 45%|████▌     | 43/95 [00:10<00:13,  3.99it/s][A
 46%|████▋     | 44/95 [00:10<00:12,  4.00it/s][A
 47%|████▋     | 45/95 [00:11<00:12,  4.00it/s][A
 48%|████▊     | 46/95 [00:11<00:12,  4.02it/s][A
 49%|████▉     | 47/95 [00:11<00:11,  4.02it/s][A
 51%|█████     | 48/95 [00:11<00:11,  4.02it/s][A
 52%|█████▏    | 49/95 [00:11<00:11,  4.04it/s][A
 53%|█████▎    | 50/95 [00:12<00:11,  4.03it/s][A
 54%|█████▎    | 51/95 [00:12<00:10,  4.02it/s][A
 55%|█████▍    | 52/95 [00:12<00:10,  4.01it/s][A
 56%|█████▌    | 53/95 [00:12<00:10,  4.02it/s][A
 57%|█████▋    | 54/95 [00:13<00:10,  4.01it/s][A
 58%|█████▊    | 55/95 [00:13<00:09,  4.01it/s][A
 59%|█████▉    | 56/95 [00:13<00:09,  4.01it/s][A
 60%|██████    | 57/95 [00:13<00:09,  4.02it/s][A
 61%|██████    | 58/95 [00:14<00:09,  4.00it/s][A
 62%|██████▏   | 59/95 [00:14<00:09,  3.99it/s][A
 63%|██████▎   | 60/95 [00:14<00:08,  4.00it/s][A
 64%|██████▍   | 61/95 [00:14<00:08,  3.99it/s][A
 65%|██████▌   | 62/95 [00:15<00:08,  3.99it/s][A
 66%|██████▋   | 63/95 [00:15<00:08,  3.99it/s][A
 67%|██████▋   | 64/95 [00:15<00:07,  3.99it/s][A
 68%|██████▊   | 65/95 [00:16<00:07,  4.00it/s][A
 69%|██████▉   | 66/95 [00:16<00:07,  3.98it/s][A
 71%|███████   | 67/95 [00:16<00:07,  3.97it/s][A
 72%|███████▏  | 68/95 [00:16<00:06,  3.98it/s][A
 73%|███████▎  | 69/95 [00:17<00:06,  3.97it/s][A
 74%|███████▎  | 70/95 [00:17<00:06,  3.99it/s][A
 75%|███████▍  | 71/95 [00:17<00:06,  4.00it/s][A
 76%|███████▌  | 72/95 [00:17<00:05,  3.99it/s][A
 77%|███████▋  | 73/95 [00:18<00:05,  4.00it/s][A
 78%|███████▊  | 74/95 [00:18<00:05,  4.02it/s][A
 79%|███████▉  | 75/95 [00:18<00:04,  4.01it/s][A
 80%|████████  | 76/95 [00:18<00:04,  4.00it/s][A
 81%|████████  | 77/95 [00:19<00:04,  4.00it/s][A
 82%|████████▏ | 78/95 [00:19<00:04,  3.99it/s][A
 83%|████████▎ | 79/95 [00:19<00:04,  3.99it/s][A
 84%|████████▍ | 80/95 [00:19<00:03,  4.01it/s][A
 85%|████████▌ | 81/95 [00:20<00:03,  4.02it/s][A
 86%|████████▋ | 82/95 [00:20<00:03,  4.03it/s][A
 87%|████████▋ | 83/95 [00:20<00:02,  4.04it/s][A
 88%|████████▊ | 84/95 [00:20<00:02,  4.02it/s][A
 89%|████████▉ | 85/95 [00:20<00:02,  4.01it/s][A
 91%|█████████ | 86/95 [00:21<00:02,  4.01it/s][A
 92%|█████████▏| 87/95 [00:21<00:02,  4.00it/s][A
 93%|█████████▎| 88/95 [00:21<00:01,  4.00it/s][A
 94%|█████████▎| 89/95 [00:21<00:01,  4.00it/s][A
 95%|█████████▍| 90/95 [00:22<00:01,  4.02it/s][A
 96%|█████████▌| 91/95 [00:22<00:00,  4.02it/s][A
 97%|█████████▋| 92/95 [00:22<00:00,  4.01it/s][A
 98%|█████████▊| 93/95 [00:22<00:00,  4.02it/s][A
 99%|█████████▉| 94/95 [00:23<00:00,  4.02it/s][A
100%|██████████| 95/95 [00:23<00:00,  4.03it/s][A                                                 
                                               [A 47%|████▋     | 120/255 [21:14<15:32,  6.91s/it]
100%|██████████| 95/95 [00:23<00:00,  4.03it/s][A
                                               [A[INFO|trainer.py:3966] 2025-05-03 13:23:14,393 >> Saving model checkpoint to /proj/long-multi/kqian/speech_2/LLaMA-Factory/saves/qwen-14b-1M-R1-CI-round4_v1/full/sft/checkpoint-120
[INFO|configuration_utils.py:423] 2025-05-03 13:23:14,396 >> Configuration saved in /proj/long-multi/kqian/speech_2/LLaMA-Factory/saves/qwen-14b-1M-R1-CI-round4_v1/full/sft/checkpoint-120/config.json
[INFO|configuration_utils.py:908] 2025-05-03 13:23:14,397 >> Configuration saved in /proj/long-multi/kqian/speech_2/LLaMA-Factory/saves/qwen-14b-1M-R1-CI-round4_v1/full/sft/checkpoint-120/generation_config.json
[INFO|modeling_utils.py:3594] 2025-05-03 13:23:26,217 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 6 checkpoint shards. You can find where each parameters has been saved in the index located at /proj/long-multi/kqian/speech_2/LLaMA-Factory/saves/qwen-14b-1M-R1-CI-round4_v1/full/sft/checkpoint-120/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2510] 2025-05-03 13:23:26,218 >> tokenizer config file saved in /proj/long-multi/kqian/speech_2/LLaMA-Factory/saves/qwen-14b-1M-R1-CI-round4_v1/full/sft/checkpoint-120/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-05-03 13:23:26,218 >> Special tokens file saved in /proj/long-multi/kqian/speech_2/LLaMA-Factory/saves/qwen-14b-1M-R1-CI-round4_v1/full/sft/checkpoint-120/special_tokens_map.json
 47%|████▋     | 121/255 [21:59<56:42, 25.39s/it] 48%|████▊     | 122/255 [22:05<43:46, 19.75s/it] 48%|████▊     | 123/255 [22:12<34:52, 15.85s/it] 49%|████▊     | 124/255 [22:19<28:38, 13.11s/it] 49%|████▉     | 125/255 [22:25<24:15, 11.20s/it] 49%|████▉     | 126/255 [22:32<21:10,  9.85s/it] 50%|████▉     | 127/255 [22:39<18:59,  8.90s/it] 50%|█████     | 128/255 [22:45<17:23,  8.22s/it] 51%|█████     | 129/255 [22:52<16:11,  7.71s/it] 51%|█████     | 130/255 [22:59<15:24,  7.39s/it][INFO|trainer.py:4289] 2025-05-03 13:24:51,605 >> 
***** Running Evaluation *****
[INFO|trainer.py:4291] 2025-05-03 13:24:51,606 >>   Num examples = 757
[INFO|trainer.py:4294] 2025-05-03 13:24:51,606 >>   Batch size = 1

  0%|          | 0/95 [00:00<?, ?it/s][A
  2%|▏         | 2/95 [00:00<00:11,  7.98it/s][A
  3%|▎         | 3/95 [00:00<00:16,  5.62it/s][A
  4%|▍         | 4/95 [00:00<00:18,  4.85it/s][A
  5%|▌         | 5/95 [00:01<00:19,  4.51it/s][A
  6%|▋         | 6/95 [00:01<00:20,  4.29it/s][A
  7%|▋         | 7/95 [00:01<00:21,  4.18it/s][A
  8%|▊         | 8/95 [00:01<00:21,  4.10it/s][A
  9%|▉         | 9/95 [00:02<00:21,  4.07it/s][A
 11%|█         | 10/95 [00:02<00:21,  4.04it/s][A
 12%|█▏        | 11/95 [00:02<00:20,  4.03it/s][A
 13%|█▎        | 12/95 [00:02<00:20,  4.03it/s][A
 14%|█▎        | 13/95 [00:03<00:20,  4.03it/s][A
 15%|█▍        | 14/95 [00:03<00:20,  4.03it/s][A
 16%|█▌        | 15/95 [00:03<00:20,  4.00it/s][A
 17%|█▋        | 16/95 [00:03<00:19,  3.99it/s][A
 18%|█▊        | 17/95 [00:04<00:19,  3.98it/s][A
 19%|█▉        | 18/95 [00:04<00:19,  3.98it/s][A
 20%|██        | 19/95 [00:04<00:19,  3.99it/s][A
 21%|██        | 20/95 [00:04<00:18,  3.98it/s][A
 22%|██▏       | 21/95 [00:05<00:18,  3.97it/s][A
 23%|██▎       | 22/95 [00:05<00:18,  3.97it/s][A
 24%|██▍       | 23/95 [00:05<00:18,  3.98it/s][A
 25%|██▌       | 24/95 [00:05<00:17,  3.98it/s][A
 26%|██▋       | 25/95 [00:06<00:17,  3.99it/s][A
 27%|██▋       | 26/95 [00:06<00:17,  3.98it/s][A
 28%|██▊       | 27/95 [00:06<00:17,  3.97it/s][A
 29%|██▉       | 28/95 [00:06<00:16,  3.98it/s][A
 31%|███       | 29/95 [00:07<00:16,  3.97it/s][A
 32%|███▏      | 30/95 [00:07<00:16,  3.96it/s][A
 33%|███▎      | 31/95 [00:07<00:16,  3.96it/s][A
 34%|███▎      | 32/95 [00:07<00:15,  3.96it/s][A
 35%|███▍      | 33/95 [00:08<00:15,  3.96it/s][A
 36%|███▌      | 34/95 [00:08<00:15,  3.98it/s][A
 37%|███▋      | 35/95 [00:08<00:15,  3.98it/s][A
 38%|███▊      | 36/95 [00:08<00:14,  3.97it/s][A
 39%|███▉      | 37/95 [00:09<00:14,  3.97it/s][A
 40%|████      | 38/95 [00:09<00:14,  3.97it/s][A
 41%|████      | 39/95 [00:09<00:14,  3.98it/s][A
 42%|████▏     | 40/95 [00:09<00:13,  3.99it/s][A
 43%|████▎     | 41/95 [00:10<00:13,  4.01it/s][A
 44%|████▍     | 42/95 [00:10<00:13,  4.01it/s][A
 45%|████▌     | 43/95 [00:10<00:12,  4.02it/s][A
 46%|████▋     | 44/95 [00:10<00:12,  4.02it/s][A
 47%|████▋     | 45/95 [00:11<00:12,  4.01it/s][A
 48%|████▊     | 46/95 [00:11<00:12,  4.03it/s][A
 49%|████▉     | 47/95 [00:11<00:11,  4.02it/s][A
 51%|█████     | 48/95 [00:11<00:11,  4.01it/s][A
 52%|█████▏    | 49/95 [00:12<00:11,  4.02it/s][A
 53%|█████▎    | 50/95 [00:12<00:11,  4.02it/s][A
 54%|█████▎    | 51/95 [00:12<00:10,  4.00it/s][A
 55%|█████▍    | 52/95 [00:12<00:10,  4.00it/s][A
 56%|█████▌    | 53/95 [00:13<00:10,  4.00it/s][A
 57%|█████▋    | 54/95 [00:13<00:10,  4.00it/s][A
 58%|█████▊    | 55/95 [00:13<00:09,  4.01it/s][A
 59%|█████▉    | 56/95 [00:13<00:09,  4.00it/s][A
 60%|██████    | 57/95 [00:14<00:09,  4.01it/s][A
 61%|██████    | 58/95 [00:14<00:09,  3.99it/s][A
 62%|██████▏   | 59/95 [00:14<00:09,  3.98it/s][A
 63%|██████▎   | 60/95 [00:14<00:08,  3.98it/s][A
 64%|██████▍   | 61/95 [00:15<00:08,  3.98it/s][A
 65%|██████▌   | 62/95 [00:15<00:08,  3.97it/s][A
 66%|██████▋   | 63/95 [00:15<00:08,  3.98it/s][A
 67%|██████▋   | 64/95 [00:15<00:07,  3.98it/s][A
 68%|██████▊   | 65/95 [00:16<00:07,  3.97it/s][A
 69%|██████▉   | 66/95 [00:16<00:07,  3.95it/s][A
 71%|███████   | 67/95 [00:16<00:07,  3.94it/s][A
 72%|███████▏  | 68/95 [00:16<00:06,  3.95it/s][A
 73%|███████▎  | 69/95 [00:17<00:06,  3.96it/s][A
 74%|███████▎  | 70/95 [00:17<00:06,  3.97it/s][A
 75%|███████▍  | 71/95 [00:17<00:06,  3.98it/s][A
 76%|███████▌  | 72/95 [00:17<00:05,  3.98it/s][A
 77%|███████▋  | 73/95 [00:18<00:05,  3.98it/s][A
 78%|███████▊  | 74/95 [00:18<00:05,  3.99it/s][A
 79%|███████▉  | 75/95 [00:18<00:05,  3.97it/s][A
 80%|████████  | 76/95 [00:18<00:04,  3.98it/s][A
 81%|████████  | 77/95 [00:19<00:04,  3.97it/s][A
 82%|████████▏ | 78/95 [00:19<00:04,  3.96it/s][A
 83%|████████▎ | 79/95 [00:19<00:04,  3.96it/s][A
 84%|████████▍ | 80/95 [00:19<00:03,  3.98it/s][A
 85%|████████▌ | 81/95 [00:20<00:03,  4.00it/s][A
 86%|████████▋ | 82/95 [00:20<00:03,  4.01it/s][A
 87%|████████▋ | 83/95 [00:20<00:02,  4.02it/s][A
 88%|████████▊ | 84/95 [00:20<00:02,  4.00it/s][A
 89%|████████▉ | 85/95 [00:21<00:02,  3.99it/s][A
 91%|█████████ | 86/95 [00:21<00:02,  3.99it/s][A
 92%|█████████▏| 87/95 [00:21<00:02,  3.98it/s][A
 93%|█████████▎| 88/95 [00:21<00:01,  3.97it/s][A
 94%|█████████▎| 89/95 [00:22<00:01,  3.99it/s][A
 95%|█████████▍| 90/95 [00:22<00:01,  4.01it/s][A
 96%|█████████▌| 91/95 [00:22<00:00,  4.01it/s][A
 97%|█████████▋| 92/95 [00:22<00:00,  3.99it/s][A
 98%|█████████▊| 93/95 [00:23<00:00,  4.00it/s][A
 99%|█████████▉| 94/95 [00:23<00:00,  4.01it/s][A
100%|██████████| 95/95 [00:23<00:00,  4.00it/s][A                                                 
                                               [A 51%|█████     | 130/255 [23:22<15:24,  7.39s/it]
100%|██████████| 95/95 [00:23<00:00,  4.00it/s][A
                                               [A 51%|█████▏    | 131/255 [23:29<29:37, 14.34s/it] 52%|█████▏    | 132/255 [23:36<24:41, 12.05s/it] 52%|█████▏    | 133/255 [23:43<21:15, 10.46s/it] 53%|█████▎    | 134/255 [23:49<18:47,  9.32s/it] 53%|█████▎    | 135/255 [23:56<16:59,  8.50s/it] 53%|█████▎    | 136/255 [24:02<15:32,  7.84s/it] 54%|█████▎    | 137/255 [24:09<14:44,  7.49s/it] 54%|█████▍    | 138/255 [24:16<14:10,  7.27s/it] 55%|█████▍    | 139/255 [24:22<13:40,  7.08s/it] 55%|█████▍    | 140/255 [24:29<13:23,  6.99s/it]                                                  55%|█████▍    | 140/255 [24:29<13:23,  6.99s/it][INFO|trainer.py:4289] 2025-05-03 13:26:21,991 >> 
***** Running Evaluation *****
[INFO|trainer.py:4291] 2025-05-03 13:26:21,992 >>   Num examples = 757
[INFO|trainer.py:4294] 2025-05-03 13:26:21,992 >>   Batch size = 1

  0%|          | 0/95 [00:00<?, ?it/s][A
  2%|▏         | 2/95 [00:00<00:11,  8.00it/s][A
  3%|▎         | 3/95 [00:00<00:16,  5.64it/s][A
  4%|▍         | 4/95 [00:00<00:18,  4.84it/s][A
  5%|▌         | 5/95 [00:01<00:20,  4.49it/s][A
  6%|▋         | 6/95 [00:01<00:20,  4.29it/s][A
  7%|▋         | 7/95 [00:01<00:21,  4.19it/s][A
  8%|▊         | 8/95 [00:01<00:21,  4.12it/s][A
  9%|▉         | 9/95 [00:02<00:21,  4.08it/s][A
 11%|█         | 10/95 [00:02<00:20,  4.06it/s][A
 12%|█▏        | 11/95 [00:02<00:20,  4.04it/s][A
 13%|█▎        | 12/95 [00:02<00:20,  4.04it/s][A
 14%|█▎        | 13/95 [00:03<00:20,  4.04it/s][A
 15%|█▍        | 14/95 [00:03<00:20,  4.03it/s][A
 16%|█▌        | 15/95 [00:03<00:19,  4.00it/s][A
 17%|█▋        | 16/95 [00:03<00:19,  4.00it/s][A
 18%|█▊        | 17/95 [00:04<00:19,  4.00it/s][A
 19%|█▉        | 18/95 [00:04<00:19,  3.99it/s][A
 20%|██        | 19/95 [00:04<00:18,  4.00it/s][A
 21%|██        | 20/95 [00:04<00:18,  4.00it/s][A
 22%|██▏       | 21/95 [00:05<00:18,  3.98it/s][A
 23%|██▎       | 22/95 [00:05<00:18,  3.97it/s][A
 24%|██▍       | 23/95 [00:05<00:18,  3.99it/s][A
 25%|██▌       | 24/95 [00:05<00:17,  3.99it/s][A
 26%|██▋       | 25/95 [00:06<00:17,  3.99it/s][A
 27%|██▋       | 26/95 [00:06<00:17,  3.98it/s][A
 28%|██▊       | 27/95 [00:06<00:17,  3.98it/s][A
 29%|██▉       | 28/95 [00:06<00:16,  3.98it/s][A
 31%|███       | 29/95 [00:07<00:16,  3.98it/s][A
 32%|███▏      | 30/95 [00:07<00:16,  3.99it/s][A
 33%|███▎      | 31/95 [00:07<00:16,  4.00it/s][A
 34%|███▎      | 32/95 [00:07<00:15,  3.99it/s][A
 35%|███▍      | 33/95 [00:08<00:15,  3.99it/s][A
 36%|███▌      | 34/95 [00:08<00:15,  4.00it/s][A
 37%|███▋      | 35/95 [00:08<00:14,  4.00it/s][A
 38%|███▊      | 36/95 [00:08<00:14,  3.99it/s][A
 39%|███▉      | 37/95 [00:09<00:14,  3.97it/s][A
 40%|████      | 38/95 [00:09<00:14,  3.96it/s][A
 41%|████      | 39/95 [00:09<00:14,  3.97it/s][A
 42%|████▏     | 40/95 [00:09<00:13,  3.99it/s][A
 43%|████▎     | 41/95 [00:10<00:13,  4.01it/s][A
 44%|████▍     | 42/95 [00:10<00:13,  4.00it/s][A
 45%|████▌     | 43/95 [00:10<00:12,  4.01it/s][A
 46%|████▋     | 44/95 [00:10<00:12,  4.01it/s][A
 47%|████▋     | 45/95 [00:11<00:12,  4.02it/s][A
 48%|████▊     | 46/95 [00:11<00:12,  4.02it/s][A
 49%|████▉     | 47/95 [00:11<00:11,  4.02it/s][A
 51%|█████     | 48/95 [00:11<00:11,  4.01it/s][A
 52%|█████▏    | 49/95 [00:12<00:11,  4.03it/s][A
 53%|█████▎    | 50/95 [00:12<00:11,  4.02it/s][A
 54%|█████▎    | 51/95 [00:12<00:10,  4.00it/s][A
 55%|█████▍    | 52/95 [00:12<00:10,  4.00it/s][A
 56%|█████▌    | 53/95 [00:13<00:10,  4.01it/s][A
 57%|█████▋    | 54/95 [00:13<00:10,  4.01it/s][A
 58%|█████▊    | 55/95 [00:13<00:09,  4.01it/s][A
 59%|█████▉    | 56/95 [00:13<00:09,  4.01it/s][A
 60%|██████    | 57/95 [00:14<00:09,  4.02it/s][A
 61%|██████    | 58/95 [00:14<00:09,  3.99it/s][A
 62%|██████▏   | 59/95 [00:14<00:09,  3.99it/s][A
 63%|██████▎   | 60/95 [00:14<00:08,  4.00it/s][A
 64%|██████▍   | 61/95 [00:15<00:08,  3.97it/s][A
 65%|██████▌   | 62/95 [00:15<00:08,  3.95it/s][A
 66%|██████▋   | 63/95 [00:15<00:08,  3.96it/s][A
 67%|██████▋   | 64/95 [00:15<00:07,  3.95it/s][A
 68%|██████▊   | 65/95 [00:16<00:07,  3.95it/s][A
 69%|██████▉   | 66/95 [00:16<00:07,  3.95it/s][A
 71%|███████   | 67/95 [00:16<00:07,  3.97it/s][A
 72%|███████▏  | 68/95 [00:16<00:06,  3.98it/s][A
 73%|███████▎  | 69/95 [00:17<00:06,  3.98it/s][A
 74%|███████▎  | 70/95 [00:17<00:06,  3.99it/s][A
 75%|███████▍  | 71/95 [00:17<00:06,  4.00it/s][A
 76%|███████▌  | 72/95 [00:17<00:05,  4.00it/s][A
 77%|███████▋  | 73/95 [00:18<00:05,  4.00it/s][A
 78%|███████▊  | 74/95 [00:18<00:05,  4.01it/s][A
 79%|███████▉  | 75/95 [00:18<00:05,  4.00it/s][A
 80%|████████  | 76/95 [00:18<00:04,  4.00it/s][A
 81%|████████  | 77/95 [00:19<00:04,  3.98it/s][A
 82%|████████▏ | 78/95 [00:19<00:04,  3.98it/s][A
 83%|████████▎ | 79/95 [00:19<00:04,  3.97it/s][A
 84%|████████▍ | 80/95 [00:19<00:03,  3.98it/s][A
 85%|████████▌ | 81/95 [00:20<00:03,  3.99it/s][A
 86%|████████▋ | 82/95 [00:20<00:03,  4.02it/s][A
 87%|████████▋ | 83/95 [00:20<00:02,  4.03it/s][A
 88%|████████▊ | 84/95 [00:20<00:02,  4.02it/s][A
 89%|████████▉ | 85/95 [00:21<00:02,  4.01it/s][A
 91%|█████████ | 86/95 [00:21<00:02,  4.01it/s][A
 92%|█████████▏| 87/95 [00:21<00:02,  3.99it/s][A
 93%|█████████▎| 88/95 [00:21<00:01,  3.96it/s][A
 94%|█████████▎| 89/95 [00:22<00:01,  3.98it/s][A
 95%|█████████▍| 90/95 [00:22<00:01,  4.00it/s][A
 96%|█████████▌| 91/95 [00:22<00:00,  4.01it/s][A
 97%|█████████▋| 92/95 [00:22<00:00,  3.99it/s][A
 98%|█████████▊| 93/95 [00:23<00:00,  4.00it/s][A
 99%|█████████▉| 94/95 [00:23<00:00,  4.02it/s][A
100%|██████████| 95/95 [00:23<00:00,  4.03it/s][A                                                 
                                               [A 55%|█████▍    | 140/255 [24:53<13:23,  6.99s/it]
100%|██████████| 95/95 [00:23<00:00,  4.03it/s][A
                                               [A[INFO|trainer.py:3966] 2025-05-03 13:26:53,334 >> Saving model checkpoint to /proj/long-multi/kqian/speech_2/LLaMA-Factory/saves/qwen-14b-1M-R1-CI-round4_v1/full/sft/checkpoint-140
[INFO|configuration_utils.py:423] 2025-05-03 13:26:53,338 >> Configuration saved in /proj/long-multi/kqian/speech_2/LLaMA-Factory/saves/qwen-14b-1M-R1-CI-round4_v1/full/sft/checkpoint-140/config.json
[INFO|configuration_utils.py:908] 2025-05-03 13:26:53,338 >> Configuration saved in /proj/long-multi/kqian/speech_2/LLaMA-Factory/saves/qwen-14b-1M-R1-CI-round4_v1/full/sft/checkpoint-140/generation_config.json
[INFO|modeling_utils.py:3594] 2025-05-03 13:27:05,168 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 6 checkpoint shards. You can find where each parameters has been saved in the index located at /proj/long-multi/kqian/speech_2/LLaMA-Factory/saves/qwen-14b-1M-R1-CI-round4_v1/full/sft/checkpoint-140/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2510] 2025-05-03 13:27:05,169 >> tokenizer config file saved in /proj/long-multi/kqian/speech_2/LLaMA-Factory/saves/qwen-14b-1M-R1-CI-round4_v1/full/sft/checkpoint-140/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-05-03 13:27:05,170 >> Special tokens file saved in /proj/long-multi/kqian/speech_2/LLaMA-Factory/saves/qwen-14b-1M-R1-CI-round4_v1/full/sft/checkpoint-140/special_tokens_map.json
 55%|█████▌    | 141/255 [25:37<48:18, 25.42s/it] 56%|█████▌    | 142/255 [25:44<37:13, 19.77s/it] 56%|█████▌    | 143/255 [25:51<29:32, 15.83s/it] 56%|█████▋    | 144/255 [25:57<24:12, 13.09s/it] 57%|█████▋    | 145/255 [26:04<20:29, 11.18s/it] 57%|█████▋    | 146/255 [26:11<17:51,  9.83s/it] 58%|█████▊    | 147/255 [26:17<16:00,  8.89s/it] 58%|█████▊    | 148/255 [26:24<14:41,  8.24s/it] 58%|█████▊    | 149/255 [26:31<13:45,  7.78s/it] 59%|█████▉    | 150/255 [26:38<13:06,  7.49s/it][INFO|trainer.py:4289] 2025-05-03 13:28:30,670 >> 
***** Running Evaluation *****
[INFO|trainer.py:4291] 2025-05-03 13:28:30,671 >>   Num examples = 757
[INFO|trainer.py:4294] 2025-05-03 13:28:30,671 >>   Batch size = 1

  0%|          | 0/95 [00:00<?, ?it/s][A
  2%|▏         | 2/95 [00:00<00:11,  8.04it/s][A
  3%|▎         | 3/95 [00:00<00:16,  5.64it/s][A
  4%|▍         | 4/95 [00:00<00:18,  4.88it/s][A
  5%|▌         | 5/95 [00:00<00:19,  4.55it/s][A
  6%|▋         | 6/95 [00:01<00:20,  4.35it/s][A
  7%|▋         | 7/95 [00:01<00:20,  4.25it/s][A
  8%|▊         | 8/95 [00:01<00:20,  4.17it/s][A
  9%|▉         | 9/95 [00:01<00:20,  4.12it/s][A
 11%|█         | 10/95 [00:02<00:20,  4.09it/s][A
 12%|█▏        | 11/95 [00:02<00:20,  4.08it/s][A
 13%|█▎        | 12/95 [00:02<00:20,  4.07it/s][A
 14%|█▎        | 13/95 [00:02<00:20,  4.06it/s][A
 15%|█▍        | 14/95 [00:03<00:20,  4.05it/s][A
 16%|█▌        | 15/95 [00:03<00:19,  4.02it/s][A
 17%|█▋        | 16/95 [00:03<00:19,  4.01it/s][A
 18%|█▊        | 17/95 [00:03<00:19,  4.01it/s][A
 19%|█▉        | 18/95 [00:04<00:19,  4.01it/s][A
 20%|██        | 19/95 [00:04<00:18,  4.02it/s][A
 21%|██        | 20/95 [00:04<00:18,  4.01it/s][A
 22%|██▏       | 21/95 [00:04<00:18,  4.01it/s][A
 23%|██▎       | 22/95 [00:05<00:18,  3.99it/s][A
 24%|██▍       | 23/95 [00:05<00:17,  4.00it/s][A
 25%|██▌       | 24/95 [00:05<00:17,  4.01it/s][A
 26%|██▋       | 25/95 [00:05<00:17,  4.02it/s][A
 27%|██▋       | 26/95 [00:06<00:17,  4.00it/s][A
 28%|██▊       | 27/95 [00:06<00:17,  4.00it/s][A
 29%|██▉       | 28/95 [00:06<00:16,  4.01it/s][A
 31%|███       | 29/95 [00:06<00:16,  4.00it/s][A
 32%|███▏      | 30/95 [00:07<00:16,  3.97it/s][A
 33%|███▎      | 31/95 [00:07<00:16,  3.97it/s][A
 34%|███▎      | 32/95 [00:07<00:15,  3.96it/s][A
 35%|███▍      | 33/95 [00:07<00:15,  3.97it/s][A
 36%|███▌      | 34/95 [00:08<00:15,  3.99it/s][A
 37%|███▋      | 35/95 [00:08<00:15,  3.99it/s][A
 38%|███▊      | 36/95 [00:08<00:14,  3.97it/s][A
 39%|███▉      | 37/95 [00:08<00:14,  3.99it/s][A
 40%|████      | 38/95 [00:09<00:14,  3.99it/s][A
 41%|████      | 39/95 [00:09<00:13,  4.00it/s][A
 42%|████▏     | 40/95 [00:09<00:13,  4.01it/s][A
 43%|████▎     | 41/95 [00:09<00:13,  4.02it/s][A
 44%|████▍     | 42/95 [00:10<00:13,  4.02it/s][A
 45%|████▌     | 43/95 [00:10<00:12,  4.02it/s][A
 46%|████▋     | 44/95 [00:10<00:12,  4.03it/s][A
 47%|████▋     | 45/95 [00:10<00:12,  4.02it/s][A
 48%|████▊     | 46/95 [00:11<00:12,  4.03it/s][A
 49%|████▉     | 47/95 [00:11<00:11,  4.04it/s][A
 51%|█████     | 48/95 [00:11<00:11,  4.03it/s][A
 52%|█████▏    | 49/95 [00:11<00:11,  4.05it/s][A
 53%|█████▎    | 50/95 [00:12<00:11,  4.05it/s][A
 54%|█████▎    | 51/95 [00:12<00:10,  4.03it/s][A
 55%|█████▍    | 52/95 [00:12<00:10,  4.02it/s][A
 56%|█████▌    | 53/95 [00:12<00:10,  4.03it/s][A
 57%|█████▋    | 54/95 [00:13<00:10,  4.02it/s][A
 58%|█████▊    | 55/95 [00:13<00:09,  4.02it/s][A
 59%|█████▉    | 56/95 [00:13<00:09,  4.02it/s][A
 60%|██████    | 57/95 [00:13<00:09,  4.03it/s][A
 61%|██████    | 58/95 [00:14<00:09,  4.02it/s][A
 62%|██████▏   | 59/95 [00:14<00:08,  4.02it/s][A
 63%|██████▎   | 60/95 [00:14<00:08,  4.01it/s][A
 64%|██████▍   | 61/95 [00:14<00:08,  4.00it/s][A
 65%|██████▌   | 62/95 [00:15<00:08,  3.99it/s][A
 66%|██████▋   | 63/95 [00:15<00:08,  4.00it/s][A
 67%|██████▋   | 64/95 [00:15<00:07,  4.00it/s][A
 68%|██████▊   | 65/95 [00:15<00:07,  4.01it/s][A
 69%|██████▉   | 66/95 [00:16<00:07,  4.01it/s][A
 71%|███████   | 67/95 [00:16<00:06,  4.01it/s][A
 72%|███████▏  | 68/95 [00:16<00:06,  4.01it/s][A
 73%|███████▎  | 69/95 [00:16<00:06,  4.00it/s][A
 74%|███████▎  | 70/95 [00:17<00:06,  4.01it/s][A
 75%|███████▍  | 71/95 [00:17<00:05,  4.02it/s][A
 76%|███████▌  | 72/95 [00:17<00:05,  4.03it/s][A
 77%|███████▋  | 73/95 [00:17<00:05,  4.03it/s][A
 78%|███████▊  | 74/95 [00:18<00:05,  4.02it/s][A
 79%|███████▉  | 75/95 [00:18<00:04,  4.00it/s][A
 80%|████████  | 76/95 [00:18<00:04,  4.00it/s][A
 81%|████████  | 77/95 [00:18<00:04,  3.99it/s][A
 82%|████████▏ | 78/95 [00:19<00:04,  3.99it/s][A
 83%|████████▎ | 79/95 [00:19<00:04,  3.98it/s][A
 84%|████████▍ | 80/95 [00:19<00:03,  4.00it/s][A
 85%|████████▌ | 81/95 [00:19<00:03,  4.01it/s][A
 86%|████████▋ | 82/95 [00:20<00:03,  4.03it/s][A
 87%|████████▋ | 83/95 [00:20<00:02,  4.04it/s][A
 88%|████████▊ | 84/95 [00:20<00:02,  4.02it/s][A
 89%|████████▉ | 85/95 [00:20<00:02,  4.02it/s][A
 91%|█████████ | 86/95 [00:21<00:02,  4.02it/s][A
 92%|█████████▏| 87/95 [00:21<00:01,  4.00it/s][A
 93%|█████████▎| 88/95 [00:21<00:01,  4.00it/s][A
 94%|█████████▎| 89/95 [00:21<00:01,  4.01it/s][A
 95%|█████████▍| 90/95 [00:22<00:01,  4.00it/s][A
 96%|█████████▌| 91/95 [00:22<00:00,  4.00it/s][A
 97%|█████████▋| 92/95 [00:22<00:00,  3.99it/s][A
 98%|█████████▊| 93/95 [00:22<00:00,  4.00it/s][A
 99%|█████████▉| 94/95 [00:23<00:00,  4.01it/s][A
100%|██████████| 95/95 [00:23<00:00,  4.02it/s][A                                                 
                                               [A 59%|█████▉    | 150/255 [27:01<13:06,  7.49s/it]
100%|██████████| 95/95 [00:23<00:00,  4.02it/s][A
                                               [A 59%|█████▉    | 151/255 [27:08<24:54, 14.37s/it] 60%|█████▉    | 152/255 [27:15<20:42, 12.06s/it] 60%|██████    | 153/255 [27:21<17:48, 10.47s/it] 60%|██████    | 154/255 [27:28<15:42,  9.33s/it] 61%|██████    | 155/255 [27:35<14:15,  8.55s/it] 61%|██████    | 156/255 [27:42<13:12,  8.00s/it] 62%|██████▏   | 157/255 [27:48<12:28,  7.64s/it] 62%|██████▏   | 158/255 [27:55<11:49,  7.32s/it] 62%|██████▏   | 159/255 [28:02<11:26,  7.15s/it] 63%|██████▎   | 160/255 [28:08<11:07,  7.03s/it]                                                  63%|██████▎   | 160/255 [28:08<11:07,  7.03s/it][INFO|trainer.py:4289] 2025-05-03 13:30:01,514 >> 
***** Running Evaluation *****
[INFO|trainer.py:4291] 2025-05-03 13:30:01,514 >>   Num examples = 757
[INFO|trainer.py:4294] 2025-05-03 13:30:01,514 >>   Batch size = 1

  0%|          | 0/95 [00:00<?, ?it/s][A
  2%|▏         | 2/95 [00:00<00:11,  8.01it/s][A
  3%|▎         | 3/95 [00:00<00:16,  5.62it/s][A
  4%|▍         | 4/95 [00:00<00:18,  4.86it/s][A
  5%|▌         | 5/95 [00:01<00:19,  4.54it/s][A
  6%|▋         | 6/95 [00:01<00:20,  4.34it/s][A
  7%|▋         | 7/95 [00:01<00:20,  4.24it/s][A
  8%|▊         | 8/95 [00:01<00:20,  4.16it/s][A
  9%|▉         | 9/95 [00:01<00:20,  4.11it/s][A
 11%|█         | 10/95 [00:02<00:20,  4.09it/s][A
 12%|█▏        | 11/95 [00:02<00:20,  4.07it/s][A
 13%|█▎        | 12/95 [00:02<00:20,  4.07it/s][A
 14%|█▎        | 13/95 [00:02<00:20,  4.06it/s][A
 15%|█▍        | 14/95 [00:03<00:19,  4.05it/s][A
 16%|█▌        | 15/95 [00:03<00:19,  4.03it/s][A
 17%|█▋        | 16/95 [00:03<00:19,  4.02it/s][A
 18%|█▊        | 17/95 [00:03<00:19,  4.01it/s][A
 19%|█▉        | 18/95 [00:04<00:19,  4.01it/s][A
 20%|██        | 19/95 [00:04<00:18,  4.02it/s][A
 21%|██        | 20/95 [00:04<00:18,  4.01it/s][A
 22%|██▏       | 21/95 [00:04<00:18,  4.00it/s][A
 23%|██▎       | 22/95 [00:05<00:18,  3.99it/s][A
 24%|██▍       | 23/95 [00:05<00:17,  4.00it/s][A
 25%|██▌       | 24/95 [00:05<00:17,  4.01it/s][A
 26%|██▋       | 25/95 [00:05<00:17,  4.01it/s][A
 27%|██▋       | 26/95 [00:06<00:17,  3.98it/s][A
 28%|██▊       | 27/95 [00:06<00:17,  3.97it/s][A
 29%|██▉       | 28/95 [00:06<00:16,  3.98it/s][A
 31%|███       | 29/95 [00:06<00:16,  3.97it/s][A
 32%|███▏      | 30/95 [00:07<00:16,  3.99it/s][A
 33%|███▎      | 31/95 [00:07<00:15,  4.00it/s][A
 34%|███▎      | 32/95 [00:07<00:15,  4.01it/s][A
 35%|███▍      | 33/95 [00:07<00:15,  4.01it/s][A
 36%|███▌      | 34/95 [00:08<00:15,  4.00it/s][A
 37%|███▋      | 35/95 [00:08<00:15,  3.99it/s][A
 38%|███▊      | 36/95 [00:08<00:14,  3.97it/s][A
 39%|███▉      | 37/95 [00:09<00:14,  3.97it/s][A
 40%|████      | 38/95 [00:09<00:14,  3.98it/s][A
 41%|████      | 39/95 [00:09<00:14,  3.99it/s][A
 42%|████▏     | 40/95 [00:09<00:13,  4.00it/s][A
 43%|████▎     | 41/95 [00:09<00:13,  4.01it/s][A
 44%|████▍     | 42/95 [00:10<00:13,  4.02it/s][A
 45%|████▌     | 43/95 [00:10<00:12,  4.02it/s][A
 46%|████▋     | 44/95 [00:10<00:12,  4.01it/s][A
 47%|████▋     | 45/95 [00:10<00:12,  4.02it/s][A
 48%|████▊     | 46/95 [00:11<00:12,  4.03it/s][A
 49%|████▉     | 47/95 [00:11<00:11,  4.03it/s][A
 51%|█████     | 48/95 [00:11<00:11,  4.03it/s][A
 52%|█████▏    | 49/95 [00:11<00:11,  4.04it/s][A
 53%|█████▎    | 50/95 [00:12<00:11,  4.03it/s][A
 54%|█████▎    | 51/95 [00:12<00:10,  4.02it/s][A
 55%|█████▍    | 52/95 [00:12<00:10,  4.01it/s][A
 56%|█████▌    | 53/95 [00:12<00:10,  4.02it/s][A
 57%|█████▋    | 54/95 [00:13<00:10,  4.02it/s][A
 58%|█████▊    | 55/95 [00:13<00:09,  4.02it/s][A
 59%|█████▉    | 56/95 [00:13<00:09,  4.01it/s][A
 60%|██████    | 57/95 [00:13<00:09,  4.02it/s][A
 61%|██████    | 58/95 [00:14<00:09,  4.01it/s][A
 62%|██████▏   | 59/95 [00:14<00:08,  4.00it/s][A
 63%|██████▎   | 60/95 [00:14<00:08,  4.01it/s][A
 64%|██████▍   | 61/95 [00:14<00:08,  4.00it/s][A
 65%|██████▌   | 62/95 [00:15<00:08,  4.00it/s][A
 66%|██████▋   | 63/95 [00:15<00:08,  3.99it/s][A
 67%|██████▋   | 64/95 [00:15<00:07,  4.00it/s][A
 68%|██████▊   | 65/95 [00:15<00:07,  4.00it/s][A
 69%|██████▉   | 66/95 [00:16<00:07,  4.01it/s][A
 71%|███████   | 67/95 [00:16<00:06,  4.01it/s][A
 72%|███████▏  | 68/95 [00:16<00:06,  4.01it/s][A
 73%|███████▎  | 69/95 [00:16<00:06,  4.00it/s][A
 74%|███████▎  | 70/95 [00:17<00:06,  3.99it/s][A
 75%|███████▍  | 71/95 [00:17<00:05,  4.01it/s][A
 76%|███████▌  | 72/95 [00:17<00:05,  4.01it/s][A
 77%|███████▋  | 73/95 [00:17<00:05,  4.02it/s][A
 78%|███████▊  | 74/95 [00:18<00:05,  4.03it/s][A
 79%|███████▉  | 75/95 [00:18<00:04,  4.01it/s][A
 80%|████████  | 76/95 [00:18<00:04,  4.01it/s][A
 81%|████████  | 77/95 [00:18<00:04,  4.00it/s][A
 82%|████████▏ | 78/95 [00:19<00:04,  3.99it/s][A
 83%|████████▎ | 79/95 [00:19<00:04,  3.97it/s][A
 84%|████████▍ | 80/95 [00:19<00:03,  4.00it/s][A
 85%|████████▌ | 81/95 [00:19<00:03,  4.00it/s][A
 86%|████████▋ | 82/95 [00:20<00:03,  4.02it/s][A
 87%|████████▋ | 83/95 [00:20<00:02,  4.03it/s][A
 88%|████████▊ | 84/95 [00:20<00:02,  4.00it/s][A
 89%|████████▉ | 85/95 [00:20<00:02,  4.00it/s][A
 91%|█████████ | 86/95 [00:21<00:02,  3.98it/s][A
 92%|█████████▏| 87/95 [00:21<00:02,  3.96it/s][A
 93%|█████████▎| 88/95 [00:21<00:01,  3.94it/s][A
 94%|█████████▎| 89/95 [00:21<00:01,  3.96it/s][A
 95%|█████████▍| 90/95 [00:22<00:01,  3.98it/s][A
 96%|█████████▌| 91/95 [00:22<00:01,  4.00it/s][A
 97%|█████████▋| 92/95 [00:22<00:00,  3.99it/s][A
 98%|█████████▊| 93/95 [00:22<00:00,  4.01it/s][A
 99%|█████████▉| 94/95 [00:23<00:00,  4.02it/s][A
100%|██████████| 95/95 [00:23<00:00,  4.03it/s][A                                                 
                                               [A 63%|██████▎   | 160/255 [28:32<11:07,  7.03s/it]
100%|██████████| 95/95 [00:23<00:00,  4.03it/s][A
                                               [A[INFO|trainer.py:3966] 2025-05-03 13:30:32,662 >> Saving model checkpoint to /proj/long-multi/kqian/speech_2/LLaMA-Factory/saves/qwen-14b-1M-R1-CI-round4_v1/full/sft/checkpoint-160
[INFO|configuration_utils.py:423] 2025-05-03 13:30:32,666 >> Configuration saved in /proj/long-multi/kqian/speech_2/LLaMA-Factory/saves/qwen-14b-1M-R1-CI-round4_v1/full/sft/checkpoint-160/config.json
[INFO|configuration_utils.py:908] 2025-05-03 13:30:32,666 >> Configuration saved in /proj/long-multi/kqian/speech_2/LLaMA-Factory/saves/qwen-14b-1M-R1-CI-round4_v1/full/sft/checkpoint-160/generation_config.json
[INFO|modeling_utils.py:3594] 2025-05-03 13:30:44,412 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 6 checkpoint shards. You can find where each parameters has been saved in the index located at /proj/long-multi/kqian/speech_2/LLaMA-Factory/saves/qwen-14b-1M-R1-CI-round4_v1/full/sft/checkpoint-160/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2510] 2025-05-03 13:30:44,413 >> tokenizer config file saved in /proj/long-multi/kqian/speech_2/LLaMA-Factory/saves/qwen-14b-1M-R1-CI-round4_v1/full/sft/checkpoint-160/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-05-03 13:30:44,413 >> Special tokens file saved in /proj/long-multi/kqian/speech_2/LLaMA-Factory/saves/qwen-14b-1M-R1-CI-round4_v1/full/sft/checkpoint-160/special_tokens_map.json
 63%|██████▎   | 161/255 [29:17<39:45, 25.37s/it] 64%|██████▎   | 162/255 [29:23<30:37, 19.76s/it] 64%|██████▍   | 163/255 [29:30<24:18, 15.85s/it] 64%|██████▍   | 164/255 [29:37<19:50, 13.08s/it] 65%|██████▍   | 165/255 [29:43<16:44, 11.17s/it] 65%|██████▌   | 166/255 [29:50<14:34,  9.83s/it] 65%|██████▌   | 167/255 [29:57<13:00,  8.87s/it] 66%|██████▌   | 168/255 [30:03<11:54,  8.22s/it] 66%|██████▋   | 169/255 [30:10<11:04,  7.73s/it] 67%|██████▋   | 170/255 [30:17<10:29,  7.40s/it][INFO|trainer.py:4289] 2025-05-03 13:32:09,673 >> 
***** Running Evaluation *****
[INFO|trainer.py:4291] 2025-05-03 13:32:09,673 >>   Num examples = 757
[INFO|trainer.py:4294] 2025-05-03 13:32:09,673 >>   Batch size = 1

  0%|          | 0/95 [00:00<?, ?it/s][A
  2%|▏         | 2/95 [00:00<00:11,  8.02it/s][A
  3%|▎         | 3/95 [00:00<00:16,  5.64it/s][A
  4%|▍         | 4/95 [00:00<00:18,  4.87it/s][A
  5%|▌         | 5/95 [00:00<00:19,  4.55it/s][A
  6%|▋         | 6/95 [00:01<00:20,  4.35it/s][A
  7%|▋         | 7/95 [00:01<00:20,  4.24it/s][A
  8%|▊         | 8/95 [00:01<00:20,  4.17it/s][A
  9%|▉         | 9/95 [00:01<00:20,  4.12it/s][A
 11%|█         | 10/95 [00:02<00:20,  4.10it/s][A
 12%|█▏        | 11/95 [00:02<00:20,  4.08it/s][A
 13%|█▎        | 12/95 [00:02<00:20,  4.07it/s][A
 14%|█▎        | 13/95 [00:02<00:20,  4.06it/s][A
 15%|█▍        | 14/95 [00:03<00:19,  4.05it/s][A
 16%|█▌        | 15/95 [00:03<00:19,  4.04it/s][A
 17%|█▋        | 16/95 [00:03<00:19,  4.02it/s][A
 18%|█▊        | 17/95 [00:03<00:19,  4.02it/s][A
 19%|█▉        | 18/95 [00:04<00:19,  4.02it/s][A
 20%|██        | 19/95 [00:04<00:18,  4.03it/s][A
 21%|██        | 20/95 [00:04<00:18,  4.02it/s][A
 22%|██▏       | 21/95 [00:04<00:18,  4.01it/s][A
 23%|██▎       | 22/95 [00:05<00:18,  4.00it/s][A
 24%|██▍       | 23/95 [00:05<00:17,  4.02it/s][A
 25%|██▌       | 24/95 [00:05<00:17,  4.02it/s][A
 26%|██▋       | 25/95 [00:05<00:17,  4.02it/s][A
 27%|██▋       | 26/95 [00:06<00:17,  4.01it/s][A
 28%|██▊       | 27/95 [00:06<00:16,  4.00it/s][A
 29%|██▉       | 28/95 [00:06<00:16,  4.01it/s][A
 31%|███       | 29/95 [00:06<00:16,  4.01it/s][A
 32%|███▏      | 30/95 [00:07<00:16,  4.02it/s][A
 33%|███▎      | 31/95 [00:07<00:15,  4.02it/s][A
 34%|███▎      | 32/95 [00:07<00:15,  4.00it/s][A
 35%|███▍      | 33/95 [00:07<00:15,  4.01it/s][A
 36%|███▌      | 34/95 [00:08<00:15,  4.02it/s][A
 37%|███▋      | 35/95 [00:08<00:14,  4.01it/s][A
 38%|███▊      | 36/95 [00:08<00:14,  4.01it/s][A
 39%|███▉      | 37/95 [00:08<00:14,  4.02it/s][A
 40%|████      | 38/95 [00:09<00:14,  4.02it/s][A
 41%|████      | 39/95 [00:09<00:13,  4.02it/s][A
 42%|████▏     | 40/95 [00:09<00:13,  4.03it/s][A
 43%|████▎     | 41/95 [00:09<00:13,  4.04it/s][A
 44%|████▍     | 42/95 [00:10<00:13,  4.04it/s][A
 45%|████▌     | 43/95 [00:10<00:12,  4.04it/s][A
 46%|████▋     | 44/95 [00:10<00:12,  4.04it/s][A
 47%|████▋     | 45/95 [00:10<00:12,  4.04it/s][A
 48%|████▊     | 46/95 [00:11<00:12,  4.05it/s][A
 49%|████▉     | 47/95 [00:11<00:11,  4.06it/s][A
 51%|█████     | 48/95 [00:11<00:11,  4.05it/s][A
 52%|█████▏    | 49/95 [00:11<00:11,  4.06it/s][A
 53%|█████▎    | 50/95 [00:12<00:11,  4.05it/s][A
 54%|█████▎    | 51/95 [00:12<00:10,  4.05it/s][A
 55%|█████▍    | 52/95 [00:12<00:10,  4.03it/s][A
 56%|█████▌    | 53/95 [00:12<00:10,  4.04it/s][A
 57%|█████▋    | 54/95 [00:13<00:10,  4.00it/s][A
 58%|█████▊    | 55/95 [00:13<00:10,  4.00it/s][A
 59%|█████▉    | 56/95 [00:13<00:09,  3.99it/s][A
 60%|██████    | 57/95 [00:13<00:09,  4.01it/s][A
 61%|██████    | 58/95 [00:14<00:09,  4.00it/s][A
 62%|██████▏   | 59/95 [00:14<00:09,  4.00it/s][A
 63%|██████▎   | 60/95 [00:14<00:08,  4.01it/s][A
 64%|██████▍   | 61/95 [00:14<00:08,  4.00it/s][A
 65%|██████▌   | 62/95 [00:15<00:08,  4.00it/s][A
 66%|██████▋   | 63/95 [00:15<00:08,  4.00it/s][A
 67%|██████▋   | 64/95 [00:15<00:07,  4.01it/s][A
 68%|██████▊   | 65/95 [00:15<00:07,  4.01it/s][A
 69%|██████▉   | 66/95 [00:16<00:07,  4.02it/s][A
 71%|███████   | 67/95 [00:16<00:06,  4.03it/s][A
 72%|███████▏  | 68/95 [00:16<00:06,  4.03it/s][A
 73%|███████▎  | 69/95 [00:16<00:06,  4.02it/s][A
 74%|███████▎  | 70/95 [00:17<00:06,  4.02it/s][A
 75%|███████▍  | 71/95 [00:17<00:05,  4.03it/s][A
 76%|███████▌  | 72/95 [00:17<00:05,  4.01it/s][A
 77%|███████▋  | 73/95 [00:17<00:05,  4.02it/s][A
 78%|███████▊  | 74/95 [00:18<00:05,  4.03it/s][A
 79%|███████▉  | 75/95 [00:18<00:04,  4.02it/s][A
 80%|████████  | 76/95 [00:18<00:04,  4.02it/s][A
 81%|████████  | 77/95 [00:18<00:04,  4.01it/s][A
 82%|████████▏ | 78/95 [00:19<00:04,  4.01it/s][A
 83%|████████▎ | 79/95 [00:19<00:03,  4.01it/s][A
 84%|████████▍ | 80/95 [00:19<00:03,  4.03it/s][A
 85%|████████▌ | 81/95 [00:19<00:03,  4.03it/s][A
 86%|████████▋ | 82/95 [00:20<00:03,  4.05it/s][A
 87%|████████▋ | 83/95 [00:20<00:02,  4.06it/s][A
 88%|████████▊ | 84/95 [00:20<00:02,  4.04it/s][A
 89%|████████▉ | 85/95 [00:20<00:02,  4.04it/s][A
 91%|█████████ | 86/95 [00:21<00:02,  4.02it/s][A
 92%|█████████▏| 87/95 [00:21<00:01,  4.01it/s][A
 93%|█████████▎| 88/95 [00:21<00:01,  4.00it/s][A
 94%|█████████▎| 89/95 [00:21<00:01,  4.01it/s][A
 95%|█████████▍| 90/95 [00:22<00:01,  4.04it/s][A
 96%|█████████▌| 91/95 [00:22<00:00,  4.03it/s][A
 97%|█████████▋| 92/95 [00:22<00:00,  4.01it/s][A
 98%|█████████▊| 93/95 [00:22<00:00,  4.02it/s][A
 99%|█████████▉| 94/95 [00:23<00:00,  4.04it/s][A
100%|██████████| 95/95 [00:23<00:00,  4.05it/s][A                                                 
                                               [A 67%|██████▋   | 170/255 [30:40<10:29,  7.40s/it]
100%|██████████| 95/95 [00:23<00:00,  4.05it/s][A
                                               [A 67%|██████▋   | 171/255 [30:47<19:59, 14.28s/it] 67%|██████▋   | 172/255 [30:50<15:13, 11.01s/it] 68%|██████▊   | 173/255 [30:57<13:16,  9.71s/it] 68%|██████▊   | 174/255 [31:03<11:40,  8.65s/it] 69%|██████▊   | 175/255 [31:09<10:30,  7.88s/it] 69%|██████▉   | 176/255 [31:16<09:51,  7.49s/it] 69%|██████▉   | 177/255 [31:23<09:26,  7.26s/it] 70%|██████▉   | 178/255 [31:29<09:06,  7.09s/it] 70%|███████   | 179/255 [31:36<08:51,  7.00s/it] 71%|███████   | 180/255 [31:43<08:38,  6.91s/it]                                                  71%|███████   | 180/255 [31:43<08:38,  6.91s/it][INFO|trainer.py:4289] 2025-05-03 13:33:35,819 >> 
***** Running Evaluation *****
[INFO|trainer.py:4291] 2025-05-03 13:33:35,820 >>   Num examples = 757
[INFO|trainer.py:4294] 2025-05-03 13:33:35,820 >>   Batch size = 1

  0%|          | 0/95 [00:00<?, ?it/s][A
  2%|▏         | 2/95 [00:00<00:11,  8.03it/s][A
  3%|▎         | 3/95 [00:00<00:16,  5.65it/s][A
  4%|▍         | 4/95 [00:00<00:18,  4.88it/s][A
  5%|▌         | 5/95 [00:01<00:19,  4.54it/s][A
  6%|▋         | 6/95 [00:01<00:20,  4.33it/s][A
  7%|▋         | 7/95 [00:01<00:20,  4.23it/s][A
  8%|▊         | 8/95 [00:01<00:20,  4.15it/s][A
  9%|▉         | 9/95 [00:02<00:21,  4.08it/s][A
 11%|█         | 10/95 [00:02<00:20,  4.05it/s][A
 12%|█▏        | 11/95 [00:02<00:20,  4.05it/s][A
 13%|█▎        | 12/95 [00:02<00:20,  4.04it/s][A
 14%|█▎        | 13/95 [00:03<00:20,  4.04it/s][A
 15%|█▍        | 14/95 [00:03<00:20,  4.03it/s][A
 16%|█▌        | 15/95 [00:03<00:19,  4.02it/s][A
 17%|█▋        | 16/95 [00:03<00:19,  4.01it/s][A
 18%|█▊        | 17/95 [00:03<00:19,  4.01it/s][A
 19%|█▉        | 18/95 [00:04<00:19,  4.01it/s][A
 20%|██        | 19/95 [00:04<00:18,  4.02it/s][A
 21%|██        | 20/95 [00:04<00:18,  4.01it/s][A
 22%|██▏       | 21/95 [00:04<00:18,  4.01it/s][A
 23%|██▎       | 22/95 [00:05<00:18,  4.00it/s][A
 24%|██▍       | 23/95 [00:05<00:17,  4.01it/s][A
 25%|██▌       | 24/95 [00:05<00:17,  4.02it/s][A
 26%|██▋       | 25/95 [00:05<00:17,  4.02it/s][A
 27%|██▋       | 26/95 [00:06<00:17,  4.00it/s][A
 28%|██▊       | 27/95 [00:06<00:17,  3.99it/s][A
 29%|██▉       | 28/95 [00:06<00:16,  4.01it/s][A
 31%|███       | 29/95 [00:06<00:16,  4.00it/s][A
 32%|███▏      | 30/95 [00:07<00:16,  4.02it/s][A
 33%|███▎      | 31/95 [00:07<00:15,  4.02it/s][A
 34%|███▎      | 32/95 [00:07<00:15,  4.02it/s][A
 35%|███▍      | 33/95 [00:07<00:15,  4.01it/s][A
 36%|███▌      | 34/95 [00:08<00:15,  4.01it/s][A
 37%|███▋      | 35/95 [00:08<00:14,  4.01it/s][A
 38%|███▊      | 36/95 [00:08<00:14,  4.01it/s][A
 39%|███▉      | 37/95 [00:08<00:14,  4.01it/s][A
 40%|████      | 38/95 [00:09<00:14,  4.02it/s][A
 41%|████      | 39/95 [00:09<00:13,  4.02it/s][A
 42%|████▏     | 40/95 [00:09<00:13,  4.02it/s][A
 43%|████▎     | 41/95 [00:09<00:13,  4.03it/s][A
 44%|████▍     | 42/95 [00:10<00:13,  4.03it/s][A
 45%|████▌     | 43/95 [00:10<00:12,  4.03it/s][A
 46%|████▋     | 44/95 [00:10<00:12,  4.03it/s][A
 47%|████▋     | 45/95 [00:10<00:12,  4.03it/s][A
 48%|████▊     | 46/95 [00:11<00:12,  4.04it/s][A
 49%|████▉     | 47/95 [00:11<00:11,  4.02it/s][A
 51%|█████     | 48/95 [00:11<00:11,  4.02it/s][A
 52%|█████▏    | 49/95 [00:11<00:11,  4.03it/s][A
 53%|█████▎    | 50/95 [00:12<00:11,  4.03it/s][A
 54%|█████▎    | 51/95 [00:12<00:10,  4.03it/s][A
 55%|█████▍    | 52/95 [00:12<00:10,  4.02it/s][A
 56%|█████▌    | 53/95 [00:12<00:10,  4.03it/s][A
 57%|█████▋    | 54/95 [00:13<00:10,  4.02it/s][A
 58%|█████▊    | 55/95 [00:13<00:09,  4.03it/s][A
 59%|█████▉    | 56/95 [00:13<00:09,  4.03it/s][A
 60%|██████    | 57/95 [00:13<00:09,  4.04it/s][A
 61%|██████    | 58/95 [00:14<00:09,  4.02it/s][A
 62%|██████▏   | 59/95 [00:14<00:08,  4.01it/s][A
 63%|██████▎   | 60/95 [00:14<00:08,  4.01it/s][A
 64%|██████▍   | 61/95 [00:14<00:08,  4.00it/s][A
 65%|██████▌   | 62/95 [00:15<00:08,  3.99it/s][A
 66%|██████▋   | 63/95 [00:15<00:08,  4.00it/s][A
 67%|██████▋   | 64/95 [00:15<00:07,  4.01it/s][A
 68%|██████▊   | 65/95 [00:15<00:07,  4.00it/s][A
 69%|██████▉   | 66/95 [00:16<00:07,  4.02it/s][A
 71%|███████   | 67/95 [00:16<00:06,  4.02it/s][A
 72%|███████▏  | 68/95 [00:16<00:06,  4.02it/s][A
 73%|███████▎  | 69/95 [00:16<00:06,  4.00it/s][A
 74%|███████▎  | 70/95 [00:17<00:06,  3.99it/s][A
 75%|███████▍  | 71/95 [00:17<00:06,  3.99it/s][A
 76%|███████▌  | 72/95 [00:17<00:05,  4.00it/s][A
 77%|███████▋  | 73/95 [00:17<00:05,  4.01it/s][A
 78%|███████▊  | 74/95 [00:18<00:05,  4.03it/s][A
 79%|███████▉  | 75/95 [00:18<00:04,  4.01it/s][A
 80%|████████  | 76/95 [00:18<00:04,  4.01it/s][A
 81%|████████  | 77/95 [00:18<00:04,  4.00it/s][A
 82%|████████▏ | 78/95 [00:19<00:04,  3.98it/s][A
 83%|████████▎ | 79/95 [00:19<00:04,  3.99it/s][A
 84%|████████▍ | 80/95 [00:19<00:03,  4.01it/s][A
 85%|████████▌ | 81/95 [00:19<00:03,  4.02it/s][A
 86%|████████▋ | 82/95 [00:20<00:03,  4.04it/s][A
 87%|████████▋ | 83/95 [00:20<00:02,  4.05it/s][A
 88%|████████▊ | 84/95 [00:20<00:02,  4.04it/s][A
 89%|████████▉ | 85/95 [00:20<00:02,  4.04it/s][A
 91%|█████████ | 86/95 [00:21<00:02,  4.03it/s][A
 92%|█████████▏| 87/95 [00:21<00:01,  4.01it/s][A
 93%|█████████▎| 88/95 [00:21<00:01,  3.99it/s][A
 94%|█████████▎| 89/95 [00:21<00:01,  4.00it/s][A
 95%|█████████▍| 90/95 [00:22<00:01,  4.02it/s][A
 96%|█████████▌| 91/95 [00:22<00:00,  4.02it/s][A
 97%|█████████▋| 92/95 [00:22<00:00,  4.01it/s][A
 98%|█████████▊| 93/95 [00:22<00:00,  4.03it/s][A
 99%|█████████▉| 94/95 [00:23<00:00,  4.04it/s][A
100%|██████████| 95/95 [00:23<00:00,  4.05it/s][A                                                 
                                               [A 71%|███████   | 180/255 [32:06<08:38,  6.91s/it]
100%|██████████| 95/95 [00:23<00:00,  4.05it/s][A
                                               [A[INFO|trainer.py:3966] 2025-05-03 13:34:06,926 >> Saving model checkpoint to /proj/long-multi/kqian/speech_2/LLaMA-Factory/saves/qwen-14b-1M-R1-CI-round4_v1/full/sft/checkpoint-180
[INFO|configuration_utils.py:423] 2025-05-03 13:34:06,929 >> Configuration saved in /proj/long-multi/kqian/speech_2/LLaMA-Factory/saves/qwen-14b-1M-R1-CI-round4_v1/full/sft/checkpoint-180/config.json
[INFO|configuration_utils.py:908] 2025-05-03 13:34:06,930 >> Configuration saved in /proj/long-multi/kqian/speech_2/LLaMA-Factory/saves/qwen-14b-1M-R1-CI-round4_v1/full/sft/checkpoint-180/generation_config.json
[INFO|modeling_utils.py:3594] 2025-05-03 13:34:18,701 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 6 checkpoint shards. You can find where each parameters has been saved in the index located at /proj/long-multi/kqian/speech_2/LLaMA-Factory/saves/qwen-14b-1M-R1-CI-round4_v1/full/sft/checkpoint-180/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2510] 2025-05-03 13:34:18,702 >> tokenizer config file saved in /proj/long-multi/kqian/speech_2/LLaMA-Factory/saves/qwen-14b-1M-R1-CI-round4_v1/full/sft/checkpoint-180/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-05-03 13:34:18,703 >> Special tokens file saved in /proj/long-multi/kqian/speech_2/LLaMA-Factory/saves/qwen-14b-1M-R1-CI-round4_v1/full/sft/checkpoint-180/special_tokens_map.json
 71%|███████   | 181/255 [32:51<31:06, 25.22s/it] 71%|███████▏  | 182/255 [32:57<23:56, 19.68s/it] 72%|███████▏  | 183/255 [33:04<18:56, 15.78s/it] 72%|███████▏  | 184/255 [33:11<15:27, 13.06s/it] 73%|███████▎  | 185/255 [33:18<13:00, 11.15s/it] 73%|███████▎  | 186/255 [33:24<11:17,  9.82s/it] 73%|███████▎  | 187/255 [33:31<10:03,  8.87s/it] 74%|███████▎  | 188/255 [33:38<09:09,  8.20s/it] 74%|███████▍  | 189/255 [33:44<08:29,  7.72s/it] 75%|███████▍  | 190/255 [33:51<08:01,  7.41s/it][INFO|trainer.py:4289] 2025-05-03 13:35:43,876 >> 
***** Running Evaluation *****
[INFO|trainer.py:4291] 2025-05-03 13:35:43,876 >>   Num examples = 757
[INFO|trainer.py:4294] 2025-05-03 13:35:43,877 >>   Batch size = 1

  0%|          | 0/95 [00:00<?, ?it/s][A
  2%|▏         | 2/95 [00:00<00:11,  8.02it/s][A
  3%|▎         | 3/95 [00:00<00:16,  5.65it/s][A
  4%|▍         | 4/95 [00:00<00:18,  4.88it/s][A
  5%|▌         | 5/95 [00:00<00:19,  4.56it/s][A
  6%|▋         | 6/95 [00:01<00:20,  4.36it/s][A
  7%|▋         | 7/95 [00:01<00:20,  4.24it/s][A
  8%|▊         | 8/95 [00:01<00:20,  4.16it/s][A
  9%|▉         | 9/95 [00:01<00:20,  4.12it/s][A
 11%|█         | 10/95 [00:02<00:20,  4.10it/s][A
 12%|█▏        | 11/95 [00:02<00:20,  4.08it/s][A
 13%|█▎        | 12/95 [00:02<00:20,  4.07it/s][A
 14%|█▎        | 13/95 [00:02<00:20,  4.07it/s][A
 15%|█▍        | 14/95 [00:03<00:19,  4.06it/s][A
 16%|█▌        | 15/95 [00:03<00:19,  4.04it/s][A
 17%|█▋        | 16/95 [00:03<00:19,  4.02it/s][A
 18%|█▊        | 17/95 [00:03<00:19,  4.02it/s][A
 19%|█▉        | 18/95 [00:04<00:19,  4.02it/s][A
 20%|██        | 19/95 [00:04<00:18,  4.03it/s][A
 21%|██        | 20/95 [00:04<00:18,  4.02it/s][A
 22%|██▏       | 21/95 [00:04<00:18,  4.02it/s][A
 23%|██▎       | 22/95 [00:05<00:18,  4.01it/s][A
 24%|██▍       | 23/95 [00:05<00:17,  4.02it/s][A
 25%|██▌       | 24/95 [00:05<00:17,  4.02it/s][A
 26%|██▋       | 25/95 [00:05<00:17,  4.02it/s][A
 27%|██▋       | 26/95 [00:06<00:17,  4.01it/s][A
 28%|██▊       | 27/95 [00:06<00:16,  4.01it/s][A
 29%|██▉       | 28/95 [00:06<00:16,  4.02it/s][A
 31%|███       | 29/95 [00:06<00:16,  4.01it/s][A
 32%|███▏      | 30/95 [00:07<00:16,  4.02it/s][A
 33%|███▎      | 31/95 [00:07<00:15,  4.03it/s][A
 34%|███▎      | 32/95 [00:07<00:15,  4.03it/s][A
 35%|███▍      | 33/95 [00:07<00:15,  4.03it/s][A
 36%|███▌      | 34/95 [00:08<00:15,  4.04it/s][A
 37%|███▋      | 35/95 [00:08<00:14,  4.03it/s][A
 38%|███▊      | 36/95 [00:08<00:14,  4.01it/s][A
 39%|███▉      | 37/95 [00:08<00:14,  3.97it/s][A
 40%|████      | 38/95 [00:09<00:14,  3.96it/s][A
 41%|████      | 39/95 [00:09<00:14,  3.97it/s][A
 42%|████▏     | 40/95 [00:09<00:13,  3.99it/s][A
 43%|████▎     | 41/95 [00:09<00:13,  4.01it/s][A
 44%|████▍     | 42/95 [00:10<00:13,  4.02it/s][A
 45%|████▌     | 43/95 [00:10<00:12,  4.02it/s][A
 46%|████▋     | 44/95 [00:10<00:12,  4.03it/s][A
 47%|████▋     | 45/95 [00:10<00:12,  4.03it/s][A
 48%|████▊     | 46/95 [00:11<00:12,  4.04it/s][A
 49%|████▉     | 47/95 [00:11<00:11,  4.04it/s][A
 51%|█████     | 48/95 [00:11<00:11,  4.04it/s][A
 52%|█████▏    | 49/95 [00:11<00:11,  4.05it/s][A
 53%|█████▎    | 50/95 [00:12<00:11,  4.04it/s][A
 54%|█████▎    | 51/95 [00:12<00:10,  4.04it/s][A
 55%|█████▍    | 52/95 [00:12<00:10,  4.04it/s][A
 56%|█████▌    | 53/95 [00:12<00:10,  4.05it/s][A
 57%|█████▋    | 54/95 [00:13<00:10,  4.04it/s][A
 58%|█████▊    | 55/95 [00:13<00:09,  4.04it/s][A
 59%|█████▉    | 56/95 [00:13<00:09,  4.04it/s][A
 60%|██████    | 57/95 [00:13<00:09,  4.04it/s][A
 61%|██████    | 58/95 [00:14<00:09,  4.02it/s][A
 62%|██████▏   | 59/95 [00:14<00:08,  4.02it/s][A
 63%|██████▎   | 60/95 [00:14<00:08,  4.02it/s][A
 64%|██████▍   | 61/95 [00:14<00:08,  4.01it/s][A
 65%|██████▌   | 62/95 [00:15<00:08,  4.01it/s][A
 66%|██████▋   | 63/95 [00:15<00:07,  4.01it/s][A
 67%|██████▋   | 64/95 [00:15<00:07,  4.02it/s][A
 68%|██████▊   | 65/95 [00:15<00:07,  4.02it/s][A
 69%|██████▉   | 66/95 [00:16<00:07,  4.02it/s][A
 71%|███████   | 67/95 [00:16<00:06,  4.03it/s][A
 72%|███████▏  | 68/95 [00:16<00:06,  4.03it/s][A
 73%|███████▎  | 69/95 [00:16<00:06,  4.03it/s][A
 74%|███████▎  | 70/95 [00:17<00:06,  4.03it/s][A
 75%|███████▍  | 71/95 [00:17<00:05,  4.05it/s][A
 76%|███████▌  | 72/95 [00:17<00:05,  4.06it/s][A
 77%|███████▋  | 73/95 [00:17<00:05,  4.07it/s][A
 78%|███████▊  | 74/95 [00:18<00:05,  4.08it/s][A
 79%|███████▉  | 75/95 [00:18<00:04,  4.06it/s][A
 80%|████████  | 76/95 [00:18<00:04,  4.07it/s][A
 81%|████████  | 77/95 [00:18<00:04,  4.03it/s][A
 82%|████████▏ | 78/95 [00:19<00:04,  4.01it/s][A
 83%|████████▎ | 79/95 [00:19<00:03,  4.00it/s][A
 84%|████████▍ | 80/95 [00:19<00:03,  4.02it/s][A
 85%|████████▌ | 81/95 [00:19<00:03,  4.03it/s][A
 86%|████████▋ | 82/95 [00:20<00:03,  4.05it/s][A
 87%|████████▋ | 83/95 [00:20<00:02,  4.06it/s][A
 88%|████████▊ | 84/95 [00:20<00:02,  4.04it/s][A
 89%|████████▉ | 85/95 [00:20<00:02,  4.03it/s][A
 91%|█████████ | 86/95 [00:21<00:02,  4.02it/s][A
 92%|█████████▏| 87/95 [00:21<00:01,  4.02it/s][A
 93%|█████████▎| 88/95 [00:21<00:01,  4.02it/s][A
 94%|█████████▎| 89/95 [00:21<00:01,  4.03it/s][A
 95%|█████████▍| 90/95 [00:22<00:01,  4.05it/s][A
 96%|█████████▌| 91/95 [00:22<00:00,  4.05it/s][A
 97%|█████████▋| 92/95 [00:22<00:00,  4.04it/s][A
 98%|█████████▊| 93/95 [00:22<00:00,  4.05it/s][A
 99%|█████████▉| 94/95 [00:23<00:00,  4.05it/s][A
100%|██████████| 95/95 [00:23<00:00,  4.06it/s][A                                                 
                                               [A 75%|███████▍  | 190/255 [34:14<08:01,  7.41s/it]
100%|██████████| 95/95 [00:23<00:00,  4.06it/s][A
                                               [A 75%|███████▍  | 191/255 [34:21<15:13, 14.27s/it] 75%|███████▌  | 192/255 [34:28<12:36, 12.01s/it] 76%|███████▌  | 193/255 [34:35<10:46, 10.42s/it] 76%|███████▌  | 194/255 [34:41<09:26,  9.29s/it]W0503 13:36:36.673000 237574 site-packages/torch/distributed/elastic/agent/server/api.py:719] Received Signals.SIGINT death signal, shutting down workers
W0503 13:36:36.673000 237574 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 237641 closing signal SIGINT
W0503 13:36:36.673000 237574 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 237642 closing signal SIGINT
W0503 13:36:36.674000 237574 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 237643 closing signal SIGINT
W0503 13:36:36.674000 237574 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 237644 closing signal SIGINT
W0503 13:36:36.674000 237574 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 237645 closing signal SIGINT
W0503 13:36:36.674000 237574 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 237646 closing signal SIGINT
W0503 13:36:36.674000 237574 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 237647 closing signal SIGINT
W0503 13:36:36.675000 237574 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 237648 closing signal SIGINT
Traceback (most recent call last):
  File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/bin/llamafactory-cli", line 8, in <module>
    sys.exit(main())
  File "/proj/long-multi/kqian/speech_2/LLaMA-Factory/src/llamafactory/cli.py", line 99, in main
    process = subprocess.run(
  File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/subprocess.py", line 505, in run
    stdout, stderr = process.communicate(input, timeout=timeout)
  File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/subprocess.py", line 1146, in communicate
    self.wait()
  File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/subprocess.py", line 1959, in _wait
    (pid, sts) = self._try_wait(0)
  File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/subprocess.py", line 1917, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
KeyboardInterrupt
[rank4]: Traceback (most recent call last):
[rank4]:   File "/proj/long-multi/kqian/speech_2/LLaMA-Factory/src/llamafactory/launcher.py", line 23, in <module>
[rank4]:     launch()
[rank4]:   File "/proj/long-multi/kqian/speech_2/LLaMA-Factory/src/llamafactory/launcher.py", line 19, in launch
[rank4]:     run_exp()
[rank4]:   File "/proj/long-multi/kqian/speech_2/LLaMA-Factory/src/llamafactory/train/tuner.py", line 107, in run_exp
[rank4]:     _training_function(config={"args": args, "callbacks": callbacks})
[rank4]:   File "/proj/long-multi/kqian/speech_2/LLaMA-Factory/src/llamafactory/train/tuner.py", line 69, in _training_function
[rank4]:     run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)
[rank4]:   File "/proj/long-multi/kqian/speech_2/LLaMA-Factory/src/llamafactory/train/sft/workflow.py", line 102, in run_sft
[rank4]:     train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
[rank4]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/transformers/trainer.py", line 2245, in train
[rank4]:     return inner_training_loop(
[rank4]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/transformers/trainer.py", line 2556, in _inner_training_loop
[rank4]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank4]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/transformers/trainer.py", line 3764, in training_step
[rank4]:     self.accelerator.backward(loss, **kwargs)
[rank4]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/accelerate/accelerator.py", line 2238, in backward
[rank4]:     self.deepspeed_engine_wrapped.backward(loss, **kwargs)
[rank4]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/accelerate/utils/deepspeed.py", line 186, in backward
[rank4]:     self.engine.backward(loss, **kwargs)
[rank4]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 18, in wrapped_fn
[rank4]:     ret_val = func(*args, **kwargs)
[rank4]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2020, in backward
[rank4]:     self.optimizer.backward(loss, retain_graph=retain_graph)
[rank4]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 18, in wrapped_fn
[rank4]:     ret_val = func(*args, **kwargs)
[rank4]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 2249, in backward
[rank4]:     self.loss_scaler.backward(loss.float(), retain_graph=retain_graph)
[rank4]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 63, in backward
[rank4]:     scaled_loss.backward(retain_graph=retain_graph)
[rank4]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/_tensor.py", line 626, in backward
[rank4]:     torch.autograd.backward(
[rank4]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/autograd/__init__.py", line 347, in backward
[rank4]:     _engine_run_backward(
[rank4]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
[rank4]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank4]: KeyboardInterrupt
[rank0]: Traceback (most recent call last):
[rank0]:   File "/proj/long-multi/kqian/speech_2/LLaMA-Factory/src/llamafactory/launcher.py", line 23, in <module>
[rank0]:     launch()
[rank0]:   File "/proj/long-multi/kqian/speech_2/LLaMA-Factory/src/llamafactory/launcher.py", line 19, in launch
[rank0]:     run_exp()
[rank0]:   File "/proj/long-multi/kqian/speech_2/LLaMA-Factory/src/llamafactory/train/tuner.py", line 107, in run_exp
[rank0]:     _training_function(config={"args": args, "callbacks": callbacks})
[rank0]:   File "/proj/long-multi/kqian/speech_2/LLaMA-Factory/src/llamafactory/train/tuner.py", line 69, in _training_function
[rank0]:     run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)
[rank0]:   File "/proj/long-multi/kqian/speech_2/LLaMA-Factory/src/llamafactory/train/sft/workflow.py", line 102, in run_sft
[rank0]:     train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
[rank0]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/transformers/trainer.py", line 2245, in train
[rank0]:     return inner_training_loop(
[rank0]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/transformers/trainer.py", line 2556, in _inner_training_loop
[rank0]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank0]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/transformers/trainer.py", line 3764, in training_step
[rank0]:     self.accelerator.backward(loss, **kwargs)
[rank0]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/accelerate/accelerator.py", line 2238, in backward
[rank0]:     self.deepspeed_engine_wrapped.backward(loss, **kwargs)
[rank0]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/accelerate/utils/deepspeed.py", line 186, in backward
[rank0]:     self.engine.backward(loss, **kwargs)
[rank0]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 18, in wrapped_fn
[rank0]:     ret_val = func(*args, **kwargs)
[rank0]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2020, in backward
[rank0]:     self.optimizer.backward(loss, retain_graph=retain_graph)
[rank0]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 18, in wrapped_fn
[rank0]:     ret_val = func(*args, **kwargs)
[rank0]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 2249, in backward
[rank0]:     self.loss_scaler.backward(loss.float(), retain_graph=retain_graph)
[rank0]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 63, in backward
[rank0]:     scaled_loss.backward(retain_graph=retain_graph)
[rank0]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/_tensor.py", line 626, in backward
[rank0]:     torch.autograd.backward(
[rank0]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/autograd/__init__.py", line 347, in backward
[rank0]:     _engine_run_backward(
[rank0]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
[rank0]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank0]: KeyboardInterrupt
[rank3]: Traceback (most recent call last):
[rank3]:   File "/proj/long-multi/kqian/speech_2/LLaMA-Factory/src/llamafactory/launcher.py", line 23, in <module>
[rank3]:     launch()
[rank3]:   File "/proj/long-multi/kqian/speech_2/LLaMA-Factory/src/llamafactory/launcher.py", line 19, in launch
[rank3]:     run_exp()
[rank3]:   File "/proj/long-multi/kqian/speech_2/LLaMA-Factory/src/llamafactory/train/tuner.py", line 107, in run_exp
[rank3]:     _training_function(config={"args": args, "callbacks": callbacks})
[rank3]:   File "/proj/long-multi/kqian/speech_2/LLaMA-Factory/src/llamafactory/train/tuner.py", line 69, in _training_function
[rank3]:     run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)
[rank3]:   File "/proj/long-multi/kqian/speech_2/LLaMA-Factory/src/llamafactory/train/sft/workflow.py", line 102, in run_sft
[rank3]:     train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
[rank3]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/transformers/trainer.py", line 2245, in train
[rank3]:     return inner_training_loop(
[rank3]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/transformers/trainer.py", line 2556, in _inner_training_loop
[rank3]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank3]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/transformers/trainer.py", line 3764, in training_step
[rank3]:     self.accelerator.backward(loss, **kwargs)
[rank3]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/accelerate/accelerator.py", line 2238, in backward
[rank3]:     self.deepspeed_engine_wrapped.backward(loss, **kwargs)
[rank3]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/accelerate/utils/deepspeed.py", line 186, in backward
[rank3]:     self.engine.backward(loss, **kwargs)
[rank3]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 18, in wrapped_fn
[rank3]:     ret_val = func(*args, **kwargs)
[rank3]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2020, in backward
[rank3]:     self.optimizer.backward(loss, retain_graph=retain_graph)
[rank3]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 18, in wrapped_fn
[rank3]:     ret_val = func(*args, **kwargs)
[rank3]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 2249, in backward
[rank3]:     self.loss_scaler.backward(loss.float(), retain_graph=retain_graph)
[rank3]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 63, in backward
[rank3]:     scaled_loss.backward(retain_graph=retain_graph)
[rank3]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/_tensor.py", line 626, in backward
[rank3]:     torch.autograd.backward(
[rank3]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/autograd/__init__.py", line 347, in backward
[rank3]:     _engine_run_backward(
[rank3]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
[rank3]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank3]: KeyboardInterrupt
[rank7]: Traceback (most recent call last):
[rank7]:   File "/proj/long-multi/kqian/speech_2/LLaMA-Factory/src/llamafactory/launcher.py", line 23, in <module>
[rank7]:     launch()
[rank7]:   File "/proj/long-multi/kqian/speech_2/LLaMA-Factory/src/llamafactory/launcher.py", line 19, in launch
[rank7]:     run_exp()
[rank7]:   File "/proj/long-multi/kqian/speech_2/LLaMA-Factory/src/llamafactory/train/tuner.py", line 107, in run_exp
[rank7]:     _training_function(config={"args": args, "callbacks": callbacks})
[rank7]:   File "/proj/long-multi/kqian/speech_2/LLaMA-Factory/src/llamafactory/train/tuner.py", line 69, in _training_function
[rank7]:     run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)
[rank7]:   File "/proj/long-multi/kqian/speech_2/LLaMA-Factory/src/llamafactory/train/sft/workflow.py", line 102, in run_sft
[rank7]:     train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
[rank7]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/transformers/trainer.py", line 2245, in train
[rank7]:     return inner_training_loop(
[rank7]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/transformers/trainer.py", line 2556, in _inner_training_loop
[rank7]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank7]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/transformers/trainer.py", line 3764, in training_step
[rank7]:     self.accelerator.backward(loss, **kwargs)
[rank7]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/accelerate/accelerator.py", line 2238, in backward
[rank7]:     self.deepspeed_engine_wrapped.backward(loss, **kwargs)
[rank7]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/accelerate/utils/deepspeed.py", line 186, in backward
[rank7]:     self.engine.backward(loss, **kwargs)
[rank7]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 18, in wrapped_fn
[rank7]:     ret_val = func(*args, **kwargs)
[rank7]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2020, in backward
[rank7]:     self.optimizer.backward(loss, retain_graph=retain_graph)
[rank7]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 18, in wrapped_fn
[rank7]:     ret_val = func(*args, **kwargs)
[rank7]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 2249, in backward
[rank7]:     self.loss_scaler.backward(loss.float(), retain_graph=retain_graph)
[rank7]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 63, in backward
[rank7]:     scaled_loss.backward(retain_graph=retain_graph)
[rank7]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/_tensor.py", line 626, in backward
[rank7]:     torch.autograd.backward(
[rank7]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/autograd/__init__.py", line 347, in backward
[rank7]:     _engine_run_backward(
[rank7]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
[rank7]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank7]: KeyboardInterrupt
[rank2]: Traceback (most recent call last):
[rank2]:   File "/proj/long-multi/kqian/speech_2/LLaMA-Factory/src/llamafactory/launcher.py", line 23, in <module>
[rank2]:     launch()
[rank2]:   File "/proj/long-multi/kqian/speech_2/LLaMA-Factory/src/llamafactory/launcher.py", line 19, in launch
[rank2]:     run_exp()
[rank2]:   File "/proj/long-multi/kqian/speech_2/LLaMA-Factory/src/llamafactory/train/tuner.py", line 107, in run_exp
[rank2]:     _training_function(config={"args": args, "callbacks": callbacks})
[rank2]:   File "/proj/long-multi/kqian/speech_2/LLaMA-Factory/src/llamafactory/train/tuner.py", line 69, in _training_function
[rank2]:     run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)
[rank2]:   File "/proj/long-multi/kqian/speech_2/LLaMA-Factory/src/llamafactory/train/sft/workflow.py", line 102, in run_sft
[rank2]:     train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
[rank2]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/transformers/trainer.py", line 2245, in train
[rank2]:     return inner_training_loop(
[rank2]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/transformers/trainer.py", line 2556, in _inner_training_loop
[rank2]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank2]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/transformers/trainer.py", line 3764, in training_step
[rank2]:     self.accelerator.backward(loss, **kwargs)
[rank2]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/accelerate/accelerator.py", line 2238, in backward
[rank2]:     self.deepspeed_engine_wrapped.backward(loss, **kwargs)
[rank2]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/accelerate/utils/deepspeed.py", line 186, in backward
[rank2]:     self.engine.backward(loss, **kwargs)
[rank2]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 18, in wrapped_fn
[rank2]:     ret_val = func(*args, **kwargs)
[rank2]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2020, in backward
[rank2]:     self.optimizer.backward(loss, retain_graph=retain_graph)
[rank2]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 18, in wrapped_fn
[rank2]:     ret_val = func(*args, **kwargs)
[rank2]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 2249, in backward
[rank2]:     self.loss_scaler.backward(loss.float(), retain_graph=retain_graph)
[rank2]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 63, in backward
[rank2]:     scaled_loss.backward(retain_graph=retain_graph)
[rank2]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/_tensor.py", line 626, in backward
[rank2]:     torch.autograd.backward(
[rank2]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/autograd/__init__.py", line 347, in backward
[rank2]:     _engine_run_backward(
[rank2]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
[rank2]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank2]: KeyboardInterrupt
[rank1]: Traceback (most recent call last):
[rank1]:   File "/proj/long-multi/kqian/speech_2/LLaMA-Factory/src/llamafactory/launcher.py", line 23, in <module>
[rank1]:     launch()
[rank1]:   File "/proj/long-multi/kqian/speech_2/LLaMA-Factory/src/llamafactory/launcher.py", line 19, in launch
[rank1]:     run_exp()
[rank1]:   File "/proj/long-multi/kqian/speech_2/LLaMA-Factory/src/llamafactory/train/tuner.py", line 107, in run_exp
[rank1]:     _training_function(config={"args": args, "callbacks": callbacks})
[rank1]:   File "/proj/long-multi/kqian/speech_2/LLaMA-Factory/src/llamafactory/train/tuner.py", line 69, in _training_function
[rank1]:     run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)
[rank1]:   File "/proj/long-multi/kqian/speech_2/LLaMA-Factory/src/llamafactory/train/sft/workflow.py", line 102, in run_sft
[rank1]:     train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
[rank1]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/transformers/trainer.py", line 2245, in train
[rank1]:     return inner_training_loop(
[rank1]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/transformers/trainer.py", line 2556, in _inner_training_loop
[rank1]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank1]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/transformers/trainer.py", line 3764, in training_step
[rank1]:     self.accelerator.backward(loss, **kwargs)
[rank1]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/accelerate/accelerator.py", line 2238, in backward
[rank1]:     self.deepspeed_engine_wrapped.backward(loss, **kwargs)
[rank1]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/accelerate/utils/deepspeed.py", line 186, in backward
[rank1]:     self.engine.backward(loss, **kwargs)
[rank1]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 18, in wrapped_fn
[rank1]:     ret_val = func(*args, **kwargs)
[rank1]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2020, in backward
[rank1]:     self.optimizer.backward(loss, retain_graph=retain_graph)
[rank1]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 18, in wrapped_fn
[rank1]:     ret_val = func(*args, **kwargs)
[rank1]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 2249, in backward
[rank1]:     self.loss_scaler.backward(loss.float(), retain_graph=retain_graph)
[rank1]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 63, in backward
[rank1]:     scaled_loss.backward(retain_graph=retain_graph)
[rank1]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/_tensor.py", line 626, in backward
[rank1]:     torch.autograd.backward(
[rank1]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/autograd/__init__.py", line 347, in backward
[rank1]:     _engine_run_backward(
[rank1]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
[rank1]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank1]: KeyboardInterrupt
[rank6]: Traceback (most recent call last):
[rank6]:   File "/proj/long-multi/kqian/speech_2/LLaMA-Factory/src/llamafactory/launcher.py", line 23, in <module>
[rank6]:     launch()
[rank6]:   File "/proj/long-multi/kqian/speech_2/LLaMA-Factory/src/llamafactory/launcher.py", line 19, in launch
[rank6]:     run_exp()
[rank6]:   File "/proj/long-multi/kqian/speech_2/LLaMA-Factory/src/llamafactory/train/tuner.py", line 107, in run_exp
[rank6]:     _training_function(config={"args": args, "callbacks": callbacks})
[rank6]:   File "/proj/long-multi/kqian/speech_2/LLaMA-Factory/src/llamafactory/train/tuner.py", line 69, in _training_function
[rank6]:     run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)
[rank6]:   File "/proj/long-multi/kqian/speech_2/LLaMA-Factory/src/llamafactory/train/sft/workflow.py", line 102, in run_sft
[rank6]:     train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
[rank6]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/transformers/trainer.py", line 2245, in train
[rank6]:     return inner_training_loop(
[rank6]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/transformers/trainer.py", line 2556, in _inner_training_loop
[rank6]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank6]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/transformers/trainer.py", line 3764, in training_step
[rank6]:     self.accelerator.backward(loss, **kwargs)
[rank6]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/accelerate/accelerator.py", line 2238, in backward
[rank6]:     self.deepspeed_engine_wrapped.backward(loss, **kwargs)
[rank6]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/accelerate/utils/deepspeed.py", line 186, in backward
[rank6]:     self.engine.backward(loss, **kwargs)
[rank6]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 18, in wrapped_fn
[rank6]:     ret_val = func(*args, **kwargs)
[rank6]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2020, in backward
[rank6]:     self.optimizer.backward(loss, retain_graph=retain_graph)
[rank6]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 18, in wrapped_fn
[rank6]:     ret_val = func(*args, **kwargs)
[rank6]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 2249, in backward
[rank6]:     self.loss_scaler.backward(loss.float(), retain_graph=retain_graph)
[rank6]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 63, in backward
[rank6]:     scaled_loss.backward(retain_graph=retain_graph)
[rank6]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/_tensor.py", line 626, in backward
[rank6]:     torch.autograd.backward(
[rank6]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/autograd/__init__.py", line 347, in backward
[rank6]:     _engine_run_backward(
[rank6]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
[rank6]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank6]: KeyboardInterrupt
[rank5]: Traceback (most recent call last):
[rank5]:   File "/proj/long-multi/kqian/speech_2/LLaMA-Factory/src/llamafactory/launcher.py", line 23, in <module>
[rank5]:     launch()
[rank5]:   File "/proj/long-multi/kqian/speech_2/LLaMA-Factory/src/llamafactory/launcher.py", line 19, in launch
[rank5]:     run_exp()
[rank5]:   File "/proj/long-multi/kqian/speech_2/LLaMA-Factory/src/llamafactory/train/tuner.py", line 107, in run_exp
[rank5]:     _training_function(config={"args": args, "callbacks": callbacks})
[rank5]:   File "/proj/long-multi/kqian/speech_2/LLaMA-Factory/src/llamafactory/train/tuner.py", line 69, in _training_function
[rank5]:     run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)
[rank5]:   File "/proj/long-multi/kqian/speech_2/LLaMA-Factory/src/llamafactory/train/sft/workflow.py", line 102, in run_sft
[rank5]:     train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
[rank5]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/transformers/trainer.py", line 2245, in train
[rank5]:     return inner_training_loop(
[rank5]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/transformers/trainer.py", line 2556, in _inner_training_loop
[rank5]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank5]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/transformers/trainer.py", line 3764, in training_step
[rank5]:     self.accelerator.backward(loss, **kwargs)
[rank5]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/accelerate/accelerator.py", line 2238, in backward
[rank5]:     self.deepspeed_engine_wrapped.backward(loss, **kwargs)
[rank5]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/accelerate/utils/deepspeed.py", line 186, in backward
[rank5]:     self.engine.backward(loss, **kwargs)
[rank5]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 18, in wrapped_fn
[rank5]:     ret_val = func(*args, **kwargs)
[rank5]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2020, in backward
[rank5]:     self.optimizer.backward(loss, retain_graph=retain_graph)
[rank5]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 18, in wrapped_fn
[rank5]:     ret_val = func(*args, **kwargs)
[rank5]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/deepspeed/runtime/zero/stage3.py", line 2249, in backward
[rank5]:     self.loss_scaler.backward(loss.float(), retain_graph=retain_graph)
[rank5]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 63, in backward
[rank5]:     scaled_loss.backward(retain_graph=retain_graph)
[rank5]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/_tensor.py", line 626, in backward
[rank5]:     torch.autograd.backward(
[rank5]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/autograd/__init__.py", line 347, in backward
[rank5]:     _engine_run_backward(
[rank5]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
[rank5]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank5]: KeyboardInterrupt
 76%|███████▌  | 194/255 [34:45<10:55, 10.75s/it]
