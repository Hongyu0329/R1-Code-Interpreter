W0403 19:41:41.522000 270079 site-packages/torch/distributed/run.py:792] 
W0403 19:41:41.522000 270079 site-packages/torch/distributed/run.py:792] *****************************************
W0403 19:41:41.522000 270079 site-packages/torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0403 19:41:41.522000 270079 site-packages/torch/distributed/run.py:792] *****************************************
[INFO|tokenization_utils_base.py:2060] 2025-04-03 19:41:47,946 >> loading file tokenizer.json from cache at /proj/long-multi/kqian/hf_cache/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/5f0b02c75b57c5855da9ae460ce51323ea669d8a/tokenizer.json
[INFO|tokenization_utils_base.py:2060] 2025-04-03 19:41:47,947 >> loading file tokenizer.model from cache at None
[INFO|tokenization_utils_base.py:2060] 2025-04-03 19:41:47,947 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2060] 2025-04-03 19:41:47,947 >> loading file special_tokens_map.json from cache at /proj/long-multi/kqian/hf_cache/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/5f0b02c75b57c5855da9ae460ce51323ea669d8a/special_tokens_map.json
[INFO|tokenization_utils_base.py:2060] 2025-04-03 19:41:47,947 >> loading file tokenizer_config.json from cache at /proj/long-multi/kqian/hf_cache/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/5f0b02c75b57c5855da9ae460ce51323ea669d8a/tokenizer_config.json
[INFO|tokenization_utils_base.py:2060] 2025-04-03 19:41:47,947 >> loading file chat_template.jinja from cache at None
[INFO|tokenization_utils_base.py:2323] 2025-04-03 19:41:48,255 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:699] 2025-04-03 19:41:48,632 >> loading configuration file config.json from cache at /proj/long-multi/kqian/hf_cache/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/5f0b02c75b57c5855da9ae460ce51323ea669d8a/config.json
[INFO|configuration_utils.py:771] 2025-04-03 19:41:48,633 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128009,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 8192,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.50.0",
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2060] 2025-04-03 19:41:48,713 >> loading file tokenizer.json from cache at /proj/long-multi/kqian/hf_cache/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/5f0b02c75b57c5855da9ae460ce51323ea669d8a/tokenizer.json
[INFO|tokenization_utils_base.py:2060] 2025-04-03 19:41:48,713 >> loading file tokenizer.model from cache at None
[INFO|tokenization_utils_base.py:2060] 2025-04-03 19:41:48,713 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2060] 2025-04-03 19:41:48,713 >> loading file special_tokens_map.json from cache at /proj/long-multi/kqian/hf_cache/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/5f0b02c75b57c5855da9ae460ce51323ea669d8a/special_tokens_map.json
[INFO|tokenization_utils_base.py:2060] 2025-04-03 19:41:48,713 >> loading file tokenizer_config.json from cache at /proj/long-multi/kqian/hf_cache/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/5f0b02c75b57c5855da9ae460ce51323ea669d8a/tokenizer_config.json
[INFO|tokenization_utils_base.py:2060] 2025-04-03 19:41:48,715 >> loading file chat_template.jinja from cache at None
[INFO|tokenization_utils_base.py:2323] 2025-04-03 19:41:49,022 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[rank1]:[W403 19:41:49.908918424 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank3]:[W403 19:41:49.011144624 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank2]:[W403 19:41:49.043074497 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Converting format of dataset (num_proc=16):   0%|          | 0/91 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16): 100%|██████████| 91/91 [00:00<00:00, 636.60 examples/s]
Converting format of dataset (num_proc=16):   0%|          | 0/1000 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16): 100%|██████████| 1000/1000 [00:00<00:00, 7308.28 examples/s]
[rank0]:[W403 19:41:50.030489960 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Running tokenizer on dataset (num_proc=16):   0%|          | 0/1091 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   6%|▋         | 69/1091 [00:00<00:09, 108.30 examples/s]Running tokenizer on dataset (num_proc=16):  13%|█▎        | 138/1091 [00:00<00:04, 211.84 examples/s]Running tokenizer on dataset (num_proc=16):  19%|█▉        | 207/1091 [00:00<00:02, 299.61 examples/s]Running tokenizer on dataset (num_proc=16):  31%|███▏      | 343/1091 [00:01<00:01, 436.75 examples/s]Running tokenizer on dataset (num_proc=16):  38%|███▊      | 411/1091 [00:01<00:01, 480.28 examples/s]Running tokenizer on dataset (num_proc=16):  44%|████▍     | 479/1091 [00:01<00:01, 514.26 examples/s]Running tokenizer on dataset (num_proc=16):  50%|█████     | 547/1091 [00:01<00:01, 543.19 examples/s]Running tokenizer on dataset (num_proc=16):  56%|█████▋    | 615/1091 [00:01<00:00, 569.78 examples/s]Running tokenizer on dataset (num_proc=16):  63%|██████▎   | 683/1091 [00:01<00:00, 591.06 examples/s]Running tokenizer on dataset (num_proc=16):  69%|██████▉   | 751/1091 [00:01<00:00, 604.21 examples/s]Running tokenizer on dataset (num_proc=16):  81%|████████▏ | 887/1091 [00:01<00:00, 643.92 examples/s]Running tokenizer on dataset (num_proc=16):  94%|█████████▍| 1023/1091 [00:02<00:00, 677.04 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 1091/1091 [00:02<00:00, 485.33 examples/s]
[INFO|configuration_utils.py:699] 2025-04-03 19:41:54,848 >> loading configuration file config.json from cache at /proj/long-multi/kqian/hf_cache/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/5f0b02c75b57c5855da9ae460ce51323ea669d8a/config.json
[INFO|configuration_utils.py:771] 2025-04-03 19:41:54,849 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128009,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 8192,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.50.0",
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|modeling_utils.py:1154] 2025-04-03 19:41:54,985 >> loading weights file model.safetensors from cache at /proj/long-multi/kqian/hf_cache/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/5f0b02c75b57c5855da9ae460ce51323ea669d8a/model.safetensors.index.json
[INFO|modeling_utils.py:3747] 2025-04-03 19:41:54,988 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|configuration_utils.py:1139] 2025-04-03 19:41:54,996 >> Generate config GenerationConfig {
  "bos_token_id": 128000,
  "eos_token_id": 128009,
  "use_cache": false
}

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:00,  4.39it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:00,  4.37it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:00,  4.37it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.00it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.00it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.00it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:05<00:15,  5.30s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:03,  3.02s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:03,  3.02s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:03,  3.02s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.90s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.90s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.90s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.84s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.84s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.84s/it]

Loading checkpoint shards:  50%|█████     | 2/4 [00:11<00:11,  5.65s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:15<00:05,  5.16s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  3.55s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  4.21s/it]
[INFO|modeling_utils.py:4987] 2025-04-03 19:42:12,055 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:4995] 2025-04-03 19:42:12,055 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Meta-Llama-3-8B-Instruct.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|configuration_utils.py:1094] 2025-04-03 19:42:12,172 >> loading configuration file generation_config.json from cache at /proj/long-multi/kqian/hf_cache/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/5f0b02c75b57c5855da9ae460ce51323ea669d8a/generation_config.json
[INFO|configuration_utils.py:1139] 2025-04-03 19:42:12,172 >> Generate config GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": [
    128001,
    128009
  ],
  "max_length": 4096,
  "temperature": 0.6,
  "top_p": 0.9
}

[INFO|trainer.py:748] 2025-04-03 19:42:12,195 >> Using auto half precision backend
[INFO|trainer.py:2409] 2025-04-03 19:42:15,372 >> ***** Running training *****
[INFO|trainer.py:2410] 2025-04-03 19:42:15,372 >>   Num examples = 1,091
[INFO|trainer.py:2411] 2025-04-03 19:42:15,372 >>   Num Epochs = 3
[INFO|trainer.py:2412] 2025-04-03 19:42:15,372 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2415] 2025-04-03 19:42:15,372 >>   Total train batch size (w. parallel, distributed & accumulation) = 8
[INFO|trainer.py:2416] 2025-04-03 19:42:15,372 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:2417] 2025-04-03 19:42:15,372 >>   Total optimization steps = 408
[INFO|trainer.py:2418] 2025-04-03 19:42:15,373 >>   Number of trainable parameters = 8,030,261,248
  0%|          | 0/408 [00:00<?, ?it/s]  0%|          | 1/408 [00:03<27:05,  3.99s/it]  0%|          | 2/408 [00:05<16:00,  2.36s/it]  1%|          | 3/408 [00:06<12:16,  1.82s/it]  1%|          | 4/408 [00:07<10:32,  1.57s/it]  1%|          | 5/408 [00:08<09:33,  1.42s/it]  1%|▏         | 6/408 [00:09<08:58,  1.34s/it]  2%|▏         | 7/408 [00:11<08:36,  1.29s/it]  2%|▏         | 8/408 [00:12<08:20,  1.25s/it]  2%|▏         | 9/408 [00:13<08:08,  1.23s/it]  2%|▏         | 10/408 [00:14<08:00,  1.21s/it]                                                  2%|▏         | 10/408 [00:14<08:00,  1.21s/it]  3%|▎         | 11/408 [00:15<07:54,  1.20s/it]  3%|▎         | 12/408 [00:16<07:51,  1.19s/it]  3%|▎         | 13/408 [00:18<07:48,  1.19s/it]  3%|▎         | 14/408 [00:19<07:45,  1.18s/it]  4%|▎         | 15/408 [00:20<07:43,  1.18s/it]  4%|▍         | 16/408 [00:21<07:41,  1.18s/it]  4%|▍         | 17/408 [00:22<07:39,  1.17s/it]  4%|▍         | 18/408 [00:23<07:37,  1.17s/it]  5%|▍         | 19/408 [00:25<07:36,  1.17s/it]  5%|▍         | 20/408 [00:26<07:34,  1.17s/it]                                                  5%|▍         | 20/408 [00:26<07:34,  1.17s/it]  5%|▌         | 21/408 [00:27<07:32,  1.17s/it]  5%|▌         | 22/408 [00:28<07:31,  1.17s/it]  6%|▌         | 23/408 [00:29<07:30,  1.17s/it]  6%|▌         | 24/408 [00:31<07:29,  1.17s/it]  6%|▌         | 25/408 [00:32<07:29,  1.17s/it]  6%|▋         | 26/408 [00:33<07:26,  1.17s/it]  7%|▋         | 27/408 [00:34<07:25,  1.17s/it]  7%|▋         | 28/408 [00:35<07:24,  1.17s/it]  7%|▋         | 29/408 [00:36<07:22,  1.17s/it]  7%|▋         | 30/408 [00:38<07:20,  1.17s/it]                                                  7%|▋         | 30/408 [00:38<07:20,  1.17s/it]  8%|▊         | 31/408 [00:39<07:20,  1.17s/it]  8%|▊         | 32/408 [00:40<07:19,  1.17s/it]  8%|▊         | 33/408 [00:41<07:17,  1.17s/it]  8%|▊         | 34/408 [00:42<07:16,  1.17s/it]  9%|▊         | 35/408 [00:43<07:15,  1.17s/it]  9%|▉         | 36/408 [00:45<07:14,  1.17s/it]  9%|▉         | 37/408 [00:46<07:12,  1.17s/it]  9%|▉         | 38/408 [00:47<07:13,  1.17s/it] 10%|▉         | 39/408 [00:48<07:11,  1.17s/it] 10%|▉         | 40/408 [00:49<07:10,  1.17s/it]                                                 10%|▉         | 40/408 [00:49<07:10,  1.17s/it] 10%|█         | 41/408 [00:50<07:09,  1.17s/it] 10%|█         | 42/408 [00:52<07:07,  1.17s/it] 11%|█         | 43/408 [00:53<07:06,  1.17s/it] 11%|█         | 44/408 [00:54<07:04,  1.17s/it] 11%|█         | 45/408 [00:55<07:04,  1.17s/it] 11%|█▏        | 46/408 [00:56<07:03,  1.17s/it] 12%|█▏        | 47/408 [00:57<07:01,  1.17s/it] 12%|█▏        | 48/408 [00:59<07:00,  1.17s/it] 12%|█▏        | 49/408 [01:00<06:59,  1.17s/it] 12%|█▏        | 50/408 [01:01<06:58,  1.17s/it]                                                 12%|█▏        | 50/408 [01:01<06:58,  1.17s/it] 12%|█▎        | 51/408 [01:02<06:57,  1.17s/it] 13%|█▎        | 52/408 [01:03<06:56,  1.17s/it] 13%|█▎        | 53/408 [01:04<06:54,  1.17s/it] 13%|█▎        | 54/408 [01:06<06:53,  1.17s/it] 13%|█▎        | 55/408 [01:07<06:51,  1.17s/it] 14%|█▎        | 56/408 [01:08<06:51,  1.17s/it] 14%|█▍        | 57/408 [01:09<06:50,  1.17s/it] 14%|█▍        | 58/408 [01:10<06:48,  1.17s/it] 14%|█▍        | 59/408 [01:11<06:48,  1.17s/it] 15%|█▍        | 60/408 [01:13<06:47,  1.17s/it]                                                 15%|█▍        | 60/408 [01:13<06:47,  1.17s/it] 15%|█▍        | 61/408 [01:14<06:46,  1.17s/it] 15%|█▌        | 62/408 [01:15<06:44,  1.17s/it] 15%|█▌        | 63/408 [01:16<06:43,  1.17s/it] 16%|█▌        | 64/408 [01:17<06:42,  1.17s/it] 16%|█▌        | 65/408 [01:18<06:41,  1.17s/it] 16%|█▌        | 66/408 [01:20<06:41,  1.17s/it] 16%|█▋        | 67/408 [01:21<06:40,  1.17s/it] 17%|█▋        | 68/408 [01:22<06:38,  1.17s/it] 17%|█▋        | 69/408 [01:23<06:37,  1.17s/it] 17%|█▋        | 70/408 [01:24<06:36,  1.17s/it]                                                 17%|█▋        | 70/408 [01:24<06:36,  1.17s/it] 17%|█▋        | 71/408 [01:25<06:35,  1.17s/it] 18%|█▊        | 72/408 [01:27<06:32,  1.17s/it] 18%|█▊        | 73/408 [01:28<06:30,  1.17s/it] 18%|█▊        | 74/408 [01:29<06:29,  1.17s/it] 18%|█▊        | 75/408 [01:30<06:29,  1.17s/it] 19%|█▊        | 76/408 [01:31<06:28,  1.17s/it] 19%|█▉        | 77/408 [01:32<06:26,  1.17s/it] 19%|█▉        | 78/408 [01:34<06:25,  1.17s/it] 19%|█▉        | 79/408 [01:35<06:24,  1.17s/it] 20%|█▉        | 80/408 [01:36<06:23,  1.17s/it]                                                 20%|█▉        | 80/408 [01:36<06:23,  1.17s/it] 20%|█▉        | 81/408 [01:37<06:22,  1.17s/it] 20%|██        | 82/408 [01:38<06:21,  1.17s/it] 20%|██        | 83/408 [01:39<06:19,  1.17s/it] 21%|██        | 84/408 [01:41<06:18,  1.17s/it] 21%|██        | 85/408 [01:42<06:17,  1.17s/it] 21%|██        | 86/408 [01:43<06:16,  1.17s/it] 21%|██▏       | 87/408 [01:44<06:14,  1.17s/it] 22%|██▏       | 88/408 [01:45<06:13,  1.17s/it] 22%|██▏       | 89/408 [01:46<06:12,  1.17s/it] 22%|██▏       | 90/408 [01:48<06:11,  1.17s/it]                                                 22%|██▏       | 90/408 [01:48<06:11,  1.17s/it] 22%|██▏       | 91/408 [01:49<06:09,  1.17s/it] 23%|██▎       | 92/408 [01:50<06:08,  1.17s/it] 23%|██▎       | 93/408 [01:51<06:06,  1.16s/it] 23%|██▎       | 94/408 [01:52<06:06,  1.17s/it] 23%|██▎       | 95/408 [01:53<06:04,  1.17s/it] 24%|██▎       | 96/408 [01:55<06:03,  1.17s/it] 24%|██▍       | 97/408 [01:56<06:02,  1.17s/it] 24%|██▍       | 98/408 [01:57<06:01,  1.17s/it] 24%|██▍       | 99/408 [01:58<05:59,  1.16s/it] 25%|██▍       | 100/408 [01:59<05:58,  1.16s/it]                                                  25%|██▍       | 100/408 [01:59<05:58,  1.16s/it] 25%|██▍       | 101/408 [02:00<05:58,  1.17s/it] 25%|██▌       | 102/408 [02:02<05:57,  1.17s/it] 25%|██▌       | 103/408 [02:03<05:56,  1.17s/it] 25%|██▌       | 104/408 [02:04<05:54,  1.17s/it] 26%|██▌       | 105/408 [02:05<05:53,  1.17s/it] 26%|██▌       | 106/408 [02:06<05:52,  1.17s/it] 26%|██▌       | 107/408 [02:07<05:51,  1.17s/it] 26%|██▋       | 108/408 [02:09<05:50,  1.17s/it] 27%|██▋       | 109/408 [02:10<05:48,  1.17s/it] 27%|██▋       | 110/408 [02:11<05:48,  1.17s/it]                                                  27%|██▋       | 110/408 [02:11<05:48,  1.17s/it] 27%|██▋       | 111/408 [02:12<05:46,  1.17s/it] 27%|██▋       | 112/408 [02:13<05:45,  1.17s/it] 28%|██▊       | 113/408 [02:14<05:44,  1.17s/it] 28%|██▊       | 114/408 [02:16<05:43,  1.17s/it] 28%|██▊       | 115/408 [02:17<05:42,  1.17s/it] 28%|██▊       | 116/408 [02:18<05:41,  1.17s/it] 29%|██▊       | 117/408 [02:19<05:40,  1.17s/it] 29%|██▉       | 118/408 [02:20<05:38,  1.17s/it] 29%|██▉       | 119/408 [02:22<05:38,  1.17s/it] 29%|██▉       | 120/408 [02:23<05:36,  1.17s/it]                                                  29%|██▉       | 120/408 [02:23<05:36,  1.17s/it] 30%|██▉       | 121/408 [02:24<05:35,  1.17s/it] 30%|██▉       | 122/408 [02:25<05:34,  1.17s/it] 30%|███       | 123/408 [02:26<05:32,  1.17s/it] 30%|███       | 124/408 [02:27<05:31,  1.17s/it] 31%|███       | 125/408 [02:29<05:29,  1.17s/it] 31%|███       | 126/408 [02:30<05:28,  1.17s/it] 31%|███       | 127/408 [02:31<05:27,  1.17s/it] 31%|███▏      | 128/408 [02:32<05:27,  1.17s/it] 32%|███▏      | 129/408 [02:33<05:25,  1.17s/it] 32%|███▏      | 130/408 [02:34<05:24,  1.17s/it]                                                  32%|███▏      | 130/408 [02:34<05:24,  1.17s/it] 32%|███▏      | 131/408 [02:36<05:23,  1.17s/it] 32%|███▏      | 132/408 [02:37<05:22,  1.17s/it] 33%|███▎      | 133/408 [02:38<05:21,  1.17s/it] 33%|███▎      | 134/408 [02:39<05:20,  1.17s/it] 33%|███▎      | 135/408 [02:40<05:19,  1.17s/it] 33%|███▎      | 136/408 [02:41<05:18,  1.17s/it] 34%|███▎      | 137/408 [02:42<04:28,  1.01it/s] 34%|███▍      | 138/408 [02:43<04:55,  1.09s/it] 34%|███▍      | 139/408 [02:44<05:01,  1.12s/it] 34%|███▍      | 140/408 [02:46<05:04,  1.14s/it]                                                  34%|███▍      | 140/408 [02:46<05:04,  1.14s/it] 35%|███▍      | 141/408 [02:47<05:07,  1.15s/it] 35%|███▍      | 142/408 [02:48<05:07,  1.16s/it] 35%|███▌      | 143/408 [02:49<05:08,  1.16s/it] 35%|███▌      | 144/408 [02:50<05:07,  1.17s/it] 36%|███▌      | 145/408 [02:52<05:07,  1.17s/it] 36%|███▌      | 146/408 [02:53<05:06,  1.17s/it] 36%|███▌      | 147/408 [02:54<05:05,  1.17s/it] 36%|███▋      | 148/408 [02:55<05:04,  1.17s/it] 37%|███▋      | 149/408 [02:56<05:02,  1.17s/it] 37%|███▋      | 150/408 [02:57<05:02,  1.17s/it]                                                  37%|███▋      | 150/408 [02:57<05:02,  1.17s/it] 37%|███▋      | 151/408 [02:59<05:00,  1.17s/it] 37%|███▋      | 152/408 [03:00<04:59,  1.17s/it] 38%|███▊      | 153/408 [03:01<04:59,  1.17s/it] 38%|███▊      | 154/408 [03:02<04:58,  1.17s/it] 38%|███▊      | 155/408 [03:03<04:56,  1.17s/it] 38%|███▊      | 156/408 [03:04<04:55,  1.17s/it] 38%|███▊      | 157/408 [03:06<04:54,  1.17s/it] 39%|███▊      | 158/408 [03:07<04:53,  1.17s/it] 39%|███▉      | 159/408 [03:08<04:51,  1.17s/it] 39%|███▉      | 160/408 [03:09<04:50,  1.17s/it]                                                  39%|███▉      | 160/408 [03:09<04:50,  1.17s/it] 39%|███▉      | 161/408 [03:10<04:49,  1.17s/it] 40%|███▉      | 162/408 [03:11<04:48,  1.17s/it] 40%|███▉      | 163/408 [03:13<04:47,  1.17s/it] 40%|████      | 164/408 [03:14<04:45,  1.17s/it] 40%|████      | 165/408 [03:15<04:44,  1.17s/it] 41%|████      | 166/408 [03:16<04:45,  1.18s/it] 41%|████      | 167/408 [03:17<04:43,  1.18s/it] 41%|████      | 168/408 [03:18<04:42,  1.18s/it] 41%|████▏     | 169/408 [03:20<04:40,  1.18s/it] 42%|████▏     | 170/408 [03:21<04:40,  1.18s/it]                                                  42%|████▏     | 170/408 [03:21<04:40,  1.18s/it] 42%|████▏     | 171/408 [03:22<04:38,  1.18s/it] 42%|████▏     | 172/408 [03:23<04:37,  1.18s/it] 42%|████▏     | 173/408 [03:24<04:35,  1.17s/it] 43%|████▎     | 174/408 [03:26<04:35,  1.18s/it] 43%|████▎     | 175/408 [03:27<04:33,  1.17s/it] 43%|████▎     | 176/408 [03:28<04:32,  1.18s/it] 43%|████▎     | 177/408 [03:29<04:31,  1.17s/it] 44%|████▎     | 178/408 [03:30<04:30,  1.17s/it] 44%|████▍     | 179/408 [03:31<04:28,  1.17s/it] 44%|████▍     | 180/408 [03:33<04:27,  1.17s/it]                                                  44%|████▍     | 180/408 [03:33<04:27,  1.17s/it] 44%|████▍     | 181/408 [03:34<04:26,  1.17s/it] 45%|████▍     | 182/408 [03:35<04:25,  1.17s/it] 45%|████▍     | 183/408 [03:36<04:23,  1.17s/it] 45%|████▌     | 184/408 [03:37<04:22,  1.17s/it] 45%|████▌     | 185/408 [03:38<04:21,  1.17s/it] 46%|████▌     | 186/408 [03:40<04:20,  1.17s/it] 46%|████▌     | 187/408 [03:41<04:18,  1.17s/it] 46%|████▌     | 188/408 [03:42<04:17,  1.17s/it] 46%|████▋     | 189/408 [03:43<04:17,  1.18s/it] 47%|████▋     | 190/408 [03:44<04:16,  1.17s/it]                                                  47%|████▋     | 190/408 [03:44<04:16,  1.17s/it] 47%|████▋     | 191/408 [03:46<04:15,  1.18s/it] 47%|████▋     | 192/408 [03:47<04:14,  1.18s/it] 47%|████▋     | 193/408 [03:48<04:12,  1.18s/it] 48%|████▊     | 194/408 [03:49<04:11,  1.18s/it] 48%|████▊     | 195/408 [03:50<04:10,  1.17s/it] 48%|████▊     | 196/408 [03:51<04:08,  1.17s/it] 48%|████▊     | 197/408 [03:53<04:07,  1.17s/it] 49%|████▊     | 198/408 [03:54<04:06,  1.17s/it] 49%|████▉     | 199/408 [03:55<04:05,  1.17s/it] 49%|████▉     | 200/408 [03:56<04:04,  1.17s/it]                                                  49%|████▉     | 200/408 [03:56<04:04,  1.17s/it] 49%|████▉     | 201/408 [03:57<04:02,  1.17s/it] 50%|████▉     | 202/408 [03:58<04:01,  1.17s/it] 50%|████▉     | 203/408 [04:00<04:00,  1.17s/it] 50%|█████     | 204/408 [04:01<03:59,  1.18s/it] 50%|█████     | 205/408 [04:02<03:59,  1.18s/it] 50%|█████     | 206/408 [04:03<03:58,  1.18s/it] 51%|█████     | 207/408 [04:04<03:56,  1.18s/it] 51%|█████     | 208/408 [04:05<03:54,  1.17s/it] 51%|█████     | 209/408 [04:07<03:53,  1.17s/it] 51%|█████▏    | 210/408 [04:08<03:52,  1.17s/it]                                                  51%|█████▏    | 210/408 [04:08<03:52,  1.17s/it] 52%|█████▏    | 211/408 [04:09<03:50,  1.17s/it] 52%|█████▏    | 212/408 [04:10<03:49,  1.17s/it] 52%|█████▏    | 213/408 [04:11<03:48,  1.17s/it] 52%|█████▏    | 214/408 [04:13<03:47,  1.17s/it] 53%|█████▎    | 215/408 [04:14<03:46,  1.17s/it] 53%|█████▎    | 216/408 [04:15<03:44,  1.17s/it] 53%|█████▎    | 217/408 [04:16<03:43,  1.17s/it] 53%|█████▎    | 218/408 [04:17<03:42,  1.17s/it] 54%|█████▎    | 219/408 [04:18<03:41,  1.17s/it] 54%|█████▍    | 220/408 [04:20<03:40,  1.17s/it]                                                  54%|█████▍    | 220/408 [04:20<03:40,  1.17s/it] 54%|█████▍    | 221/408 [04:21<03:39,  1.17s/it] 54%|█████▍    | 222/408 [04:22<03:37,  1.17s/it] 55%|█████▍    | 223/408 [04:23<03:36,  1.17s/it] 55%|█████▍    | 224/408 [04:24<03:35,  1.17s/it] 55%|█████▌    | 225/408 [04:25<03:34,  1.17s/it] 55%|█████▌    | 226/408 [04:27<03:32,  1.17s/it] 56%|█████▌    | 227/408 [04:28<03:31,  1.17s/it] 56%|█████▌    | 228/408 [04:29<03:30,  1.17s/it] 56%|█████▌    | 229/408 [04:30<03:29,  1.17s/it] 56%|█████▋    | 230/408 [04:31<03:28,  1.17s/it]                                                  56%|█████▋    | 230/408 [04:31<03:28,  1.17s/it] 57%|█████▋    | 231/408 [04:32<03:27,  1.17s/it] 57%|█████▋    | 232/408 [04:34<03:26,  1.17s/it] 57%|█████▋    | 233/408 [04:35<03:24,  1.17s/it] 57%|█████▋    | 234/408 [04:36<03:23,  1.17s/it] 58%|█████▊    | 235/408 [04:37<03:22,  1.17s/it] 58%|█████▊    | 236/408 [04:38<03:21,  1.17s/it] 58%|█████▊    | 237/408 [04:39<03:20,  1.17s/it] 58%|█████▊    | 238/408 [04:41<03:19,  1.17s/it] 59%|█████▊    | 239/408 [04:42<03:17,  1.17s/it] 59%|█████▉    | 240/408 [04:43<03:16,  1.17s/it]                                                  59%|█████▉    | 240/408 [04:43<03:16,  1.17s/it] 59%|█████▉    | 241/408 [04:44<03:15,  1.17s/it] 59%|█████▉    | 242/408 [04:45<03:13,  1.17s/it] 60%|█████▉    | 243/408 [04:46<03:12,  1.17s/it] 60%|█████▉    | 244/408 [04:48<03:12,  1.17s/it] 60%|██████    | 245/408 [04:49<03:10,  1.17s/it] 60%|██████    | 246/408 [04:50<03:09,  1.17s/it] 61%|██████    | 247/408 [04:51<03:08,  1.17s/it] 61%|██████    | 248/408 [04:52<03:07,  1.17s/it] 61%|██████    | 249/408 [04:53<03:05,  1.17s/it] 61%|██████▏   | 250/408 [04:55<03:04,  1.17s/it]                                                  61%|██████▏   | 250/408 [04:55<03:04,  1.17s/it] 62%|██████▏   | 251/408 [04:56<03:03,  1.17s/it] 62%|██████▏   | 252/408 [04:57<03:02,  1.17s/it] 62%|██████▏   | 253/408 [04:58<03:01,  1.17s/it] 62%|██████▏   | 254/408 [04:59<03:00,  1.17s/it] 62%|██████▎   | 255/408 [05:00<02:59,  1.17s/it] 63%|██████▎   | 256/408 [05:02<02:57,  1.17s/it] 63%|██████▎   | 257/408 [05:03<02:56,  1.17s/it] 63%|██████▎   | 258/408 [05:04<02:56,  1.17s/it] 63%|██████▎   | 259/408 [05:05<02:54,  1.17s/it] 64%|██████▎   | 260/408 [05:06<02:53,  1.17s/it]                                                  64%|██████▎   | 260/408 [05:06<02:53,  1.17s/it] 64%|██████▍   | 261/408 [05:08<02:52,  1.17s/it] 64%|██████▍   | 262/408 [05:09<02:51,  1.17s/it] 64%|██████▍   | 263/408 [05:10<02:50,  1.17s/it] 65%|██████▍   | 264/408 [05:11<02:48,  1.17s/it]Process Process-5:
Process Process-5:
Process Process-6:
Process Process-6:
Process Process-5:
Process Process-6:
Process Process-7:
Traceback (most recent call last):
Process Process-5:
  File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
KeyboardInterrupt
Traceback (most recent call last):
  File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
KeyboardInterrupt
Process Process-8:
Process Process-6:
Process Process-7:
Traceback (most recent call last):
  File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
KeyboardInterrupt
Traceback (most recent call last):
Traceback (most recent call last):
  File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
Traceback (most recent call last):
  File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
Traceback (most recent call last):
Traceback (most recent call last):
  File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 372, in _worker_loop
    if done_event.is_set():
  File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 372, in _worker_loop
    if done_event.is_set():
  File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/multiprocessing/synchronize.py", line 328, in is_set
    with self._cond:
  File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 372, in _worker_loop
    if done_event.is_set():
  File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/multiprocessing/synchronize.py", line 327, in is_set
    def is_set(self):
  File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/multiprocessing/synchronize.py", line 233, in __exit__
    return self._lock.__exit__(*args)
  File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/multiprocessing/synchronize.py", line 329, in is_set
    if self._flag.acquire(False):
  File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/multiprocessing/synchronize.py", line 97, in __exit__
    def __exit__(self, *args):
  File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 372, in _worker_loop
    if done_event.is_set():
KeyboardInterrupt
  File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 372, in _worker_loop
    if done_event.is_set():
Traceback (most recent call last):
KeyboardInterrupt
  File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/multiprocessing/synchronize.py", line 328, in is_set
    with self._cond:
KeyboardInterrupt
Traceback (most recent call last):
Traceback (most recent call last):
  File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/multiprocessing/synchronize.py", line 328, in is_set
    with self._cond:
  File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/multiprocessing/process.py", line 317, in _bootstrap
    util._exit_function()
  File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/multiprocessing/synchronize.py", line 230, in __enter__
    return self._lock.__enter__()
  File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/multiprocessing/synchronize.py", line 230, in __enter__
    return self._lock.__enter__()
  File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/multiprocessing/util.py", line 332, in _exit_function
    info('process shutting down')
  File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/multiprocessing/util.py", line 52, in info
    def info(msg, *args):
  File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/multiprocessing/process.py", line 317, in _bootstrap
    util._exit_function()
  File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/multiprocessing/process.py", line 317, in _bootstrap
    util._exit_function()
  File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/multiprocessing/synchronize.py", line 94, in __enter__
    def __enter__(self):
  File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/multiprocessing/util.py", line 320, in _exit_function
    def _exit_function(info=info, debug=debug, _run_finalizers=_run_finalizers,
  File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/multiprocessing/synchronize.py", line 94, in __enter__
    def __enter__(self):
  File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/multiprocessing/util.py", line 320, in _exit_function
    def _exit_function(info=info, debug=debug, _run_finalizers=_run_finalizers,
KeyboardInterrupt
KeyboardInterrupt
KeyboardInterrupt
KeyboardInterrupt
KeyboardInterrupt
Traceback (most recent call last):
  File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/subprocess.py", line 1959, in _wait
    (pid, sts) = self._try_wait(0)
  File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/subprocess.py", line 1917, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/bin/llamafactory-cli", line 8, in <module>
    sys.exit(main())
  File "/proj/long-multi/kqian/yongchaochen/LLaMA-Factory/src/llamafactory/cli.py", line 99, in main
    process = subprocess.run(
  File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/subprocess.py", line 505, in run
    stdout, stderr = process.communicate(input, timeout=timeout)
  File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/subprocess.py", line 1146, in communicate
    self.wait()
  File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/subprocess.py", line 1222, in wait
    self._wait(timeout=sigint_timeout)
  File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/subprocess.py", line 1927, in _wait
    def _wait(self, timeout):
KeyboardInterrupt
[rank1]: Traceback (most recent call last):
[rank1]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 745, in _fn
[rank1]:     return fn(*args, **kwargs)
[rank1]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 489, in checkpoint
[rank1]:     return CheckpointFunction.apply(function, preserve, *args)
[rank1]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/autograd/function.py", line 575, in apply
[rank1]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank1]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 264, in forward
[rank1]:     outputs = run_function(*args)
[rank1]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank1]:     return inner()
[rank1]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank1]:     result = forward_call(*args, **kwargs)
[rank1]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 343, in forward
[rank1]:     hidden_states, self_attn_weights = self.self_attn(
[rank1]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank1]:     return inner()
[rank1]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank1]:     result = forward_call(*args, **kwargs)
[rank1]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 299, in forward
[rank1]:     attn_output, attn_weights = attention_interface(
[rank1]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/transformers/integrations/sdpa_attention.py", line 30, in sdpa_attention_forward
[rank1]:     key = repeat_kv(key, module.num_key_value_groups)
[rank1]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/transformers/integrations/sdpa_attention.py", line 15, in repeat_kv
[rank1]:     return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
[rank1]: KeyboardInterrupt

[rank1]: During handling of the above exception, another exception occurred:

[rank1]: Traceback (most recent call last):
[rank1]:   File "/proj/long-multi/kqian/yongchaochen/LLaMA-Factory/src/llamafactory/launcher.py", line 23, in <module>
[rank1]:     launch()
[rank1]:   File "/proj/long-multi/kqian/yongchaochen/LLaMA-Factory/src/llamafactory/launcher.py", line 19, in launch
[rank1]:     run_exp()
[rank1]:   File "/proj/long-multi/kqian/yongchaochen/LLaMA-Factory/src/llamafactory/train/tuner.py", line 107, in run_exp
[rank1]:     _training_function(config={"args": args, "callbacks": callbacks})
[rank1]:   File "/proj/long-multi/kqian/yongchaochen/LLaMA-Factory/src/llamafactory/train/tuner.py", line 69, in _training_function
[rank1]:     run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)
[rank1]:   File "/proj/long-multi/kqian/yongchaochen/LLaMA-Factory/src/llamafactory/train/sft/workflow.py", line 102, in run_sft
[rank1]:     train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
[rank1]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/transformers/trainer.py", line 2245, in train
[rank1]:     return inner_training_loop(
[rank1]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/transformers/trainer.py", line 2556, in _inner_training_loop
[rank1]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank1]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/transformers/trainer.py", line 3718, in training_step
[rank1]:     loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
[rank1]:   File "/proj/long-multi/kqian/yongchaochen/LLaMA-Factory/src/llamafactory/train/sft/trainer.py", line 101, in compute_loss
[rank1]:     return super().compute_loss(model, inputs, *args, **kwargs)
[rank1]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/transformers/trainer.py", line 3783, in compute_loss
[rank1]:     outputs = model(**inputs)
[rank1]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 18, in wrapped_fn
[rank1]:     ret_val = func(*args, **kwargs)
[rank1]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1899, in forward
[rank1]:     loss = self.module(*inputs, **kwargs)
[rank1]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank1]:     return inner()
[rank1]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank1]:     result = forward_call(*args, **kwargs)
[rank1]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
[rank1]:     return func(*args, **kwargs)
[rank1]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 853, in forward
[rank1]:     outputs = self.model(
[rank1]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank1]:     return inner()
[rank1]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank1]:     result = forward_call(*args, **kwargs)
[rank1]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 589, in forward
[rank1]:     layer_outputs = self._gradient_checkpointing_func(
[rank1]:   File "/proj/long-multi/kqian/yongchaochen/LLaMA-Factory/src/llamafactory/model/model_utils/checkpointing.py", line 93, in custom_gradient_checkpointing_func
[rank1]:     return gradient_checkpointing_func(func, *args, **kwargs)
[rank1]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/_compile.py", line 32, in inner
[rank1]:     return disable_fn(*args, **kwargs)
[rank1]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 748, in _fn
[rank1]:     set_skip_guard_eval_unsafe(prior_skip_guard_eval_unsafe)
[rank1]: KeyboardInterrupt
[rank3]: Traceback (most recent call last):
[rank3]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/transformers/trainer.py", line 3718, in training_step
[rank3]:     loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
[rank3]:   File "/proj/long-multi/kqian/yongchaochen/LLaMA-Factory/src/llamafactory/train/sft/trainer.py", line 101, in compute_loss
[rank3]:     return super().compute_loss(model, inputs, *args, **kwargs)
[rank3]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/transformers/trainer.py", line 3783, in compute_loss
[rank3]:     outputs = model(**inputs)
[rank3]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 18, in wrapped_fn
[rank3]:     ret_val = func(*args, **kwargs)
[rank3]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1899, in forward
[rank3]:     loss = self.module(*inputs, **kwargs)
[rank3]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank3]:     return inner()
[rank3]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank3]:     result = forward_call(*args, **kwargs)
[rank3]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
[rank3]:     return func(*args, **kwargs)
[rank3]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 853, in forward
[rank3]:     outputs = self.model(
[rank3]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank3]:     return inner()
[rank3]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank3]:     result = forward_call(*args, **kwargs)
[rank3]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 589, in forward
[rank3]:     layer_outputs = self._gradient_checkpointing_func(
[rank3]:   File "/proj/long-multi/kqian/yongchaochen/LLaMA-Factory/src/llamafactory/model/model_utils/checkpointing.py", line 93, in custom_gradient_checkpointing_func
[rank3]:     return gradient_checkpointing_func(func, *args, **kwargs)
[rank3]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/_compile.py", line 32, in inner
[rank3]:     return disable_fn(*args, **kwargs)
[rank3]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 745, in _fn
[rank3]:     return fn(*args, **kwargs)
[rank3]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 489, in checkpoint
[rank3]:     return CheckpointFunction.apply(function, preserve, *args)
[rank3]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/autograd/function.py", line 575, in apply
[rank3]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank3]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 264, in forward
[rank3]:     outputs = run_function(*args)
[rank3]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank3]:     return inner()
[rank3]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank3]:     result = forward_call(*args, **kwargs)
[rank3]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 359, in forward
[rank3]:     hidden_states = self.mlp(hidden_states)
[rank3]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank3]:     return inner()
[rank3]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank3]:     result = forward_call(*args, **kwargs)
[rank3]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 197, in forward
[rank3]:     down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
[rank3]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank3]:     return inner()
[rank3]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1782, in inner
[rank3]:     args_result = hook(self, args)
[rank3]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/deepspeed/runtime/zero/parameter_offload.py", line 422, in _post_backward_module_hook
[rank3]:     return apply_to_tensors_only(module.post_bwd_fn.apply,
[rank3]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/deepspeed/runtime/zero/utils.py", line 133, in apply_to_tensors_only
[rank3]:     touched_output = apply_to_tensors_only(function, elem)
[rank3]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/deepspeed/runtime/zero/utils.py", line 149, in apply_to_tensors_only
[rank3]:     touched_output = function(value)
[rank3]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/autograd/function.py", line 575, in apply
[rank3]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank3]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/deepspeed/runtime/zero/parameter_offload.py", line 396, in forward
[rank3]:     @staticmethod
[rank3]: KeyboardInterrupt

[rank3]: During handling of the above exception, another exception occurred:

[rank3]: Traceback (most recent call last):
[rank3]:   File "/proj/long-multi/kqian/yongchaochen/LLaMA-Factory/src/llamafactory/launcher.py", line 23, in <module>
[rank3]:     launch()
[rank3]:   File "/proj/long-multi/kqian/yongchaochen/LLaMA-Factory/src/llamafactory/launcher.py", line 19, in launch
[rank3]:     run_exp()
[rank3]:   File "/proj/long-multi/kqian/yongchaochen/LLaMA-Factory/src/llamafactory/train/tuner.py", line 107, in run_exp
[rank3]:     _training_function(config={"args": args, "callbacks": callbacks})
[rank3]:   File "/proj/long-multi/kqian/yongchaochen/LLaMA-Factory/src/llamafactory/train/tuner.py", line 69, in _training_function
[rank3]:     run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)
[rank3]:   File "/proj/long-multi/kqian/yongchaochen/LLaMA-Factory/src/llamafactory/train/sft/workflow.py", line 102, in run_sft
[rank3]:     train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
[rank3]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/transformers/trainer.py", line 2245, in train
[rank3]:     return inner_training_loop(
[rank3]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/transformers/trainer.py", line 2556, in _inner_training_loop
[rank3]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank3]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/transformers/trainer.py", line 3718, in training_step
[rank3]:     loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
[rank3]: KeyboardInterrupt
[rank2]: Traceback (most recent call last):
[rank2]:   File "/proj/long-multi/kqian/yongchaochen/LLaMA-Factory/src/llamafactory/launcher.py", line 23, in <module>
[rank2]:     launch()
[rank2]:   File "/proj/long-multi/kqian/yongchaochen/LLaMA-Factory/src/llamafactory/launcher.py", line 19, in launch
[rank2]:     run_exp()
[rank2]:   File "/proj/long-multi/kqian/yongchaochen/LLaMA-Factory/src/llamafactory/train/tuner.py", line 107, in run_exp
[rank2]:     _training_function(config={"args": args, "callbacks": callbacks})
[rank2]:   File "/proj/long-multi/kqian/yongchaochen/LLaMA-Factory/src/llamafactory/train/tuner.py", line 69, in _training_function
[rank2]:     run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)
[rank2]:   File "/proj/long-multi/kqian/yongchaochen/LLaMA-Factory/src/llamafactory/train/sft/workflow.py", line 102, in run_sft
[rank2]:     train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
[rank2]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/transformers/trainer.py", line 2245, in train
[rank2]:     return inner_training_loop(
[rank2]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/transformers/trainer.py", line 2556, in _inner_training_loop
[rank2]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank2]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/transformers/trainer.py", line 3718, in training_step
[rank2]:     loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
[rank2]:   File "/proj/long-multi/kqian/yongchaochen/LLaMA-Factory/src/llamafactory/train/sft/trainer.py", line 101, in compute_loss
[rank2]:     return super().compute_loss(model, inputs, *args, **kwargs)
[rank2]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/transformers/trainer.py", line 3783, in compute_loss
[rank2]:     outputs = model(**inputs)
[rank2]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 18, in wrapped_fn
[rank2]:     ret_val = func(*args, **kwargs)
[rank2]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1899, in forward
[rank2]:     loss = self.module(*inputs, **kwargs)
[rank2]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank2]:     return inner()
[rank2]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank2]:     result = forward_call(*args, **kwargs)
[rank2]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
[rank2]:     return func(*args, **kwargs)
[rank2]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 853, in forward
[rank2]:     outputs = self.model(
[rank2]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank2]:     return inner()
[rank2]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank2]:     result = forward_call(*args, **kwargs)
[rank2]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 589, in forward
[rank2]:     layer_outputs = self._gradient_checkpointing_func(
[rank2]:   File "/proj/long-multi/kqian/yongchaochen/LLaMA-Factory/src/llamafactory/model/model_utils/checkpointing.py", line 93, in custom_gradient_checkpointing_func
[rank2]:     return gradient_checkpointing_func(func, *args, **kwargs)
[rank2]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/_compile.py", line 32, in inner
[rank2]:     return disable_fn(*args, **kwargs)
[rank2]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 745, in _fn
[rank2]:     return fn(*args, **kwargs)
[rank2]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 489, in checkpoint
[rank2]:     return CheckpointFunction.apply(function, preserve, *args)
[rank2]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/autograd/function.py", line 575, in apply
[rank2]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank2]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 264, in forward
[rank2]:     outputs = run_function(*args)
[rank2]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank2]:     return inner()
[rank2]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank2]:     result = forward_call(*args, **kwargs)
[rank2]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 343, in forward
[rank2]:     hidden_states, self_attn_weights = self.self_attn(
[rank2]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank2]:     return inner()
[rank2]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank2]:     result = forward_call(*args, **kwargs)
[rank2]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 311, in forward
[rank2]:     attn_output = self.o_proj(attn_output)
[rank2]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank2]:     return inner()
[rank2]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank2]:     result = forward_call(*args, **kwargs)
[rank2]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 125, in forward
[rank2]:     return F.linear(input, self.weight, self.bias)
[rank2]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py", line 116, in zero3_linear_wrap
[rank2]:     return LinearFunctionForZeroStage3.apply(input, weight)
[rank2]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/autograd/function.py", line 575, in apply
[rank2]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank2]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/amp/autocast_mode.py", line 503, in decorate_fwd
[rank2]:     return fwd(*args, **kwargs)
[rank2]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py", line 64, in forward
[rank2]:     output = input.matmul(weight.t())
[rank2]: KeyboardInterrupt
[rank0]: Traceback (most recent call last):
[rank0]:   File "/proj/long-multi/kqian/yongchaochen/LLaMA-Factory/src/llamafactory/launcher.py", line 23, in <module>
[rank0]:     launch()
[rank0]:   File "/proj/long-multi/kqian/yongchaochen/LLaMA-Factory/src/llamafactory/launcher.py", line 19, in launch
[rank0]:     run_exp()
[rank0]:   File "/proj/long-multi/kqian/yongchaochen/LLaMA-Factory/src/llamafactory/train/tuner.py", line 107, in run_exp
[rank0]:     _training_function(config={"args": args, "callbacks": callbacks})
[rank0]:   File "/proj/long-multi/kqian/yongchaochen/LLaMA-Factory/src/llamafactory/train/tuner.py", line 69, in _training_function
[rank0]:     run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)
[rank0]:   File "/proj/long-multi/kqian/yongchaochen/LLaMA-Factory/src/llamafactory/train/sft/workflow.py", line 102, in run_sft
[rank0]:     train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
[rank0]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/transformers/trainer.py", line 2245, in train
[rank0]:     return inner_training_loop(
[rank0]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/transformers/trainer.py", line 2556, in _inner_training_loop
[rank0]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank0]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/transformers/trainer.py", line 3718, in training_step
[rank0]:     loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
[rank0]:   File "/proj/long-multi/kqian/yongchaochen/LLaMA-Factory/src/llamafactory/train/sft/trainer.py", line 101, in compute_loss
[rank0]:     return super().compute_loss(model, inputs, *args, **kwargs)
[rank0]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/transformers/trainer.py", line 3783, in compute_loss
[rank0]:     outputs = model(**inputs)
[rank0]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 18, in wrapped_fn
[rank0]:     ret_val = func(*args, **kwargs)
[rank0]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1899, in forward
[rank0]:     loss = self.module(*inputs, **kwargs)
[rank0]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank0]:     return inner()
[rank0]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank0]:     result = forward_call(*args, **kwargs)
[rank0]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
[rank0]:     return func(*args, **kwargs)
[rank0]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 853, in forward
[rank0]:     outputs = self.model(
[rank0]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank0]:     return inner()
[rank0]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank0]:     result = forward_call(*args, **kwargs)
[rank0]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 589, in forward
[rank0]:     layer_outputs = self._gradient_checkpointing_func(
[rank0]:   File "/proj/long-multi/kqian/yongchaochen/LLaMA-Factory/src/llamafactory/model/model_utils/checkpointing.py", line 93, in custom_gradient_checkpointing_func
[rank0]:     return gradient_checkpointing_func(func, *args, **kwargs)
[rank0]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/_compile.py", line 32, in inner
[rank0]:     return disable_fn(*args, **kwargs)
[rank0]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 745, in _fn
[rank0]:     return fn(*args, **kwargs)
[rank0]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 489, in checkpoint
[rank0]:     return CheckpointFunction.apply(function, preserve, *args)
[rank0]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/autograd/function.py", line 575, in apply
[rank0]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank0]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 264, in forward
[rank0]:     outputs = run_function(*args)
[rank0]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank0]:     return inner()
[rank0]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank0]:     result = forward_call(*args, **kwargs)
[rank0]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 343, in forward
[rank0]:     hidden_states, self_attn_weights = self.self_attn(
[rank0]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank0]:     return inner()
[rank0]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank0]:     result = forward_call(*args, **kwargs)
[rank0]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 311, in forward
[rank0]:     attn_output = self.o_proj(attn_output)
[rank0]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank0]:     return inner()
[rank0]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1782, in inner
[rank0]:     args_result = hook(self, args)
[rank0]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 18, in wrapped_fn
[rank0]:     ret_val = func(*args, **kwargs)
[rank0]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/deepspeed/runtime/zero/parameter_offload.py", line 275, in _pre_forward_module_hook
[rank0]:     self.pre_sub_module_forward_function(module)
[rank0]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank0]:     return func(*args, **kwargs)
[rank0]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/deepspeed/runtime/zero/parameter_offload.py", line 449, in pre_sub_module_forward_function
[rank0]:     param_coordinator.fetch_sub_module(sub_module, forward=True)
[rank0]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 745, in _fn
[rank0]:     return fn(*args, **kwargs)
[rank0]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 18, in wrapped_fn
[rank0]:     ret_val = func(*args, **kwargs)
[rank0]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank0]:     return func(*args, **kwargs)
[rank0]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/deepspeed/runtime/zero/partitioned_param_coordinator.py", line 386, in fetch_sub_module
[rank0]:     self.__all_gather_params(params_to_prefetch, forward)
[rank0]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 18, in wrapped_fn
[rank0]:     ret_val = func(*args, **kwargs)
[rank0]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/deepspeed/runtime/zero/partitioned_param_coordinator.py", line 435, in __all_gather_params
[rank0]:     self.__all_gather_params_(nonquantized_params, forward, quantize=self.zero_quantized_weights)
[rank0]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/deepspeed/runtime/zero/partitioned_param_coordinator.py", line 464, in __all_gather_params_
[rank0]:     handle = param_group[0].all_gather_coalesced(param_group, quantize=quantize)
[rank0]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 18, in wrapped_fn
[rank0]:     ret_val = func(*args, **kwargs)
[rank0]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py", line 1253, in all_gather_coalesced
[rank0]:     handles = _dist_allgather_fn(
[rank0]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py", line 95, in _dist_allgather_fn
[rank0]:     return instrument_w_nvtx(dist.allgather_fn)(output_tensor, input_tensor, group=group, async_op=True)
[rank0]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 18, in wrapped_fn
[rank0]:     ret_val = func(*args, **kwargs)
[rank0]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/deepspeed/comm/comm.py", line 320, in allgather_fn
[rank0]:     return all_gather_into_tensor(output_tensor, input_tensor, group=group, async_op=async_op, debug=debug)
[rank0]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/deepspeed/comm/comm.py", line 117, in log_wrapper
[rank0]:     return func(*args, **kwargs)
[rank0]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/deepspeed/comm/comm.py", line 305, in all_gather_into_tensor
[rank0]:     return cdb.all_gather_into_tensor(output_tensor=output_tensor, input_tensor=tensor, group=group, async_op=async_op)
[rank0]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 745, in _fn
[rank0]:     return fn(*args, **kwargs)
[rank0]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/deepspeed/comm/torch.py", line 214, in all_gather_into_tensor
[rank0]:     return self.all_gather_function(output_tensor=output_tensor,
[rank0]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
[rank0]:     return func(*args, **kwargs)
[rank0]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 3798, in all_gather_into_tensor
[rank0]:     work = group._allgather_base(output_tensor, input_tensor, opts)
[rank0]: KeyboardInterrupt
Exception ignored in atexit callback: <function matmul_ext_update_autotune_table at 0x154ce1b2fc70>
Traceback (most recent call last):
  File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 477, in matmul_ext_update_autotune_table
    fp16_matmul._update_autotune_table()
  File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 454, in _update_autotune_table
    TritonMatmul._update_autotune_table(__class__.__name__ + "_2d_kernel", __class__._2d_kernel)
  File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 183, in _update_autotune_table
    cache_manager.put(autotune_table)
  File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/deepspeed/ops/transformer/inference/triton/matmul_ext.py", line 99, in put
    with FileLock(self.lock_path):
  File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/filelock/_api.py", line 376, in __enter__
    self.acquire()
  File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/filelock/_api.py", line 344, in acquire
    time.sleep(poll_interval)
  File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 273561) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.
Exception ignored in atexit callback: <function _exit_function at 0x1550e7c185e0>
Traceback (most recent call last):
  File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/multiprocessing/util.py", line 357, in _exit_function
    p.join()
  File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/multiprocessing/process.py", line 149, in join
    res = self._popen.wait(timeout)
  File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/multiprocessing/popen_fork.py", line 43, in wait
    return self.poll(os.WNOHANG if timeout == 0.0 else 0)
  File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/multiprocessing/popen_fork.py", line 27, in poll
    pid, sts = os.waitpid(self.pid, flag)
  File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 273567) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.
Exception ignored in atexit callback: <function _exit_function at 0x14f3929385e0>
Traceback (most recent call last):
  File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/multiprocessing/util.py", line 357, in _exit_function
    p.join()
  File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/multiprocessing/process.py", line 149, in join
    res = self._popen.wait(timeout)
  File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/multiprocessing/popen_fork.py", line 43, in wait
    return self.poll(os.WNOHANG if timeout == 0.0 else 0)
  File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/multiprocessing/popen_fork.py", line 27, in poll
    pid, sts = os.waitpid(self.pid, flag)
  File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 273560) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.
Exception ignored in atexit callback: <function _exit_function at 0x14c36e2c45e0>
Traceback (most recent call last):
  File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/multiprocessing/util.py", line 357, in _exit_function
    p.join()
  File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/multiprocessing/process.py", line 149, in join
    res = self._popen.wait(timeout)
  File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/multiprocessing/popen_fork.py", line 43, in wait
    return self.poll(os.WNOHANG if timeout == 0.0 else 0)
  File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/multiprocessing/popen_fork.py", line 27, in poll
    pid, sts = os.waitpid(self.pid, flag)
  File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 273558) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.
 65%|██████▍   | 264/408 [37:50<20:38,  8.60s/it]
W0420 15:32:19.915000 4075228 site-packages/torch/distributed/run.py:792] 
W0420 15:32:19.915000 4075228 site-packages/torch/distributed/run.py:792] *****************************************
W0420 15:32:19.915000 4075228 site-packages/torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0420 15:32:19.915000 4075228 site-packages/torch/distributed/run.py:792] *****************************************
[INFO|tokenization_utils_base.py:2060] 2025-04-20 15:32:26,985 >> loading file vocab.json from cache at /proj/long-multi/kqian/hf_cache/hub/models--Qwen--Qwen2.5-7B-Instruct-1M/snapshots/e28526f7bb80e2a9c8af03b831a9af3812f18fba/vocab.json
[INFO|tokenization_utils_base.py:2060] 2025-04-20 15:32:26,985 >> loading file merges.txt from cache at /proj/long-multi/kqian/hf_cache/hub/models--Qwen--Qwen2.5-7B-Instruct-1M/snapshots/e28526f7bb80e2a9c8af03b831a9af3812f18fba/merges.txt
[INFO|tokenization_utils_base.py:2060] 2025-04-20 15:32:26,985 >> loading file tokenizer.json from cache at /proj/long-multi/kqian/hf_cache/hub/models--Qwen--Qwen2.5-7B-Instruct-1M/snapshots/e28526f7bb80e2a9c8af03b831a9af3812f18fba/tokenizer.json
[INFO|tokenization_utils_base.py:2060] 2025-04-20 15:32:26,985 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2060] 2025-04-20 15:32:26,985 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2060] 2025-04-20 15:32:26,985 >> loading file tokenizer_config.json from cache at /proj/long-multi/kqian/hf_cache/hub/models--Qwen--Qwen2.5-7B-Instruct-1M/snapshots/e28526f7bb80e2a9c8af03b831a9af3812f18fba/tokenizer_config.json
[INFO|tokenization_utils_base.py:2060] 2025-04-20 15:32:26,985 >> loading file chat_template.jinja from cache at None
[INFO|tokenization_utils_base.py:2323] 2025-04-20 15:32:27,191 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:699] 2025-04-20 15:32:27,384 >> loading configuration file config.json from cache at /proj/long-multi/kqian/hf_cache/hub/models--Qwen--Qwen2.5-7B-Instruct-1M/snapshots/e28526f7bb80e2a9c8af03b831a9af3812f18fba/config.json
[INFO|configuration_utils.py:771] 2025-04-20 15:32:27,385 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dual_chunk_attention_config": {
    "chunk_size": 262144,
    "local_size": 8192,
    "original_max_position_embeddings": 262144
  },
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 1010000,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000000.0,
  "sliding_window": 32768,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.50.0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2060] 2025-04-20 15:32:27,473 >> loading file vocab.json from cache at /proj/long-multi/kqian/hf_cache/hub/models--Qwen--Qwen2.5-7B-Instruct-1M/snapshots/e28526f7bb80e2a9c8af03b831a9af3812f18fba/vocab.json
[INFO|tokenization_utils_base.py:2060] 2025-04-20 15:32:27,473 >> loading file merges.txt from cache at /proj/long-multi/kqian/hf_cache/hub/models--Qwen--Qwen2.5-7B-Instruct-1M/snapshots/e28526f7bb80e2a9c8af03b831a9af3812f18fba/merges.txt
[INFO|tokenization_utils_base.py:2060] 2025-04-20 15:32:27,473 >> loading file tokenizer.json from cache at /proj/long-multi/kqian/hf_cache/hub/models--Qwen--Qwen2.5-7B-Instruct-1M/snapshots/e28526f7bb80e2a9c8af03b831a9af3812f18fba/tokenizer.json
[INFO|tokenization_utils_base.py:2060] 2025-04-20 15:32:27,473 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2060] 2025-04-20 15:32:27,473 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2060] 2025-04-20 15:32:27,473 >> loading file tokenizer_config.json from cache at /proj/long-multi/kqian/hf_cache/hub/models--Qwen--Qwen2.5-7B-Instruct-1M/snapshots/e28526f7bb80e2a9c8af03b831a9af3812f18fba/tokenizer_config.json
[INFO|tokenization_utils_base.py:2060] 2025-04-20 15:32:27,473 >> loading file chat_template.jinja from cache at None
[rank1]:[W420 15:32:27.349477332 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank3]:[W420 15:32:27.350898740 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[INFO|tokenization_utils_base.py:2323] 2025-04-20 15:32:27,677 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[rank2]:[W420 15:32:27.616818871 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Setting num_proc from 16 back to 1 for the train split to disable multiprocessing as it only contains one shard.
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 6798 examples [00:00, 68097.29 examples/s]
Converting format of dataset (num_proc=16):   0%|          | 0/6798 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16): 100%|██████████| 6798/6798 [00:00<00:00, 64007.14 examples/s]Converting format of dataset (num_proc=16): 100%|██████████| 6798/6798 [00:00<00:00, 37807.01 examples/s]
[rank0]:[W420 15:32:28.384120679 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Running tokenizer on dataset (num_proc=16):   0%|          | 0/6798 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   6%|▋         | 425/6798 [00:01<00:16, 391.49 examples/s]Running tokenizer on dataset (num_proc=16):  13%|█▎        | 850/6798 [00:01<00:07, 839.27 examples/s]Running tokenizer on dataset (num_proc=16):  25%|██▌       | 1700/6798 [00:01<00:02, 1893.47 examples/s]Running tokenizer on dataset (num_proc=16):  38%|███▊      | 2550/6798 [00:01<00:01, 2526.84 examples/s]Running tokenizer on dataset (num_proc=16):  56%|█████▋    | 3825/6798 [00:01<00:00, 4004.15 examples/s]Running tokenizer on dataset (num_proc=16):  69%|██████▉   | 4675/6798 [00:01<00:00, 4228.65 examples/s]Running tokenizer on dataset (num_proc=16):  81%|████████▏ | 5525/6798 [00:01<00:00, 4719.44 examples/s]Running tokenizer on dataset (num_proc=16):  94%|█████████▍| 6374/6798 [00:02<00:00, 5054.54 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 6798/6798 [00:02<00:00, 3075.05 examples/s]
[INFO|configuration_utils.py:699] 2025-04-20 15:32:33,206 >> loading configuration file config.json from cache at /proj/long-multi/kqian/hf_cache/hub/models--Qwen--Qwen2.5-7B-Instruct-1M/snapshots/e28526f7bb80e2a9c8af03b831a9af3812f18fba/config.json
[INFO|configuration_utils.py:771] 2025-04-20 15:32:33,207 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dual_chunk_attention_config": {
    "chunk_size": 262144,
    "local_size": 8192,
    "original_max_position_embeddings": 262144
  },
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 1010000,
  "max_window_layers": 28,
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000000.0,
  "sliding_window": 32768,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.50.0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|modeling_utils.py:1154] 2025-04-20 15:32:33,374 >> loading weights file model.safetensors from cache at /proj/long-multi/kqian/hf_cache/hub/models--Qwen--Qwen2.5-7B-Instruct-1M/snapshots/e28526f7bb80e2a9c8af03b831a9af3812f18fba/model.safetensors.index.json
[INFO|modeling_utils.py:3747] 2025-04-20 15:32:33,378 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|configuration_utils.py:1139] 2025-04-20 15:32:33,386 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "use_cache": false
}

Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
[WARNING|logging.py:329] 2025-04-20 15:32:33,402 >> Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.02s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.02s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.02s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.15s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:08,  4.02s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:08,  4.02s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:08,  4.02s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:08,  4.14s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:11<00:03,  3.80s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:11<00:03,  3.80s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:11<00:03,  3.80s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:11<00:03,  3.91s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:13<00:00,  3.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:13<00:00,  3.46s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:13<00:00,  3.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:13<00:00,  3.46s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:13<00:00,  3.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:13<00:00,  3.46s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.80s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.89s/it]
[INFO|modeling_utils.py:4987] 2025-04-20 15:32:49,130 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.

[INFO|modeling_utils.py:4995] 2025-04-20 15:32:49,130 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at Qwen/Qwen2.5-7B-Instruct-1M.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.
[INFO|configuration_utils.py:1094] 2025-04-20 15:32:49,184 >> loading configuration file generation_config.json from cache at /proj/long-multi/kqian/hf_cache/hub/models--Qwen--Qwen2.5-7B-Instruct-1M/snapshots/e28526f7bb80e2a9c8af03b831a9af3812f18fba/generation_config.json
[INFO|configuration_utils.py:1139] 2025-04-20 15:32:49,186 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "repetition_penalty": 1.05,
  "temperature": 0.7,
  "top_k": 20,
  "top_p": 0.8
}

[INFO|trainer.py:748] 2025-04-20 15:32:49,207 >> Using auto half precision backend
[INFO|trainer.py:2409] 2025-04-20 15:32:52,271 >> ***** Running training *****
[INFO|trainer.py:2410] 2025-04-20 15:32:52,272 >>   Num examples = 6,118
[INFO|trainer.py:2411] 2025-04-20 15:32:52,272 >>   Num Epochs = 3
[INFO|trainer.py:2412] 2025-04-20 15:32:52,272 >>   Instantaneous batch size per device = 5
[INFO|trainer.py:2415] 2025-04-20 15:32:52,272 >>   Total train batch size (w. parallel, distributed & accumulation) = 40
[INFO|trainer.py:2416] 2025-04-20 15:32:52,272 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:2417] 2025-04-20 15:32:52,272 >>   Total optimization steps = 459
[INFO|trainer.py:2418] 2025-04-20 15:32:52,272 >>   Number of trainable parameters = 7,615,616,512
  0%|          | 0/459 [00:00<?, ?it/s]  0%|          | 1/459 [00:05<44:40,  5.85s/it]  0%|          | 2/459 [00:08<29:45,  3.91s/it][rank3]: Traceback (most recent call last):
[rank3]:   File "/proj/long-multi/kqian/speech_2/LLaMA-Factory/src/llamafactory/launcher.py", line 23, in <module>
[rank3]:     launch()
[rank3]:   File "/proj/long-multi/kqian/speech_2/LLaMA-Factory/src/llamafactory/launcher.py", line 19, in launch
[rank3]:     run_exp()
[rank3]:   File "/proj/long-multi/kqian/speech_2/LLaMA-Factory/src/llamafactory/train/tuner.py", line 107, in run_exp
[rank3]:     _training_function(config={"args": args, "callbacks": callbacks})
[rank3]:   File "/proj/long-multi/kqian/speech_2/LLaMA-Factory/src/llamafactory/train/tuner.py", line 69, in _training_function
[rank3]:     run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)
[rank3]:   File "/proj/long-multi/kqian/speech_2/LLaMA-Factory/src/llamafactory/train/sft/workflow.py", line 102, in run_sft
[rank3]:     train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
[rank3]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/transformers/trainer.py", line 2245, in train
[rank3]:     return inner_training_loop(
[rank3]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/transformers/trainer.py", line 2556, in _inner_training_loop
[rank3]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank3]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/transformers/trainer.py", line 3718, in training_step
[rank3]:     loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
[rank3]:   File "/proj/long-multi/kqian/speech_2/LLaMA-Factory/src/llamafactory/train/sft/trainer.py", line 101, in compute_loss
[rank3]:     return super().compute_loss(model, inputs, *args, **kwargs)
[rank3]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/transformers/trainer.py", line 3783, in compute_loss
[rank3]:     outputs = model(**inputs)
[rank3]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 18, in wrapped_fn
[rank3]:     ret_val = func(*args, **kwargs)
[rank3]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1899, in forward
[rank3]:     loss = self.module(*inputs, **kwargs)
[rank3]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank3]:     return inner()
[rank3]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank3]:     result = forward_call(*args, **kwargs)
[rank3]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
[rank3]:     return func(*args, **kwargs)
[rank3]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py", line 876, in forward
[rank3]:     loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)
[rank3]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/transformers/loss/loss_utils.py", line 56, in ForCausalLMLoss
[rank3]:     loss = fixed_cross_entropy(logits, shift_labels, num_items_in_batch, ignore_index, **kwargs)
[rank3]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/transformers/loss/loss_utils.py", line 27, in fixed_cross_entropy
[rank3]:     loss = nn.functional.cross_entropy(source, target, ignore_index=ignore_index, reduction=reduction)
[rank3]:   File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/nn/functional.py", line 3494, in cross_entropy
[rank3]:     return torch._C._nn.cross_entropy_loss(
[rank3]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 11.60 GiB. GPU 3 has a total capacity of 79.21 GiB of which 4.79 GiB is free. Including non-PyTorch memory, this process has 74.41 GiB memory in use. Of the allocated memory 53.34 GiB is allocated by PyTorch, and 16.51 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
W0420 15:33:05.383000 4075228 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 4075299 closing signal SIGTERM
W0420 15:33:05.384000 4075228 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 4075300 closing signal SIGTERM
W0420 15:33:05.384000 4075228 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 4075301 closing signal SIGTERM
E0420 15:33:06.713000 4075228 site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 3 (pid: 4075302) of binary: /proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/bin/python3.10
Traceback (most recent call last):
  File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
  File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/distributed/run.py", line 918, in main
    run(args)
  File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/distributed/run.py", line 909, in run
    elastic_launch(
  File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/proj/long-multi/kqian/miniforge3/envs/llama3_tune_3/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/proj/long-multi/kqian/speech_2/LLaMA-Factory/src/llamafactory/launcher.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-04-20_15:33:05
  host      : p2-r25-n2.bluevela.rmf.ibm.com
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 4075302)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
